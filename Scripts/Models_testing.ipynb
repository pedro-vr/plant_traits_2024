{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b533ebb5-25f2-4de2-b526-3fd07bddd8fc",
   "metadata": {},
   "source": [
    "# PRUEBA DE MODELOS - EMBEDDING Y CLASIFICACIÃ“N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6198e1-a0a1-4f3b-92e2-e9275b1cbce8",
   "metadata": {},
   "source": [
    "Lo que se pretende en este libro de Jupyter es empezar con las pruebas de algunos modelos que se pudieron obtener de referencia en investigaciones anteriores. Todo esto con el fin de poder tener una mejor nociÃ³n de estos modelos y en el mejor de los casos, ya tener un modelo definido con el que se trabajarÃ¡ posteriormente. \n",
    "\n",
    "Algunos de estos modelos son simplemente de embedding, otros tantos si conllevan algunas tÃ©cnicas ya avanzadas de clasificaciÃ³n o detecciÃ³n de imÃ¡genes, segÃºn corresponda. La lista de modelos que se pretende probar es la siguiente:\n",
    "\n",
    "- OpenL3 (Solo de embedding)\n",
    "\n",
    "- ViT (de Google)\n",
    "\n",
    "- Contrastors: AquÃ­ podemos hacer el uso a su vez de dos modelos, CLIP y MRL\n",
    "\n",
    "- Sports: Lista de varios modelos que nos pueden ayudar con la tarea asignada\n",
    "\n",
    "- ResNet: Modelo moldeable con el nÃºmero de capas que Ã©ste utiliza, es una red neuronal convolucional\n",
    "\n",
    "- InceptionV3: Modelo moldeable con el nÃºmero de capas y parÃ¡metros que Ã©ste utiliza, es una red neuronal convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb999c8-ad49-443e-a579-0ba4778e1f6c",
   "metadata": {},
   "source": [
    "Ejemplos de modelos que algunas personas tomaron para la soluciÃ³n del proyecto. Tomado de [aquÃ­](https://www.kaggle.com/competitions/planttraits2024/code)\n",
    "\n",
    "1. Primer ejemplo: [KerasCV - EfficientNetV2](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook#ğŸ”-%7C-Loss-&-Metric)\n",
    "\n",
    "AquÃ­ en este ejemplo podemos ver los detalles del significado de cada _plant trait_ y quÃ© significa cada uno. AquÃ­ es importante definir algunos parÃ¡metros (que tambiÃ©n he visto dentro de ViT, por ejemplo, y que son el tamÃ±o de las imÃ¡genes, epochs, seed, verbose, etc. AquÃ­ tambiÃ©n se hace el augmentations de las imÃ¡genes (flip, rotation, brightness, etc.), aquÃ­ los augmentations se hacen a un batch, lo cual acelera el entrenamiento y reduce el cuello de botella de CPU. \n",
    "\n",
    "Dentro del procedimiento de aplicar los augmenters, se ven funciones dentro de funciones, lo cual me confunde un poco. DespuÃ©s del proceso de augmenter, viene un proceso de la decodificaciÃ³n de las imÃ¡genes el cual involucra una modificaciÃ³n en el tamaÃ±o de la imagen y asignaciÃ³n de etiquetas (aquÃ­ hay mÃ¡s del proceso del mejoramiento de la imagen pero es un poco confuso)\n",
    "\n",
    "Divide la data en 5 conjuntos, despuÃ©s crea segmentos basados en las 6 _plant traits_ y las combina dentro de una columna final de segmento, al final, utiliza este segmento para balancear distribuciones similares de segmentos a travÃ©s de los conjuntos. \n",
    "\n",
    "Antes de entrenar el modelo como tal, se aplica una normalizaciÃ³n estÃ¡ndar usando StandardScaler, esto asegura que los features tengan escalas consistentes, el cual es crucial para el desempeÃ±o Ã³ptimo de las capas lineales o densas --**Build Train & Valid Dataset**--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb689b-a783-4210-887d-07519371ef3c",
   "metadata": {},
   "source": [
    "## Modelo 1: OpenL3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1456c3-f456-4ff5-ae08-bf6d6a1128ab",
   "metadata": {},
   "source": [
    "Las especificaciones de este modelo se pueden encontrar en la pÃ¡gina de [GitHub](https://github.com/marl/openl3?tab=readme-ov-file) o su [DocumentaciÃ³n](https://openl3.readthedocs.io/en/latest/tutorial.html#introduction) oficial. [API Reference](https://openl3.readthedocs.io/en/latest/api.html) \n",
    "\n",
    "Papers de referencia:\n",
    "\n",
    "- [Paper 1: Look Listen and Learn](https://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf)\n",
    "\n",
    "- [Paper 2: Look, Listen and Learn more: design choices for deep audio embeddings](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf)\n",
    "\n",
    "\n",
    "Los parÃ¡metros para los mÃ©todos de este modelo son los siguientes:\n",
    "\n",
    "- content_type: \"env\", \"music\" (default). \"music\" es para videos, imÃ¡genes o mÃºsica\n",
    "\n",
    "- input_repr: \"linear\", \"mel128\" (default), \"mel256\"\n",
    "\n",
    "- embedding_size: 512, 8192 (default). TamaÃ±o del array resultante con el embedding de la imagen\n",
    "\n",
    "Si el embedding ya existe, entonces no crea uno nuevo, deja el \"original\"\n",
    "\n",
    "Para este modelo, existen 3 posibilidades:\n",
    "\n",
    "1. Puedes ejecutar el modelo directamente a una imagen (o lista de imÃ¡genes) con el mÃ©todo \"get_image_embedding\"\n",
    "\n",
    "2. Puedes guardar el embedding en la misma carpeta de donde viene la imagen para un uso posterior. Para guardar el embedding es el mÃ©todo \"process_image_file\" y para cargarlo es el mÃ©todo \"np.load\" con np la librerÃ­a \"numpy\"\n",
    "\n",
    "3. Puedes pre cargar desde un principio el modelo para que no estÃ©s cargandolo cada que lo requieras para una imagen. El mÃ©todo para pre cargar el modelo es \"openl3.models.load_image_embedding_model\", despuÃ©s, para usarlo en los mÃ©todos de los puntos anteriores, pasas el modelo con el argumento \"model\"\n",
    "\n",
    "- Del mÃ©todo \"imread\" obtuvimos un array de matrices sobre la imagen\n",
    "\n",
    "- De los mÃ©todos de openl3, los argumentos significan lo siguiente:\n",
    "\n",
    "     1. input_repr: RepresentaciÃ³n del espectograma usado por el modelo. Es ignorado si el parÃ¡metro \"modelo\" es un modelo de tipo Keras vÃ¡lido. \"linear-frequency log-magnitude spectogram\", \"Mel-frequency log-magnitude spectogram\": este Ãºltimo captura informaciÃ³n perceptivamente relevante de manera mÃ¡s eficiente con menos bandas de frecuencia (128 o 256 bandas) que el espectograma lineal\n",
    "\n",
    "    2. content_type: Tipo de contenido utilizado para entrenar el modelo de embedding. Es ignorado si el parÃ¡metro \"modelo\" es un modelo de tipo Keras vÃ¡lido. \"music\" se refiere a contenido de mÃºsica como tal, instrumentos, tonos, etc; \"env\" es de environmental y se refiere a sonidos humanos o de la naturaleza, aquellos que son reproducidos de manera \"natural\"\n",
    "\n",
    "    3. embedding_size: DimensiÃ³n que tendrÃ¡ el embedding. Es ignorado si el parÃ¡metro \"modelo\" es un modelo de tipo Keras vÃ¡lido\n",
    "\n",
    "- Consideraciones extras sobre el modelo (obtenidas de los papers correspondientes):\n",
    "\n",
    "    1. El espectrograma \"Mel\" captura informaciÃ³n de manera mÃ¡s eficiente con menos bandas de frecuencia comparado con el espectrograma lineal, por eso siempre es mejor utilizar este argumento \"Mel\" a la hora de utilizar el modelo\n",
    "\n",
    "    2. En la realizaciÃ³n del modelo se utilizÃ³ el optimizador Adam para minimizar la pÃ©rdida de la entropia cruzada binaria con regularizaciÃ³n L2\n",
    " \n",
    "    3. Para las pruebas estadÃ­sticas del modelo se utilizÃ³ la Prueba de rangos de Wilcoxon\n",
    " \n",
    "    4. El modelo L3 no requiere data con etiquetas, la representaciÃ³n Mel es la mejor dentro de este modelo\n",
    " \n",
    "    5. El modelo L3 es no supervisado y aprende informaciÃ³n a partir de entradas de de audio y visual al mismo tiempo\n",
    " \n",
    "    6. Dentro de este modelo se utiliza la tÃ©cnica de Activation Visualization el cual sirve para reconocer las regiones exactas de donde surgen los audios o videos, sus origenes.\n",
    " \n",
    "- Al final, se pudo ejecutar el modelo de manera correcta y se tiene ya un mejor y mayor entendimiento del rendimiento y uso del modelo.\n",
    "\n",
    "- **NOTA: Al ejecutar la funciÃ³n me sale un aviso de que estoy ejecutando una funciÃ³n costosa y me da los siguientes consejos para evitar estos cargos excesivos: Poner como argumento \"reduce_tracing = True\" o consultar documentaciÃ³n de TensorFlow [Doc1](https://www.tensorflow.org/guide/function#controlling_retracing) [Doc2](https://www.tensorflow.org/api_docs/python/tf/function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2945cd4-835d-4077-bb31-6359ec2fd399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tentensorflow<1.14 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tentensorflow<1.14\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Librerias a utilizar en todo este proceso\n",
    "\n",
    "#!pip install openl3\n",
    "\n",
    "import openl3\n",
    "from skimage.io import imread\n",
    "import functions.general_functions as gf\n",
    "import os\n",
    "\n",
    "#Los embeddings se pueden leer con numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512dc05-3dfa-4892-8778-938211136968",
   "metadata": {},
   "source": [
    "### 1.1 AplicaciÃ³n directa del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9ba985-4708-4c9c-839f-5cb242f5d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre cargamos el modelo, al hacerlo solo una vez no es necesario pre cargar el modelo cada vez que se va a utilizar\n",
    "modelo = openl3.models.load_image_embedding_model(input_repr=\"mel256\", content_type=\"music\", embedding_size=512)\n",
    "\n",
    "#Variable global, de donde obtenemos la ruta de las imÃ¡genes de entrenamiento y prueba\n",
    "ruta_imagenes_train = gf.get_data_path('train_images')\n",
    "ruta_imagenes_test = gf.get_data_path('test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf42efa-e983-4461-87e4-c9c6fcf90ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.175706  ,  1.5434446 ,  1.8470824 ,  1.6111743 ,  3.9059458 ,\n",
       "         1.6161033 ,  1.1251589 ,  1.8484113 ,  1.2820483 ,  1.2422009 ,\n",
       "         1.2345903 ,  0.83172244,  1.2606227 ,  1.6085217 ,  2.310515  ,\n",
       "         1.9807837 ,  2.63912   ,  1.8400848 ,  1.7005421 ,  1.7075837 ,\n",
       "         1.0354801 ,  2.1518202 ,  0.6044424 ,  1.4306686 ,  0.98116654,\n",
       "         0.777962  ,  3.3654976 ,  4.162442  ,  1.9882624 ,  0.7811913 ,\n",
       "         2.5927725 ,  1.8348336 ,  1.7911009 ,  1.8612864 ,  2.2643867 ,\n",
       "         2.5106506 ,  1.129749  ,  0.7803635 ,  1.5808517 ,  2.0452437 ,\n",
       "         0.7477303 ,  2.566805  ,  1.2202104 ,  2.673956  ,  1.3030437 ,\n",
       "         0.9613706 ,  1.4589942 ,  1.1933473 ,  1.6517575 ,  1.4095986 ,\n",
       "         1.3867158 ,  1.8570193 ,  3.5165267 ,  1.0719959 ,  0.7293594 ,\n",
       "         2.3112679 ,  0.84064364,  2.1612198 ,  3.0060468 ,  1.9224309 ,\n",
       "         1.1812272 ,  1.891209  ,  2.472405  ,  1.2888657 ,  1.6927787 ,\n",
       "         2.1506999 ,  1.3459386 ,  2.0038981 ,  1.5930213 ,  0.47655213,\n",
       "         2.6193697 ,  1.9717464 ,  4.062987  ,  1.1046907 ,  1.1110872 ,\n",
       "         2.361168  ,  2.197923  ,  2.2356052 ,  1.5180908 ,  1.4714471 ,\n",
       "         0.7321862 ,  2.3340888 ,  2.2026641 ,  2.900152  ,  1.8455327 ,\n",
       "         2.8180366 ,  3.579984  ,  1.7780606 ,  0.6885492 ,  0.74330634,\n",
       "         1.4548118 ,  1.7892987 ,  2.623982  ,  2.2822454 ,  2.6337693 ,\n",
       "         1.810327  ,  2.0195496 ,  1.8318466 ,  0.5800682 ,  1.9717218 ,\n",
       "         1.1088709 ,  2.4765005 ,  1.3142545 ,  2.3426805 ,  3.3546166 ,\n",
       "         1.8336228 ,  1.4230171 ,  1.3460764 ,  1.2419913 ,  1.6326134 ,\n",
       "         1.8541243 ,  3.3094888 ,  3.1520083 ,  1.1276817 ,  2.1059825 ,\n",
       "         1.4034028 ,  2.2198758 ,  1.3768513 ,  2.19128   ,  0.87318957,\n",
       "         1.9403172 ,  2.2162051 ,  2.1737297 ,  1.9482977 ,  3.2339306 ,\n",
       "         2.4979992 ,  3.8708446 ,  0.30502355,  1.1382095 ,  1.8605169 ,\n",
       "         2.1578095 ,  0.5733128 ,  0.98379403,  0.8280359 ,  0.8086305 ,\n",
       "         2.3863525 ,  2.9791481 ,  1.9045479 ,  1.9796925 ,  1.8616962 ,\n",
       "         3.3632848 ,  1.8634696 ,  1.107185  ,  1.4592223 ,  1.6343129 ,\n",
       "         1.3946321 ,  1.3572518 ,  0.35844022,  1.2527131 ,  3.150411  ,\n",
       "         1.6096046 ,  1.1554693 ,  2.1358314 ,  1.7029598 ,  1.141921  ,\n",
       "         1.6153053 ,  1.6055027 ,  3.1051896 ,  3.6214821 ,  1.6230621 ,\n",
       "         1.5834004 ,  1.6298965 ,  2.8244247 ,  1.1675341 ,  2.7904487 ,\n",
       "         1.7303566 ,  2.130632  ,  0.6963417 ,  1.751126  ,  1.3575698 ,\n",
       "         2.9695826 ,  1.3795626 ,  2.0887012 ,  2.9579601 ,  1.0759045 ,\n",
       "         0.6480726 ,  1.1689644 ,  2.195088  ,  0.7771737 ,  1.8708503 ,\n",
       "         1.973008  ,  2.593222  ,  1.3974954 ,  3.137685  ,  1.4733729 ,\n",
       "         2.9118552 ,  0.6655876 ,  1.1402754 ,  1.6189083 ,  2.1220033 ,\n",
       "         1.8366833 ,  1.8314385 ,  2.2287154 ,  3.9584134 ,  1.6328561 ,\n",
       "         2.620542  ,  1.4777068 ,  0.15441383,  1.1240487 ,  3.3462892 ,\n",
       "         1.7969214 ,  1.556514  ,  2.4305325 ,  0.8590141 ,  1.6037987 ,\n",
       "         2.3268068 ,  1.0716914 ,  2.353262  ,  0.7552768 ,  2.2437394 ,\n",
       "         1.8851045 ,  1.3443502 ,  1.2537194 ,  0.95854366,  1.3386508 ,\n",
       "         0.98735577,  2.4222198 ,  1.471299  ,  0.40153554,  0.9825933 ,\n",
       "         1.8274686 ,  2.1684446 ,  1.9626218 ,  3.0763166 ,  2.1571612 ,\n",
       "         1.7695723 ,  2.6577256 ,  1.9825742 ,  2.0900128 ,  1.4026421 ,\n",
       "         1.4734573 ,  1.1778079 ,  1.3406878 ,  1.0191549 ,  2.4203165 ,\n",
       "         0.9642346 ,  1.3775394 ,  1.2101616 ,  2.7640269 ,  2.3669791 ,\n",
       "         0.7761775 ,  2.118767  ,  1.8146111 ,  1.7414246 ,  1.2149911 ,\n",
       "         1.6906894 ,  1.9912071 ,  4.472443  ,  0.4392021 ,  1.4774317 ,\n",
       "         1.5085039 ,  0.56578064,  2.3826897 ,  1.9044176 ,  1.2921616 ,\n",
       "         0.982777  ,  1.7379091 ,  1.5467534 ,  0.9382145 ,  2.8912523 ,\n",
       "         2.1456141 ,  2.7281427 ,  2.3932753 ,  1.2604761 ,  2.3788674 ,\n",
       "         1.9280934 ,  2.3026574 ,  2.244354  ,  1.2846617 ,  1.1534966 ,\n",
       "         2.1021774 ,  1.657334  ,  1.0025334 ,  2.1684284 ,  1.6966815 ,\n",
       "         3.6313214 ,  1.7694956 ,  2.2314448 ,  0.88858825,  0.8019584 ,\n",
       "         0.42250943,  2.4249673 ,  2.2758982 ,  1.6080158 ,  1.3630352 ,\n",
       "         1.0489814 , -0.0961695 ,  0.0613366 ,  3.0018718 ,  2.657539  ,\n",
       "         1.7546085 ,  2.5809753 ,  2.8700643 ,  1.0275998 ,  2.5966926 ,\n",
       "         1.8877069 ,  0.92376304,  0.8094482 ,  1.3924993 ,  0.9060505 ,\n",
       "         0.7375886 ,  1.5616958 ,  0.9408794 ,  1.807155  ,  1.5401651 ,\n",
       "         2.1586545 ,  1.2999291 ,  1.3566748 ,  0.8188017 ,  0.20982033,\n",
       "         1.4128046 ,  2.732088  ,  2.1413565 ,  0.877346  ,  2.5168564 ,\n",
       "         2.8613496 ,  1.3230908 ,  1.6020036 ,  1.7247399 ,  1.008273  ,\n",
       "         1.827462  ,  3.3676603 ,  1.7086353 ,  0.81798834,  0.38888142,\n",
       "         1.4026177 ,  0.32413113,  1.3097413 ,  1.0313536 ,  1.7172253 ,\n",
       "         2.3557925 ,  1.3111204 ,  1.3517301 ,  0.45486632,  1.5576808 ,\n",
       "         2.4745257 ,  1.9757316 ,  1.4509815 ,  2.2598398 ,  0.9587231 ,\n",
       "         1.054393  ,  0.22588788,  1.3125238 ,  1.3522398 ,  1.6951782 ,\n",
       "         3.7570148 ,  2.4094355 ,  2.3221521 ,  0.7649872 ,  0.41615215,\n",
       "         2.2115865 ,  2.4118917 ,  2.3731947 ,  1.2105519 ,  0.14909582,\n",
       "         2.1226873 ,  1.071508  ,  1.3133154 ,  0.13713017,  1.3183657 ,\n",
       "         2.7107043 ,  3.4891021 ,  1.7466547 ,  2.1628833 ,  1.3982184 ,\n",
       "         0.69732356,  3.354535  ,  1.4641207 ,  1.2138011 ,  1.1258827 ,\n",
       "         2.3859258 ,  2.208928  ,  3.1228552 ,  0.7562111 ,  2.751453  ,\n",
       "         2.0329013 ,  2.5957615 ,  1.6002452 ,  1.8631349 ,  1.2944238 ,\n",
       "         4.089752  ,  1.2022967 ,  1.8000019 ,  3.009405  ,  0.61956596,\n",
       "         1.9118965 ,  1.7915144 ,  0.7789993 ,  0.9837974 ,  1.3568966 ,\n",
       "         0.3999674 ,  1.9177532 ,  0.95263   ,  2.9284015 ,  1.149992  ,\n",
       "         2.1916583 ,  1.2177802 ,  1.5857089 ,  2.1940875 ,  0.44687366,\n",
       "         2.3858454 ,  3.9504175 ,  0.5655163 ,  2.6389308 ,  0.15950087,\n",
       "         2.1763046 ,  0.97411096,  0.27542314,  1.9027846 ,  0.509019  ,\n",
       "         1.4849098 ,  1.5165368 ,  0.02513056,  1.8518919 ,  1.4084322 ,\n",
       "         2.115561  ,  3.5317254 ,  0.74647844,  1.6053514 ,  1.0835618 ,\n",
       "         1.3358855 ,  0.8422781 ,  1.0444161 ,  1.0711006 ,  3.2195356 ,\n",
       "         1.7400088 ,  3.472991  ,  2.0065453 ,  2.3838098 ,  1.6747029 ,\n",
       "         1.0819947 ,  2.4989796 ,  0.9762982 ,  0.6131284 ,  1.9837892 ,\n",
       "         1.5807332 ,  1.2274867 ,  1.3495158 ,  2.3833666 ,  2.5631557 ,\n",
       "         1.5764227 ,  2.9738996 ,  0.93225515,  2.347555  ,  0.77388114,\n",
       "         2.2777517 ,  2.3635151 ,  1.9934646 ,  3.1203682 ,  1.4682986 ,\n",
       "         1.6839352 ,  2.1998355 ,  0.62159514,  3.8899078 ,  1.1311991 ,\n",
       "         0.533277  ,  1.5396821 ,  1.8221093 ,  1.110822  ,  0.28853527,\n",
       "         1.3727562 ,  1.1685401 ,  1.3225605 ,  0.8580937 ,  1.8518877 ,\n",
       "         1.3909699 ,  3.2519639 ,  2.1847272 ,  0.913566  ,  0.7795896 ,\n",
       "         1.2857395 ,  1.1553231 ,  1.0589856 ,  1.4379913 ,  2.7278197 ,\n",
       "         1.6744288 ,  1.1484152 ,  0.85729074,  1.5635169 ,  1.4469551 ,\n",
       "         1.3510762 ,  2.0154874 ,  1.0346603 ,  2.4419563 ,  1.469019  ,\n",
       "         2.9397585 ,  1.2478424 ,  0.93469393,  0.9532465 ,  1.6992624 ,\n",
       "         2.7834325 ,  0.32698208,  1.1154722 ,  3.0087442 ,  1.8318268 ,\n",
       "         1.8996798 ,  1.71941   ,  0.7863625 ,  1.6181277 ,  1.6955439 ,\n",
       "         1.9642757 ,  1.7384559 ,  1.752031  ,  2.4207637 ,  2.0373354 ,\n",
       "         1.7210752 ,  1.866144  ,  1.0500181 ,  1.8593454 ,  1.6727269 ,\n",
       "         2.3892353 ,  1.1329664 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MÃ©todo para generar los embeddings de manera directa a una sola imagen\n",
    "\n",
    "#Nombre de la imagen a la cual aplicaremos el modelo\n",
    "imagen_name = '993123.jpeg'\n",
    "\n",
    "#Leemos la imagen\n",
    "imagen1 = imread(ruta_imagenes_train + imagen_name)\n",
    "#Generamos el embedding de la imagen de manera directa\n",
    "emb = openl3.get_image_embedding(imagen1, content_type=\"env\", input_repr=\"linear\", embedding_size=512)\n",
    "\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49b5a1-03cb-46de-bb64-dea1ea37768c",
   "metadata": {},
   "source": [
    "### 1.2 Guardar Embedding para uso futuro (Varisas imÃ¡genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd6eab2-7011-44bf-9a3f-66677a1ca423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/998892.jpeg (1/2)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/998892.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/994535.jpeg (2/2)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/994535.npz exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "#AplicaciÃ³n del modelo a mÃ¡s de una imagen\n",
    "\n",
    "#Rutas finales de las imÃ¡genes a procesar\n",
    "\n",
    "imagen2 = ruta_imagenes_train + '998892.jpeg'\n",
    "imagen3 = ruta_imagenes_train + '994535.jpeg'\n",
    "#Lista para guardar todas las imÃ¡genes\n",
    "imagen_array = [imagen2, imagen3]\n",
    "#MÃ©todo para guardar los embeddings de cada imagen en la misma carpeta de donde vienen las imÃ¡genes\n",
    "openl3.process_image_file(imagen_array, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c128fd0a-7177-46bf-b751-598276d30784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17171964,  2.7841794 ,  0.58611256, ...,  0.55718577,\n",
       "        1.9009621 ,  1.2161709 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#En este mÃ©todo se leerÃ¡n los embeddings de las imÃ¡genes generadas en la secciÃ³n anterior\n",
    "\n",
    "#Cargamos la data (embedding) de la imagen especificada\n",
    "data = np.load(ruta_imagenes_train + '998892.npz')\n",
    "#Obtenemos solo el embedding\n",
    "emb = data['embedding']\n",
    "\n",
    "emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f15cd4-5a29-450a-86e7-23b19270e59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/174383279.jpeg (1/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/174383279.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194747578.jpeg (2/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194747578.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/196588153.jpeg (3/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/196588153.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/8324721.jpeg (4/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/8324721.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/179983287.jpeg (5/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/179983287.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/56516675.jpeg (6/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/56516675.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194269576.jpeg (7/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194269576.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/64653712.jpeg (8/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/64653712.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/195871735.jpeg (9/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/195871735.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/188616414.jpeg (10/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/188616414.npz exists, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.47553322,  2.9182515 , -0.14319089, ...,  0.74044186,\n",
       "        2.1243396 , -0.49356037], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#En esta secciÃ³n se aplica el embedding a 10 imÃ¡genes distintas dentro de la carpeta de train images de manera directa\n",
    "\n",
    "#Obtenemos la lista con los nombres (id's) de las imÃ¡genes de entrenamiento\n",
    "train_file_names = os.listdir(ruta_imagenes_train)\n",
    "#Nos quedamos solo con las primeras 10 imÃ¡genes\n",
    "train_file_names = train_file_names[:10]\n",
    "\n",
    "#Concatenamos el resto de la ruta del archivo al nombre de cada imagen\n",
    "train_complete_file_names = [ruta_imagenes_train + x for x in train_file_names]\n",
    "\n",
    "#MÃ©todo para guardar los embeddings de cada imagen en la misma carpeta de donde vienen las imÃ¡genes\n",
    "openl3.process_image_file(train_complete_file_names, batch_size = 32)\n",
    "\n",
    "#Empezamos con el proceso de cargar los embeddings de cada imagen\n",
    "\n",
    "#Lista en donde se guardarÃ¡ cada embedding\n",
    "embs_files = list()\n",
    "#Loop para ir guardando cada embedding\n",
    "for imagen in train_file_names:\n",
    "    #Cargamos la data (embedding) de la imagen especificada\n",
    "    data = np.load(ruta_imagenes_train + imagen[:imagen.find('.')] + '.npz')\n",
    "    #Obtenemos solo el embedding\n",
    "    embs_files.append(data['embedding'][0])\n",
    "\n",
    "embs_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d10175-9b11-4e7e-8060-964125f85bff",
   "metadata": {},
   "source": [
    "### 1.3 Uso del modelo pre cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86c25a7-d2d9-43cf-a914-bc5f83688ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.20485121,  2.7495792 ,  1.1525512 ,  1.6354275 ,  1.5630054 ,\n",
       "         3.0043402 ,  1.1680847 ,  0.20437273,  0.47765186,  1.6069542 ,\n",
       "         0.99042135,  2.744746  ,  3.0113118 ,  0.697831  ,  2.5043488 ,\n",
       "         2.8977034 ,  2.1103582 ,  1.0919865 ,  2.5187943 ,  2.6752896 ,\n",
       "         2.5137546 ,  0.80452126,  0.9297262 ,  2.2770233 ,  2.8381968 ,\n",
       "         0.6253177 ,  0.7834051 ,  1.436738  ,  1.1495699 ,  1.3403784 ,\n",
       "         1.7564527 ,  1.0250019 ,  2.2259736 ,  0.5685906 ,  2.5769222 ,\n",
       "         0.8931727 ,  2.3390589 ,  1.2697175 ,  1.2542069 ,  1.3876815 ,\n",
       "         1.3700166 ,  1.7157243 ,  0.76253283,  1.8189112 ,  0.24554229,\n",
       "         1.335274  ,  1.7735906 ,  1.3587192 ,  1.4913703 ,  1.041074  ,\n",
       "         0.53341097,  0.9961289 ,  0.8008581 ,  1.6766714 ,  1.8453351 ,\n",
       "         1.4003036 ,  1.7122384 ,  1.1727496 ,  1.851693  ,  2.0431597 ,\n",
       "         2.2497199 ,  1.0162674 ,  2.1898563 ,  1.1334101 ,  0.64272827,\n",
       "         1.9252778 ,  2.6810825 ,  1.1118598 ,  2.4728882 ,  2.0378585 ,\n",
       "         0.18904294,  0.79025114,  2.4218001 ,  2.8007655 ,  1.5280496 ,\n",
       "         1.5474136 ,  1.9214152 ,  1.3252783 ,  1.7291185 ,  1.6519809 ,\n",
       "         1.6730304 ,  0.71860415,  1.2508231 ,  1.5200086 ,  2.0593126 ,\n",
       "         1.9230796 ,  1.7712485 ,  3.1574547 ,  1.2433559 ,  2.713454  ,\n",
       "         1.4150548 ,  0.8275679 ,  0.17761555,  1.4295996 ,  0.402724  ,\n",
       "         3.7560945 ,  2.9716074 ,  2.1054938 ,  1.4818022 ,  2.4419835 ,\n",
       "         1.5843626 ,  1.7393502 ,  1.9626493 ,  1.830607  ,  0.7150854 ,\n",
       "         1.1647476 ,  1.1310599 ,  1.9774538 ,  1.4355531 ,  1.5156869 ,\n",
       "         1.5979875 ,  0.5193798 ,  1.9295925 ,  3.1267235 ,  0.37254888,\n",
       "         1.7504405 ,  2.0851407 ,  1.2470307 ,  1.0443157 , -0.09026211,\n",
       "         3.181277  ,  0.9165494 ,  1.2001781 ,  2.5041108 ,  0.9305678 ,\n",
       "         0.09242424,  1.0307933 ,  1.8854965 ,  1.5079899 ,  1.8392226 ,\n",
       "         1.8071203 ,  0.6490128 ,  1.5491434 ,  2.1097796 ,  0.8568534 ,\n",
       "         1.0614096 ,  1.536929  ,  1.2888623 ,  2.3350706 ,  0.6976912 ,\n",
       "         0.8105583 ,  1.9423215 ,  0.6261319 ,  2.3393662 ,  1.6619316 ,\n",
       "         2.0141628 ,  1.1772301 ,  0.92156947,  1.8653541 ,  1.2902074 ,\n",
       "         2.5316038 ,  0.24059922,  2.2795978 ,  1.9879104 ,  1.425057  ,\n",
       "         0.92885405,  1.5962604 ,  1.719163  ,  0.79301786,  2.2725556 ,\n",
       "         1.0584172 ,  1.4747607 ,  2.0802364 ,  0.7287637 ,  0.02222875,\n",
       "         0.68939507,  1.3485879 ,  0.55696934,  1.7018901 ,  1.2952051 ,\n",
       "         0.9304137 ,  0.71346456,  1.0721709 ,  2.3936746 ,  1.3517103 ,\n",
       "         1.8400532 ,  3.0535743 ,  1.5248033 ,  2.1646209 ,  2.8433282 ,\n",
       "         0.7472565 ,  1.3872107 ,  1.4184986 ,  1.7283857 ,  1.9101846 ,\n",
       "         3.4059415 ,  1.0183837 ,  1.2752259 ,  0.52478236,  1.4572835 ,\n",
       "         1.8859854 ,  1.1009525 ,  2.018782  ,  2.5567327 ,  2.2113345 ,\n",
       "         1.599143  ,  1.7910511 ,  0.9189052 ,  1.5837723 ,  1.4791529 ,\n",
       "         1.2947991 ,  2.3006215 ,  2.7331173 ,  1.2402371 ,  0.98130715,\n",
       "         1.3301815 ,  1.9842585 ,  1.447342  ,  1.2027946 ,  1.9843901 ,\n",
       "         2.4563801 ,  1.3951555 ,  0.72004324,  1.7766222 ,  1.9360775 ,\n",
       "         3.3049457 ,  0.8194702 ,  1.4591975 ,  1.3551744 ,  2.109625  ,\n",
       "         2.322318  ,  2.5884986 ,  2.1426508 ,  0.669836  ,  3.2447402 ,\n",
       "         1.3465672 ,  1.6958401 ,  1.9680761 ,  1.6791688 ,  2.2260683 ,\n",
       "         0.70770514,  2.1221418 ,  1.3575913 ,  0.82252675,  1.5782276 ,\n",
       "         1.5124403 ,  0.96351004,  1.4934953 ,  1.4218223 ,  2.134638  ,\n",
       "         1.7454535 ,  2.6233408 ,  1.7791142 ,  2.0155365 ,  2.0383744 ,\n",
       "         1.2378262 ,  1.0985494 ,  1.5937773 ,  1.8554164 ,  0.17792201,\n",
       "         0.53817767,  0.98444164,  1.4498764 ,  1.3323634 ,  2.3908966 ,\n",
       "         1.3856658 ,  1.3186507 ,  0.9486842 ,  3.846665  ,  1.5070676 ,\n",
       "         0.33823866,  0.7877687 , -0.16274174,  1.0884465 ,  1.9007766 ,\n",
       "         2.5469697 ,  0.66021   ,  1.5684887 ,  1.2924713 ,  0.63552445,\n",
       "         1.5935831 ,  1.4875512 ,  1.5960518 ,  0.8582631 ,  2.1310747 ,\n",
       "         1.0317056 ,  1.0481809 ,  1.8717718 ,  2.0543463 ,  1.1496912 ,\n",
       "         2.888617  ,  1.7371292 ,  2.2067773 ,  3.1110735 ,  1.0411993 ,\n",
       "         0.27202043,  0.9156175 ,  0.5618163 ,  1.6236361 ,  1.6475532 ,\n",
       "         2.3125062 ,  0.49094838,  1.1980839 ,  1.3158065 ,  3.740868  ,\n",
       "         1.9677469 ,  1.0197357 ,  1.1762849 ,  0.7334433 ,  0.34872186,\n",
       "         0.7972447 ,  1.5106252 ,  2.366034  ,  2.192229  ,  1.6929114 ,\n",
       "         1.4890344 ,  3.0548801 ,  1.0835251 ,  2.1524706 ,  0.96201026,\n",
       "         1.0682323 ,  2.3660114 ,  1.1081941 ,  0.83726406,  1.2536136 ,\n",
       "         2.2398672 ,  2.112317  ,  2.3908331 ,  0.47984797,  2.062965  ,\n",
       "         0.38893068,  1.8647356 ,  1.670881  ,  0.6539917 ,  1.2611164 ,\n",
       "         1.5717337 ,  0.50576305,  2.5663433 ,  1.8686793 ,  2.172477  ,\n",
       "         1.0179551 ,  1.0023283 ,  1.2324402 ,  1.3330743 ,  1.8540252 ,\n",
       "         2.8859663 ,  1.2206006 ,  2.1910655 ,  0.19158404,  1.1950855 ,\n",
       "         1.1625215 ,  0.5881799 ,  1.9052645 ,  0.8708216 ,  1.4036076 ,\n",
       "         0.9691802 ,  0.95226324,  1.4744202 ,  1.4248235 ,  1.2367972 ,\n",
       "         2.808134  ,  2.4231102 ,  0.5292709 ,  2.915683  ,  0.8944221 ,\n",
       "         1.0192145 ,  3.3238676 ,  1.347873  ,  1.9790424 ,  0.4238795 ,\n",
       "         1.3618789 ,  2.133664  ,  1.2368494 ,  1.7652725 ,  1.6785655 ,\n",
       "         1.1529132 ,  3.577917  ,  1.3276899 ,  3.103002  ,  1.8847266 ,\n",
       "         0.86568314,  1.7473139 ,  2.0703397 ,  1.0410285 ,  1.8536546 ,\n",
       "         1.6400666 ,  0.85971665,  0.5454826 ,  1.9495049 ,  1.6841565 ,\n",
       "         2.3934028 ,  2.4245036 ,  1.7368637 ,  1.1532226 ,  3.5255368 ,\n",
       "         2.6359017 ,  2.0186172 ,  0.97741675,  3.3602288 ,  1.6372435 ,\n",
       "         1.0248289 ,  1.292603  ,  0.53276104,  2.1690738 ,  0.18544208,\n",
       "         1.077914  ,  1.9338853 ,  3.343952  ,  1.8495357 ,  2.0533946 ,\n",
       "         2.0159342 ,  1.9051903 ,  2.091736  ,  1.2920363 ,  1.8911469 ,\n",
       "         2.4060938 ,  2.1457841 ,  0.90916306,  2.5142744 ,  1.8741465 ,\n",
       "         1.0483673 ,  1.9130058 ,  1.5597029 ,  1.6793998 ,  0.8772772 ,\n",
       "         0.43882233,  2.4843915 ,  3.2221475 ,  1.9798054 ,  1.8952355 ,\n",
       "         1.946561  ,  1.1553272 ,  1.1124197 ,  0.7784294 ,  0.44111228,\n",
       "         1.2199454 ,  0.80838984,  1.7045665 ,  1.4436787 ,  0.6235423 ,\n",
       "         1.405282  ,  0.7928036 ,  2.8732536 ,  1.6124893 ,  1.6760724 ,\n",
       "         0.7731418 ,  1.2888249 ,  1.5158899 ,  1.500979  ,  0.69608504,\n",
       "         2.3611884 ,  1.0385586 ,  2.2581875 ,  1.2921201 ,  2.0748973 ,\n",
       "         1.8155406 ,  2.4921267 ,  2.6781292 ,  1.3267325 ,  2.2051191 ,\n",
       "         0.9098008 ,  0.9012551 ,  0.12941791,  2.409787  ,  0.6900594 ,\n",
       "         2.1228986 ,  2.2754078 ,  1.1853325 ,  0.4757692 ,  1.8257791 ,\n",
       "         0.93363845,  0.8266524 ,  1.4521326 ,  1.8899059 ,  1.731703  ,\n",
       "         1.3749826 ,  2.3283339 ,  1.0289295 ,  0.7492265 ,  2.640315  ,\n",
       "         0.99254185,  1.5912545 ,  1.7530429 ,  1.5952451 ,  0.78447586,\n",
       "         2.51337   ,  2.1093833 ,  1.606523  ,  1.1113402 ,  0.39814588,\n",
       "         1.3742307 ,  0.92801857,  2.9369183 ,  1.7763023 ,  1.0433584 ,\n",
       "         0.09030688,  0.3081057 ,  1.561068  ,  1.2821492 ,  1.9407334 ,\n",
       "         2.2916896 ,  1.6688268 ,  0.95677584,  2.0200806 ,  1.8097007 ,\n",
       "         0.8366819 , -0.2874099 ,  1.8230293 ,  1.1122035 ,  2.466584  ,\n",
       "         3.9499621 ,  1.5631782 ,  1.0250038 ,  1.4679947 ,  2.22061   ,\n",
       "         0.03978098,  3.3677886 ,  1.2276036 ,  2.0392818 ,  1.563246  ,\n",
       "         1.9572912 ,  1.6245346 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MÃ©todo en el cual generamos el embedding de la imagen ya con el modelo pre cargado\n",
    "\n",
    "#Obtenemos el embedding final\n",
    "emb = openl3.get_image_embedding(imagen1, model=modelo)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9465fa9-0af9-4ea4-bdbd-eca5ea6be2ee",
   "metadata": {},
   "source": [
    "## Modelo 2: ViT (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be12689-b62f-4b51-af77-dd7016b27707",
   "metadata": {},
   "source": [
    "[GitHub](https://github.com/lukemelas/PyTorch-Pretrained-ViT?tab=readme-ov-file) , [Colab](https://colab.research.google.com/drive/1muZ4QFgVfwALgqmrfOkp7trAvqDemckO?usp=sharing)\n",
    "\n",
    "Papers de referencia:\n",
    "\n",
    "- [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://openreview.net/pdf?id=YicbFdNTTy)\n",
    "\n",
    "- [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)\n",
    " \n",
    "- [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/pdf/2106.10270)\n",
    "   \n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v7)\n",
    "\n",
    "Proceso general del modelo:\n",
    "\n",
    "1. Considera una imagen como una secuencia mono-dimensional (1 dimensiÃ³n) de parches (token embeddings)\n",
    "  \n",
    "2. Antepone un token de clasificaciÃ³n a la secuencia generada en el punto 1\n",
    "\n",
    "3. Pone estos parches a travÃ©s de un codificador transformador (como BERT)\n",
    "\n",
    "4. Pasa el primer token del output generado por el transformador a travÃ©s de un pequeÃ±o MLP (Multilayer Perceptron) para obtener los logits de clasificaciÃ³n. Deja una capa escondida para pre-entrenamiento y una sola capa lineal para el fine-tunning\n",
    "\n",
    "Puntos sobre la lectura:\n",
    "\n",
    "- Se divide una imagen en parches y provee la secuencia de embeddings lineales de esos parches como entrada a un Transformador\n",
    "\n",
    "- Los parches son tratados como Tokens (palabras) en una aplicaciÃ³n NLP\n",
    "\n",
    "- El encoder Transformer consiste en alternar capas de multiheaded self-attention (MSA) y bloques MLP\n",
    "\n",
    "- En este modelo existen dos tipos de capas: las capas MLP y MSA\n",
    "\n",
    "- La longitud de la secuencia del Transformador es inversamente proporcional al cuadrado del tamaÃ±o del parche, por lo que modelos con tamaÃ±os pequeÃ±os de parches son computacionalmente mÃ¡s caros\n",
    "\n",
    "- AquÃ­ tambiÃ©n se utiliza el optmizador Adam (la otra alternativa es SGD), es un optimizador estocÃ¡stico. TambiÃ©n existe el optmizador RAdam que brinda mejor control en la varianza del gradiente, el cual es necesario cuando el modelo se entrena a un valor alto de LR; RAdam detecta inestabilidad en la varianza y cambia el LR de manera suave para evitar la divergencia en las etapas iniciales del entrenamiento.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation)**: Combina el optimizador RMSProp (Root Mean Square Propagation) con el Momentum. $\\beta_1$ es el factor de olvido del gradiente y $\\beta_2$ es el factor de olvido del segundo momento del gradiente\n",
    "\n",
    "- **FLOPs (Floating Point Operations)**: MÃ©tricas que se utilizan comÃºnmente para calcular la complejidad computacional de los modelos de deep learning, sirve para entender fÃ¡cilmente el nÃºmero de operaciones aritmÃ©ticas requeridas para realizar cierta operacion computacional. NÃºmero de operaciones de punto flotante (suma, resta, multiplicaciÃ³n y divisiÃ³n en nÃºmeros de punto flotante). Al optimizar esta mÃ©trica podemos reducir la energÃ­a requerida para correr nuestra red neuronal [Ref 1](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html) [Ref 2](https://medium.com/@pashashaik/a-guide-to-hand-calculating-flops-and-macs-fa5221ce5ccc)\n",
    "\n",
    "- **EPOCH**: Se refiere al pase entero del conjunto de datos de entrenamientoa travÃ©s de los algoritmos. Es un hiper-parametro que determina el proceso de entrenar el modelo entero de ML. Es cuando pasas TODO tu conjunto de entrenamiento a travÃ©s del modelo y entonces se define como el nÃºmero total de iteraciones que tuvo todo tu conjunto de entrenamiento para entrenar el modelo de ML, un \"paso\" de la data de entrenamiento se toma en cuenta cuando Ã©sta haya pasado tanto por delante como por detrÃ¡s en todos los procesos del modelo. Un epoch estÃ¡ completo cuando ha procesado todos los _batches_ y ha actualizado sus parÃ¡metros basados en el cÃ¡lculo de pÃ©rdida. De manera general, incrementando el nÃºmero de epochs mejora el performance del modelo al permitirle aprender comportamientos mÃ¡s complejos en la data, pero si existen muchos epochs, el modelo se sobre-entrena. IteraciÃ³n completa: Procesar cada batch de la data de entrenamiento, calcular la pÃ©rdida y actualizar los parÃ¡metros del modelo. Ayuda a procesar o entrenar el modelo con un conjunto enorme de datos, si se tiene un conjunto enorme de datos y no es posible entrenar todos los datos de una sola vez (debido a memoria), los epochs te permiten entrenar el modelo en mini batches independientes que se realiza uno por uno. [Ref 1](https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-epoch-in-machine-learning#:~:text=Machine%20learning%20models%20are%20trained,training%20data%20through%20the%20algorithm.) [Ref 2](https://www.geeksforgeeks.org/epoch-in-machine-learning/)\n",
    "\n",
    "- **LEARNING RATE WARMUP**: Proceso para mejorar la optimizaciÃ³n del modelo; incrementa gradualmente la taza (rate) de aprendizaje (tamaÃ±o del paso tomado en cada iteraciÃ³n para actualizar los parÃ¡metros del modelo) durante las etapas iniciales del entrenamiento antes de iniciar con las tazas de aprendizaje programadas; se incrementa linealmente la taza de aprendizaje durante las iteraciones iniciales y despuÃ©s se continua con las tazas programadas de aprendizaje. Se necesita ajustar los parÃ¡metros del calentamiento (warm-up) basado en caracterÃ­sticas especÃ­ficas del modelo y conjunto de datos. [Ref 1](https://techkluster.com/technology/learning-rate-warm-up/#:~:text=Learning%20rate%20warm%2Dup%20involves,how%20it%20can%20be%20implemented.) [Ref 2](https://www.baeldung.com/cs/learning-rate-warm-up)\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent)**: MÃ©todo iterativo para optimizar la funciÃ³n objetivo con propiedades de suavidad adecuadas (diferenciabilidad, sub-diferenciabilidad), puede ser tomado como la aproximaciÃ³n estocÃ¡stica de la tÃ©cnica gradiente descendiente, ya que reemplaza el actual gradiente por otro gradiente estimado a partir de un subconjunto aleatorio de la data. AquÃ­, dentro del proceso de optimizaciÃ³n de la funciÃ³n objetivo, en vez de calcular el verdadero gradiente de la funciÃ³n, lo que se hace es estimar ese gradiente con el gradiente de un solo registro o muestra, despuÃ©s actualiza este valor para cada entrada (muestra) del conjunto de entrenamiento hasta que se logra la conversiÃ³n. AquÃ­ se beneficia el costo computacional por iteraciÃ³n, aunque de manera general toma un nÃºmero mÃ¡s alto de iteraciones que el mÃ©todo tradicional de gradiente descendiente. Este mÃ©todo es mejor para casos en donde el conjunto de datos es grande. Una elecciÃ³n de un valor alto de LR puede llevar a que el modelo tome pasos grandes en cada iteraciÃ³n y \"pase\" de largo el valor mÃ­nimo y una elecciÃ³n de LR baja puede ocasionar que el modelo converga lentamente. [Ref 1](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) [Ref 2](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/) [Ref 3](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31) [Ref 4](https://scikit-learn.org/stable/modules/sgd.html)\n",
    "\n",
    "- fine-tunning con SGD con momentum de 0.9, el fine-tunning general del modelo se hace con una resoluciÃ³n de 384 (a menos que se estipule otra cosa). EL Vision Transformer se beneficia con mejores resoluciones.\n",
    "\n",
    "- SGD con momentum recuerda la actualizaciÃ³n del cambio en el parÃ¡metro w que minimiza la funciÃ³n objetivo en cada iteraciÃ³n y determina la sig. actualizaciÃ³n del parÃ¡metro como una combinaciÃ³n lineal del gradiente y la actualizaciÃ³n anterior. Utiliza un factor de decadencia entre 0 y 1 que determina la contribuciÃ³n relativa del actual gradiente y gradientes anteriores con respecto al cambio en los pesos. \n",
    "\n",
    "RÃ¡pida descripciÃ³n de la inspecciÃ³n interna de ViT:\n",
    "\n",
    "1. La primera capa de ViT proyecta linealmente los parches aplanados a un espacio con dimensiÃ³n menor\n",
    "\n",
    "2. Se agrega una posiciÃ³n de embedding a cada parche. El modelo aprende a calcular la distancia (similitud) entre las imagenes con respecto a su posiciÃ³n de embedding; parches cercanos tienden a tener posiciones de embeddings mÃ¡s similares. Esta distancia es de coseno entre la posiciÃ³n del embedding del parche de un determinado renglÃ³n y columna y la posiciÃ³n del embedding de los demÃ¡s parches. Parches que se encuentran en la misma columna o renglÃ³n tienen embeddings similares\n",
    "\n",
    "3. Una estructura sinusoidal (de tipo seno) se utiliza tÃ­picamente para mallas grandes\n",
    "\n",
    "4. Self-attention permite a ViT integrar informaciÃ³n a travÃ©s de toda la imagen, incluso en las capas mÃ¡s bajas. Es una distancia que se mide entre el pixel de consulta (query) y los demÃ¡s pixeles, todas con un peso de atenciÃ³n (weight). Mientras la capa sea mÃ¡s baja (profunda), la atenciÃ³n que se le da a la imagen es mayor.\n",
    "\n",
    "5. El modelo atiende a regiones de la imagen que son semÃ¡nticamente relevantes para la clasificaciÃ³n\n",
    "\n",
    "\n",
    "**TRATAR DE REPLICAR ESTOS EJEMPLOS:** [Kaggle Example](https://www.kaggle.com/code/raufmomin/vision-transformer-vit-from-scratch) [Medium Example](https://medium.com/@diego.machado/fine-tuning-vit-for-image-classification-with-hugging-face-48c4be31e367) [Busqueda de Google (referencias)](https://www.google.com/search?q=using+vit+model+with+csv+file&client=safari&sca_esv=41a53a54b5bc3fc7&sca_upv=1&sxsrf=ADLYWIL7bNtclYzSlPXpYVpatUTD_zwpYw%3A1723677174417&source=hp&ei=9jm9Zry5F-7HkPIPiLy8iAo&iflsig=AL9hbdgAAAAAZr1IBha6iJ9JBpkWgf9h1nv91qK1KZIl&ved=0ahUKEwi8jL_tzfWHAxXuI0QIHQgeD6EQ4dUDCBY&uact=5&oq=using+vit+model+with+csv+file&gs_lp=Egdnd3Mtd2l6Ih11c2luZyB2aXQgbW9kZWwgd2l0aCBjc3YgZmlsZTIFECEYoAFIsj9QAFjZPnAAeACQAQCYAZ0BoAG_GKoBBDYuMjK4AQPIAQD4AQGYAhugArIYwgIKECMYgAQYJxiKBcICBBAjGCfCAg4QABiABBixAxiDARiKBcICERAuGIAEGLEDGNEDGIMBGMcBwgILEC4YgAQYsQMYgwHCAgsQABiABBixAxiDAcICBRAAGIAEwgIREC4YgAQYsQMYxwEYjgUYrwHCAggQLhiABBixA8ICCBAAGIAEGLEDwgIFEC4YgATCAg4QLhiABBjHARiOBRivAcICChAAGIAEGLEDGArCAgsQLhiABBjHARivAcICDRAAGIAEGLEDGEYY_wHCAgYQABgWGB7CAgcQABiABBgTwgIJEAAYgAQYExgKwgIIEAAYExgWGB7CAgQQIRgVmAMAkgcEMy4yNKAH734&sclient=gws-wiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f46b1-10d9-42b9-b227-c36c5710dbe7",
   "metadata": {},
   "source": [
    "El siguiente ejemplo se tomÃ³ de [GitHub](https://github.com/lukemelas/PyTorch-Pretrained-ViT?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb8d86c-6edf-4ef0-8623-75b4e5c7bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_vit\n",
      "  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch (from pytorch_pretrained_vit)\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch->pytorch_pretrained_vit)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from torch->pytorch_pretrained_vit) (4.12.2)\n",
      "Collecting sympy (from torch->pytorch_pretrained_vit)\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from torch->pytorch_pretrained_vit) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from torch->pytorch_pretrained_vit) (3.1.4)\n",
      "Collecting fsspec (from torch->pytorch_pretrained_vit)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from jinja2->torch->pytorch_pretrained_vit) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->pytorch_pretrained_vit)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pytorch_pretrained_vit\n",
      "  Building wheel for pytorch_pretrained_vit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorch_pretrained_vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11114 sha256=9f5a18f9e5c23c5849b1b1a6b48aaaf0e5b85aafac1e211526c97205ad4ea5e6\n",
      "  Stored in directory: /Users/pedrovela/Library/Caches/pip/wheels/79/c8/4f/9ad72c6f247a7e6888cc7767db9632675cf82656fffec85518\n",
      "Successfully built pytorch_pretrained_vit\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch, pytorch_pretrained_vit\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 mpmath-1.3.0 pytorch_pretrained_vit-0.0.7 sympy-1.13.2 torch-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\" to /Users/pedrovela/.cache/torch/hub/checkpoints/B_16_imagenet1k.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331M/331M [00:08<00:00, 38.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "#Librerias a importar\n",
    "\n",
    "#!pip3 install pytorch_pretrained_vit\n",
    "\n",
    "from pytorch_pretrained_vit import ViT\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dadbf64-964c-4820-a00e-6802fb515b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n",
      "torch.Size([1, 3, 384, 384])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install torchvision\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from functions import general_functions as gf\n",
    "\n",
    "ruta_imagenes_train = gf.get_data_path('train_images')\n",
    "img_name = ruta_imagenes_train + '993123.jpeg'\n",
    "\n",
    "# Load ViT\n",
    "from pytorch_pretrained_vit import ViT\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load image\n",
    "# NOTE: Assumes an image `img.jpg` exists in the current directory\n",
    "img = transforms.Compose([\n",
    "    transforms.Resize((384, 384)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])(Image.open(img_name)).unsqueeze(0)\n",
    "print(img.shape) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "# Classify\n",
    "with torch.no_grad():\n",
    "    outputs = model(img).squeeze(0)\n",
    "print(outputs.shape)  # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4af481-bb40-4adb-96dd-4be416557bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1168e-02, -3.6647e-01, -8.2826e-02, -5.2003e-03, -6.9432e-01,\n",
       "         6.3112e-01,  1.1538e-01, -9.5176e-01, -2.8889e-01,  1.1204e-01,\n",
       "         9.6513e-01, -3.8003e-01, -2.1552e-01, -6.1709e-01,  8.0917e-01,\n",
       "        -2.5889e-01,  5.4547e-02, -1.1185e+00, -5.8974e-01, -3.2512e-01,\n",
       "        -2.5198e-01,  5.5554e-01, -2.1421e-02,  3.8290e-01, -2.5087e-02,\n",
       "         1.9608e+00,  3.3388e+00,  2.1743e+00,  1.2444e+00,  8.5103e-01,\n",
       "         9.4214e-01,  1.1039e+00,  2.9827e+00, -1.2655e+00,  3.1554e-01,\n",
       "         9.3914e-01,  8.1178e-01,  4.6449e-01,  2.8589e+00,  3.0353e-01,\n",
       "         1.3648e+00,  3.1621e+00,  7.6384e-01,  1.6017e+00,  2.9447e+00,\n",
       "         1.9694e+00,  1.7545e+00,  2.4520e+00, -8.3982e-03,  6.9470e-02,\n",
       "        -5.9563e-01, -4.6310e-01,  3.2046e+00,  2.7494e+00,  2.3458e+00,\n",
       "         1.7325e+00,  2.8559e+00,  1.9144e+00,  9.2152e-01,  2.1196e+00,\n",
       "         1.6230e+00,  9.6469e-01,  1.4489e+00,  1.3848e+00,  1.8677e+00,\n",
       "         6.0899e-01,  1.9174e+00,  2.1616e+00,  2.7669e+00, -9.9660e-01,\n",
       "         3.9333e+00,  1.6711e+00,  2.3127e+00,  3.5069e+00,  2.0710e+00,\n",
       "         1.6983e+00,  1.0906e+00,  2.0403e+00,  3.1204e+00,  2.2113e+00,\n",
       "         1.3856e+00,  1.9282e+00,  1.4429e+00,  5.3731e-01,  3.4887e-01,\n",
       "        -2.4345e-01,  1.3111e+00, -1.1046e+00,  4.2548e-02,  1.0457e+00,\n",
       "         4.3298e-01,  1.3300e-01,  4.2804e-01, -9.9829e-01,  6.1586e-01,\n",
       "         6.7688e-01, -1.7117e+00, -7.2654e-01,  3.1594e-01, -4.0683e-01,\n",
       "         3.2949e-01, -1.0752e+00,  1.6184e+00, -1.2399e+00, -6.2792e-01,\n",
       "        -1.0170e+00, -1.4297e-01, -1.3180e-01,  8.4345e-01, -8.0051e-01,\n",
       "         1.5062e+00,  2.0540e+00, -1.0911e-01,  2.1944e+00,  3.2397e+00,\n",
       "         7.3684e-01, -2.1486e-01, -1.9343e-01,  4.1691e-01,  1.4511e+00,\n",
       "         1.4117e+00,  5.7832e-02, -7.1749e-03, -8.0117e-01,  3.1938e-01,\n",
       "         2.5990e-01,  2.2331e+00, -1.1631e+00, -7.5400e-02, -1.7564e+00,\n",
       "         9.8538e-03, -9.5136e-01, -5.4334e-01,  5.3501e-03, -1.0813e+00,\n",
       "         1.1229e-01, -5.8678e-01, -8.3818e-01,  1.0756e+00,  8.1807e-01,\n",
       "         2.8066e-01,  5.0494e-01,  5.5894e-01,  6.9712e-01, -9.8801e-01,\n",
       "        -2.4797e-01,  3.9322e-01, -2.0777e-01,  2.6230e-02, -7.3113e-01,\n",
       "         4.2627e-01, -1.0014e-01, -8.3944e-01, -7.1207e-01, -1.3300e+00,\n",
       "        -1.0026e+00, -1.0011e+00, -3.5355e-01, -2.7932e-01, -6.3326e-01,\n",
       "        -2.0266e+00, -5.0000e-01,  7.0404e-02, -2.1838e-01, -5.8161e-01,\n",
       "        -2.7019e-01,  4.4338e-01, -9.5197e-01, -8.2223e-02, -9.1261e-01,\n",
       "        -1.6013e+00,  9.7389e-01, -5.9385e-01, -1.1468e+00, -5.5552e-01,\n",
       "        -1.7261e+00, -1.1529e+00, -5.5075e-01,  6.5345e-01, -5.6858e-02,\n",
       "        -1.4461e-01,  3.4404e-01, -1.7313e+00, -1.1631e+00, -1.4553e+00,\n",
       "        -8.3759e-01, -1.4087e+00, -7.5500e-01, -1.0823e+00, -1.1800e+00,\n",
       "        -1.3814e+00, -1.3580e+00, -1.1169e+00, -1.0051e+00, -7.8645e-01,\n",
       "         4.0795e-01, -8.8525e-01, -2.0180e+00, -1.8984e+00, -1.5952e+00,\n",
       "        -1.8436e+00, -8.4293e-01, -2.2927e+00, -8.8634e-01, -1.0539e+00,\n",
       "        -1.0166e+00, -7.7361e-01, -1.1645e+00, -5.4145e-02, -9.3864e-01,\n",
       "         2.2528e-01,  4.5755e-01, -1.0382e+00, -1.7036e+00, -2.0323e+00,\n",
       "        -1.1887e+00, -1.7114e+00, -9.9984e-01, -7.3405e-01, -1.9381e+00,\n",
       "        -1.8301e+00, -6.4421e-01, -7.5570e-01, -2.8457e-01, -1.4033e+00,\n",
       "        -1.3007e+00, -1.8446e+00, -1.3030e+00, -1.4239e+00, -1.4555e+00,\n",
       "        -8.3883e-01, -1.4656e+00, -9.3830e-01, -1.3109e+00, -7.3162e-01,\n",
       "        -1.5076e+00, -2.4312e-01,  4.6379e-03, -4.6894e-01, -1.0337e+00,\n",
       "        -6.1888e-02, -3.3572e-01, -3.0150e-01, -4.6039e-01, -1.5975e+00,\n",
       "        -7.4560e-01,  3.8803e-01, -1.1875e+00, -7.2280e-01,  1.3671e-01,\n",
       "         6.4118e-01, -2.0195e-01, -8.4569e-01, -6.0579e-01, -2.5960e-01,\n",
       "        -1.0477e+00, -1.9313e+00, -1.9360e-02, -8.6606e-02, -6.2941e-01,\n",
       "        -7.5817e-01, -5.7888e-01, -7.8532e-01, -2.4488e-01, -3.2668e-01,\n",
       "        -6.2050e-02,  4.1200e-01, -1.0372e+00, -3.7898e-01,  3.2074e-01,\n",
       "         2.1915e-01, -2.2707e-01,  2.2745e-01, -3.5002e-01,  2.9384e-02,\n",
       "        -4.9763e-01, -7.3610e-01,  3.9763e-01,  2.3249e-01,  5.8555e-01,\n",
       "         1.3995e+00, -3.4781e-01, -7.7838e-01, -7.8467e-01, -7.4402e-01,\n",
       "        -3.2940e-01, -3.9144e-01,  3.2016e-01, -1.6993e-01,  1.8216e-01,\n",
       "        -4.5186e-01, -7.0215e-01, -5.4624e-01, -1.3429e-01, -7.4436e-01,\n",
       "        -1.0177e+00, -5.1993e-01, -1.1596e+00,  3.5700e-01,  7.1961e-01,\n",
       "         3.5158e+00,  3.1373e+00,  3.2926e+00,  2.8622e+00,  2.0885e+00,\n",
       "         3.4101e+00,  1.4803e+00,  1.8022e+00,  1.4543e+00,  2.9384e+00,\n",
       "         3.4243e+00,  4.0282e+00,  3.0364e+00,  2.4017e+00,  1.8663e+00,\n",
       "         3.3399e+00,  2.8954e+00,  1.8773e+00,  3.1626e+00,  2.7417e+00,\n",
       "         3.7851e+00,  2.4693e+00,  4.6723e+00,  2.9071e+00,  3.0573e+00,\n",
       "         3.5883e+00,  3.7828e+00,  6.7367e-01,  7.0693e-01,  6.1704e-02,\n",
       "         6.1139e-01, -4.2987e-03, -2.8763e-01,  8.3422e-01,  8.0594e-01,\n",
       "         8.1811e-01,  1.3735e+00, -3.7762e-01, -1.0273e+00, -1.6728e-01,\n",
       "         3.3887e-01, -1.0877e+00, -7.0069e-01, -7.8717e-01, -9.8512e-01,\n",
       "        -8.7440e-01, -1.2665e+00, -6.5122e-01,  1.9033e-01,  6.9602e-01,\n",
       "         1.0764e+00, -8.5651e-01, -8.4488e-01, -2.3287e-01, -7.7346e-01,\n",
       "        -4.4957e-01,  1.5453e+00,  8.9512e-01,  1.1968e+00,  1.1509e-01,\n",
       "         3.3873e-01,  3.1633e-01,  4.3355e-01, -3.5741e-01, -5.0220e-01,\n",
       "        -8.9215e-01, -1.3151e+00, -3.0366e-01, -1.3485e+00, -1.4128e+00,\n",
       "        -2.3350e-01,  4.2517e-01, -1.0047e+00, -7.9467e-01, -2.1988e-01,\n",
       "        -1.1085e+00, -1.5861e+00, -8.7015e-02, -6.6336e-01, -1.7336e-01,\n",
       "         1.3440e-01, -7.0766e-01, -4.8446e-01,  1.1174e+00,  1.2331e+00,\n",
       "        -3.3156e-01, -2.6324e-01, -3.0282e-01, -2.0501e+00,  3.2185e-02,\n",
       "         9.1381e-01,  2.4483e-01,  2.0996e-01,  2.4459e-02, -3.8633e-01,\n",
       "         9.3509e-01,  9.1750e-02, -1.9178e-01, -9.0969e-01, -6.0290e-01,\n",
       "        -2.0366e-01, -2.1135e-01, -6.3131e-01, -6.4213e-01,  2.5490e-01,\n",
       "        -1.3417e+00, -9.5330e-01,  4.3120e-01,  8.2717e-01, -7.3563e-01,\n",
       "         2.7190e-01,  5.7451e-01,  1.9557e-01,  4.0642e-01,  2.4713e-02,\n",
       "        -5.8788e-01,  1.4998e-01, -8.7239e-02, -5.4671e-01,  2.5241e-01,\n",
       "         3.1235e-03, -8.5227e-01, -1.8259e-01, -6.8012e-01,  6.2426e-02,\n",
       "         3.7516e-01,  3.3387e-01,  1.2676e+00,  2.9962e-01,  1.3741e+00,\n",
       "        -3.8969e-01, -3.9703e-01,  1.1737e-01, -9.2184e-01, -9.6948e-02,\n",
       "        -4.9524e-01,  5.3546e-01, -3.5336e-01, -4.3691e-01, -6.5207e-01,\n",
       "         5.3269e-02, -1.3944e-01, -1.0861e+00, -3.7627e-01, -3.9291e-01,\n",
       "         1.4247e-01, -6.7763e-01,  2.7155e-01,  3.9185e-02, -6.5577e-02,\n",
       "        -8.1487e-01,  5.1319e-01, -4.2864e-01, -3.1421e-01, -1.2266e+00,\n",
       "         1.2333e+00, -4.4900e-02, -5.3717e-01, -5.2242e-01, -7.3242e-01,\n",
       "        -3.4712e-01, -3.9096e-01,  5.9648e-02,  4.3788e-01, -1.0977e+00,\n",
       "         4.4682e-02, -4.5545e-01, -3.0675e-01, -3.0604e-01,  1.2009e-01,\n",
       "        -4.0047e-01, -1.0223e+00,  3.3356e-01,  1.0372e-02, -5.2635e-01,\n",
       "        -5.5119e-01, -7.2843e-02,  4.2490e-01, -1.6256e-01,  1.1271e+00,\n",
       "         6.9395e-01, -6.2672e-03, -2.0958e-01,  1.9731e-01, -1.4454e+00,\n",
       "        -1.0495e+00, -1.8070e-01, -5.9833e-01, -1.9903e-01,  1.1636e+00,\n",
       "        -3.7101e-01,  2.4326e-01,  1.7736e-01, -1.8066e-01, -1.0157e+00,\n",
       "        -1.0136e+00, -4.7563e-01, -5.7059e-01, -9.3047e-01, -5.2690e-02,\n",
       "         5.2338e-01, -1.0183e+00,  1.6415e-01, -1.9810e-01,  1.7473e-02,\n",
       "         4.0204e-01, -1.4003e-01,  3.6955e-01, -5.1234e-01, -2.7032e-01,\n",
       "        -3.6590e-01,  1.4747e-03, -2.1043e-01, -7.6419e-01,  2.5213e-01,\n",
       "        -5.5918e-02,  6.6711e-01,  3.6876e-02, -1.0096e-01,  4.8464e-01,\n",
       "         3.8541e-01, -1.3456e+00,  5.7231e-01, -4.2470e-02,  4.3133e-01,\n",
       "        -8.4049e-02, -2.6305e-01, -4.6414e-01, -1.1372e-01,  2.2947e-01,\n",
       "        -5.2887e-01, -3.4392e-01, -2.2255e-01,  3.6742e-01, -1.4386e+00,\n",
       "         7.2409e-01, -2.3698e-01, -1.6448e-01, -1.5687e+00, -1.3258e+00,\n",
       "        -3.8216e-01, -1.0692e+00, -7.2697e-01, -3.7392e-01, -1.4315e-01,\n",
       "        -9.3109e-01, -6.5581e-01, -6.3122e-01, -6.2383e-01, -2.4160e+00,\n",
       "        -7.5089e-01,  8.1680e-02,  5.2389e-02,  6.0548e-01, -5.5790e-01,\n",
       "         1.6947e-01, -4.6718e-02, -1.1821e+00, -6.0695e-01, -7.6135e-01,\n",
       "        -1.0650e+00, -5.6378e-01, -2.7429e-01, -5.5994e-01, -1.7744e-01,\n",
       "        -2.0655e-01, -4.8465e-01, -1.4198e+00, -8.8112e-01, -9.6030e-02,\n",
       "        -4.2952e-01, -4.7959e-01, -1.0171e-01,  1.2004e-01, -1.2173e-01,\n",
       "        -1.3749e+00, -1.0508e+00, -6.6805e-01, -3.7798e-01, -9.8350e-01,\n",
       "         6.3704e-01,  3.6199e-01, -2.0396e-02,  3.0103e-01, -4.9200e-01,\n",
       "         2.0001e-01,  3.1668e-01,  5.9360e-01,  9.7401e-01, -5.2448e-01,\n",
       "        -1.5190e-01,  1.2539e-01, -2.6305e-01, -7.9693e-01,  2.9097e-01,\n",
       "        -3.0721e-01,  6.4088e-01,  1.0766e-01, -1.7094e+00,  1.2725e+00,\n",
       "        -8.6696e-02, -5.3121e-01,  4.1654e-01, -3.0984e-01,  1.6124e-01,\n",
       "        -4.5992e-01, -1.4941e-01,  1.2842e-01, -3.0104e-02,  8.2607e-01,\n",
       "         3.3000e-01, -1.1985e+00, -4.4296e-01, -2.7740e-01, -4.0548e-01,\n",
       "        -3.4770e-01,  6.9184e-01, -8.0681e-02,  8.6727e-01, -9.9535e-01,\n",
       "        -2.0594e+00, -7.4443e-02,  7.7793e-01, -1.5041e+00, -1.4131e+00,\n",
       "         4.7774e-01,  7.1617e-01, -1.5353e+00, -1.2535e+00, -1.3690e-01,\n",
       "         2.9112e-01,  1.1169e+00, -1.7713e+00, -1.1103e+00,  3.8009e-01,\n",
       "        -6.9413e-01, -1.9473e-01,  3.4498e-02, -3.5046e-01, -1.4358e-01,\n",
       "        -6.1537e-03, -1.9874e-02, -2.5464e-01, -1.0155e+00,  2.1997e-01,\n",
       "         2.8780e-01, -4.7487e-01, -2.8096e-01, -7.1042e-01,  6.8230e-01,\n",
       "        -3.3568e-01, -1.2166e+00, -9.0697e-01,  1.2215e-01, -1.7390e+00,\n",
       "        -3.2139e-01, -4.7601e-01, -4.7051e-01, -2.5235e-01, -7.3702e-01,\n",
       "        -1.7650e-01,  2.4883e-01,  5.3045e-01,  2.6200e-01, -1.7633e+00,\n",
       "        -1.0401e+00, -5.4094e-01, -9.2651e-01, -8.4484e-01, -5.3297e-01,\n",
       "        -5.4115e-01,  1.0649e-01, -2.9897e-02, -7.3113e-01,  4.6768e-01,\n",
       "        -6.1681e-01, -1.2394e+00,  1.3555e+00, -1.4528e+00, -3.9816e-01,\n",
       "        -3.7695e-01, -1.3179e+00, -3.4122e-01, -2.7023e-01, -1.3537e-01,\n",
       "         2.9659e-01,  2.9337e-01, -7.9024e-01, -7.6011e-01,  2.1546e-01,\n",
       "         3.6362e-01,  8.0484e-02,  9.7451e-01,  1.7527e+00,  4.0523e-01,\n",
       "        -6.5913e-02,  4.7158e-01, -1.2273e-01, -5.4626e-01, -3.2069e-01,\n",
       "         5.9521e-02, -2.8239e-01,  9.3321e-02,  5.4704e-01,  1.2708e+00,\n",
       "         3.7720e-02, -4.4450e-01, -2.6901e-01, -4.4626e-01, -3.4592e-02,\n",
       "         1.0194e-01, -2.3339e-01,  4.4487e-01,  3.3936e-01,  2.4321e-01,\n",
       "         2.0211e-01,  3.3796e-01, -7.8064e-01, -1.6992e+00, -9.5757e-01,\n",
       "         2.6221e-01, -4.1715e-01,  4.7921e-01, -4.4031e-02, -1.1713e+00,\n",
       "         1.4446e-01,  7.2909e-01, -1.6373e+00,  3.3419e-01,  5.6544e-02,\n",
       "         1.8892e-01,  1.0617e-01, -1.0292e+00,  6.8586e-01,  1.3947e-02,\n",
       "         7.5578e-01, -4.1465e-02, -5.7326e-02,  2.3749e+00, -8.7912e-01,\n",
       "         9.9355e-01, -9.0319e-01,  5.6646e-01,  4.6569e-01, -1.1790e-01,\n",
       "        -1.1215e+00,  8.5247e-04, -1.1088e-01, -3.0665e-01,  1.5602e+00,\n",
       "        -8.9040e-01,  3.2294e-01, -5.7312e-01, -2.7327e-01, -4.5776e-01,\n",
       "        -3.5135e-01,  1.2604e+00, -1.1591e+00,  6.0895e-01, -2.3839e-02,\n",
       "        -1.2414e+00, -5.6151e-01, -9.8610e-02, -5.2664e-02, -1.1397e-01,\n",
       "        -2.2334e-01, -1.2210e+00,  4.4270e-01, -2.9248e-01,  4.0192e-01,\n",
       "         5.1236e-01, -7.5999e-01,  6.9734e-01, -6.5716e-01, -4.0890e-02,\n",
       "        -1.0566e-01, -1.2025e-01,  3.3041e-01, -3.6257e-01,  8.9573e-01,\n",
       "        -7.5122e-01, -7.2522e-01, -9.3669e-01,  5.3541e-01,  9.8458e-01,\n",
       "        -6.6255e-01,  2.1266e-01, -6.7777e-01, -5.4911e-01,  6.6891e-01,\n",
       "         4.8033e-01, -2.7051e-01,  3.4499e-01, -2.7397e-01, -9.7528e-01,\n",
       "        -3.3758e-01, -1.6120e+00, -1.5429e-01, -3.0252e-01,  3.7784e-01,\n",
       "        -8.0197e-01, -6.7167e-01, -3.0710e-01, -5.1659e-02, -2.8810e-01,\n",
       "         1.1968e-02,  4.8108e-01,  1.9145e-02,  6.8697e-02, -1.1712e+00,\n",
       "         8.4081e-01, -1.1797e+00,  8.3842e-02, -7.7231e-01,  3.1158e-01,\n",
       "         1.8174e-01,  1.7131e-01,  3.0156e-01,  1.0160e-01, -6.3591e-01,\n",
       "         5.3666e-01, -1.7055e+00, -8.5954e-01, -7.6677e-01, -6.1682e-02,\n",
       "         1.1917e+00,  9.5173e-02, -9.0544e-01, -5.4406e-01,  7.9547e-02,\n",
       "        -1.3502e-01, -9.4616e-01, -9.9362e-01, -3.8204e-01, -2.5304e-01,\n",
       "        -1.3443e+00, -2.4780e-02, -4.9610e-01,  7.8225e-01, -9.9303e-01,\n",
       "        -3.7499e-01, -1.0988e+00,  1.0256e+00,  7.9206e-01, -1.1220e+00,\n",
       "         1.8751e-01, -2.7323e-01,  7.1185e-01, -1.1991e-01, -4.0646e-01,\n",
       "        -6.8417e-01, -7.3193e-01,  6.7616e-01, -4.9689e-01, -1.4372e+00,\n",
       "        -1.4862e-02,  7.9633e-01, -6.0245e-01, -1.5254e-01, -1.1453e+00,\n",
       "         5.5803e-01, -3.1614e-01, -3.2544e-02, -8.8137e-01, -3.6552e-02,\n",
       "        -9.9482e-01,  1.0695e-01,  2.5453e-01,  6.5065e-01, -3.9501e-01,\n",
       "         9.9171e-02, -5.0370e-01,  3.6602e-01, -4.7946e-01,  1.0609e-01,\n",
       "        -1.0575e+00, -1.6334e-01,  5.9121e-01,  1.4640e-01, -6.5209e-01,\n",
       "         2.0023e-01, -4.3255e-01, -2.2385e-01,  1.7217e-01, -1.4867e-01,\n",
       "        -7.2663e-01,  2.2605e-01, -1.0006e+00, -2.8136e-01,  1.4472e-01,\n",
       "         5.8588e-01, -4.4207e-01, -7.0224e-01, -1.0914e+00, -7.1152e-01,\n",
       "        -2.0456e-02, -6.5664e-01, -2.7428e-03, -1.5722e-01,  5.0217e-01,\n",
       "         7.1508e-01,  3.5123e-01,  7.9195e-02, -1.0162e+00,  7.6332e-01,\n",
       "        -9.2266e-01, -1.4160e-01, -1.4416e-01, -1.9193e-01, -1.2449e+00,\n",
       "        -3.4593e-01,  2.8422e-01,  1.3690e+00, -9.5024e-01, -1.0418e-01,\n",
       "        -2.1390e-01, -1.0808e+00, -1.2149e+00, -7.3044e-01,  4.9418e-01,\n",
       "        -5.3101e-03,  7.1854e-01, -1.0582e+00,  1.5716e-01,  2.4118e-01,\n",
       "        -1.1823e-01, -8.6130e-01, -9.6115e-02, -3.2542e-02, -5.5819e-01,\n",
       "         1.2366e-01,  3.0340e-01,  5.5899e-01,  5.6439e-01, -4.1328e-02,\n",
       "        -5.5495e-02,  1.4490e+00, -1.6481e-01,  2.5917e-01,  2.8859e-01,\n",
       "         1.5971e+00,  1.3329e+00,  8.3958e-01,  3.6907e-01,  4.7855e-01,\n",
       "        -4.1180e-01,  1.9639e+00,  1.6070e+00,  1.3636e-02,  5.7129e-01,\n",
       "         2.6093e-01, -6.0772e-02,  1.1129e+00, -1.3175e+00, -3.2930e-01,\n",
       "         1.5176e-01,  6.3380e-01,  2.3787e-01,  6.0232e-01, -4.5577e-01,\n",
       "         9.7763e-01, -8.3347e-01,  3.0875e-01, -8.9208e-01, -8.8099e-01,\n",
       "         7.7705e-01, -2.5459e-01, -4.6234e-02, -3.0294e-01, -4.5818e-01,\n",
       "         3.3745e-01,  3.9379e-01,  6.6701e-01, -7.0002e-01,  3.0053e-01,\n",
       "         9.6884e-01,  4.9355e-02,  2.7887e-01, -1.6099e-01,  1.0769e+00,\n",
       "         6.6698e-01, -3.5536e-01, -6.1739e-01, -2.9499e-01, -5.9342e-01,\n",
       "         2.3594e+00,  1.9801e+00,  1.9181e+00,  2.1723e+00,  2.0243e+00,\n",
       "         1.3820e+00,  1.7312e+00,  6.1605e-01,  1.0870e+00,  2.3154e+00,\n",
       "         2.5561e+00,  2.2475e+00,  1.0393e+00,  3.3476e+00, -3.8315e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b55d3d-b5d7-4f73-a010-c19b5c7a15e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[322, 311, 70, 320]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(outputs, k=4).indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c42fe7-ae24-4716-97f6-46fac72daaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04724839702248573"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(outputs, -1)[322].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ab57c-68b7-44e6-ba63-65803a98033d",
   "metadata": {},
   "source": [
    "El siguiente ejemplo se tomÃ³ de [Kaggle](https://www.kaggle.com/code/raufmomin/vision-transformer-vit-from-scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c86abe-8a78-4cbb-aa91-46b584544d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.13.0\n"
     ]
    }
   ],
   "source": [
    "#Importamos las librerias a utlizar\n",
    "\n",
    "#!pip3 install tensorflow_addons\n",
    "#!pip3 install --upgrade typing-extensions==4.5\n",
    "#!pip3 install --upgrade tensorflow==2.13\n",
    "#!pip3 install 'keras<3.0.0' mediapipe-model-maker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593993c-3999-42b8-9694-7a0a936ed193",
   "metadata": {},
   "source": [
    "FUNCION DE PARECIDO, PUEDE SER LA FUNCION EUCLIDIANA, COSENO (TE QUEDA AL REVÃˆS) CUANDO COINCIDE VALE 1 Y CUANDO NO, VALE 0. TENGO LOS EMBEDDINGS. EN UNA CIERTA CAPA DE LA RED TENGO CIERTOS COMPONENTES PRINCIPALES. RED DE REGULARIZACIONES PARA QUE NO SE TE SALTEN EN LAS CAPAS.\n",
    "\n",
    "PROBLEMA DE CLASIFICACION, MUESTRA DE TRAIN, UNA DE TEST. TENGO UNA IMAGEN DE TEST, LA TRUNCO, SACO LOS RESULTADOS Y OBTENDO SIMILITUDES. SE PUEDE METER EN EL ARQUETIPO A PRIORI. COMO HACER UNA METRICA PARA QUE SE PAREZCA \n",
    "\n",
    "TEXTO DESTRUCTURADO, BASE DE DATOS DE GRAFOS (GRAFOS), LO METES RFLIP.\n",
    "\n",
    "CONTRUIR LOS ARQUETIPOS, NIVEL AL CUAL ESTOY CRTANDO LA RED Y LA METRICA PARA LA COMPARACION. ETIQUETAS: LO QUE ESTOY VIENDO ES QUE ESA MUESTRA A QUE CLUSTER CORRESPONDE. ENTRENAR EL MODELO CON IMAGENES \"EQUIS\" DE PERROS, ANIMALES, ETC, ETC. PARA TENERLO SUPERVISADO EN EL ENTRENAMIENTO, IR POR FASES Y UNA VEZ QUE HAYA AGARRADO VELOCIDAD ENTONCES SI METERLE LO DE LAS PLANTAS. SEMISUPERVISADO -> IR ALARGANDOLO PARA QUE VAYA APRENDIENDO DE DIFERENTES RAMAS. \n",
    "\n",
    "DEEP LEARNING MUY PODEROSO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fcec1-0cfd-4a2e-b4fb-526365c03e72",
   "metadata": {},
   "source": [
    "## Prueba extra: SoluciÃ³n de Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbebb2-87cc-4824-a291-57c4e1363ad4",
   "metadata": {},
   "source": [
    "AquÃ­ se va a probar una soluciÃ³n proveÃ­da por el equipo de Keras dentro del mismo reto de Kaggle. Esto con el fin de poder entender como se puede ir aplicando el modelo y el propÃ³sito es que de aquÃ­ se van obteniendo vertientes diferentes.\n",
    "\n",
    "Primero, se probarÃ¡ si es que se puede replicar con mis propios recursos la misma soluciÃ³n para ver si es factible desde mis recursos.\n",
    "\n",
    "Una vez que tenga la confirmaciÃ³n de que puedo replicar la soluciÃ³n con mis recursos, entonces se investigarÃ¡ que parte(s) es la que puedo editar para tener algo diferente y de ahÃ­ ir obteniendo mi proyecto.\n",
    "\n",
    "Ya que tengo la teorÃ­a investigada de mi lado entonces si entrarÃ­amos en la fase de experimentaciÃ³n\n",
    "\n",
    "[Referencia en pÃ¡gina de Kaggle](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook#ğŸ”-%7C-Loss-&-Metric)\n",
    "\n",
    "**OTRAS REFERENCIAS:**\n",
    "\n",
    "[Keras CV Models](https://keras.io/api/keras_cv/models/)\n",
    "\n",
    "[Train an image classifier from scratch - Keras CV](https://keras.io/guides/keras_cv/classification_with_keras_cv/)\n",
    "\n",
    "[Learning Rate Schedulers](https://github.com/Mr-TalhaIlyas/Learning-Rate-Schedulers-Packege-Tensorflow-PyTorch-Keras?tab=readme-ov-file)\n",
    "\n",
    "[SGDR](https://arxiv.org/pdf/1608.03983)\n",
    "\n",
    "[batch function doc](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)\n",
    "\n",
    "[StratifiedKFold - Medium](https://medium.com/@literallywords/stratified-k-fold-with-keras-e57c487b1416)\n",
    "\n",
    "[StratifiedKFold - scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n",
    "\n",
    "[StratifiedKFold - Visualization](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2df8d-8f28-428c-98ab-8d8554dabfae",
   "metadata": {},
   "source": [
    "### ReplicaciÃ³n de la soluciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e553ab-2a0f-455c-8477-647bb0cd8863",
   "metadata": {},
   "source": [
    "En la soluciÃ³n publicada de Kaggle, el modelo que utlizan es **EfficientNev2** (Ã¡re ade oportunidad para cambiar de modelo?). \n",
    "\n",
    "Partes (pasos) esenciales para la ejecuciÃ³n de este proyecto:\n",
    "\n",
    "- Designing a data pipeline for a multi-input and multi-output model.\n",
    "- Creating random augmentation pipeline with KerasCV.\n",
    "- Loading the data efficiently using tf.data.\n",
    "- Creating the model using KerasCV presets.\n",
    "- Training the model.\n",
    "- Inference and Submission on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d07aa3-6c0c-4175-8fde-1cc6bad43e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 20:15:53.440906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#InstalaciÃ³n de las librerÃ­as necesarias a utilizar\n",
    "\n",
    "#!pip3 install keras_cv==0.8.2 --no-deps\n",
    "#!pip3 install tensorflow==2.15.0 --no-deps\n",
    "#!pip3 install keras==3.0.4 --no-deps\n",
    "\n",
    "#Importamos las librerias que vayamos a necesitar\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" #Aqui el autor dice que tambiÃ©n podemos utilizar tensorflow o torch (ÃREA DE OPORTUNIDAD)\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Funciones creadas para facilitar algunos procesos\n",
    "import functions.general_functions as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2448e526-81ff-4561-aab6-484e8426e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.15.0\n",
      "Keras 3.0.4\n",
      "KerasCV 0.8.2\n"
     ]
    }
   ],
   "source": [
    "#Imprimimos las versiones de las librerias a utilizar \n",
    "\n",
    "print(\"TensorFlow\", tf.__version__)\n",
    "print(\"Keras\",keras.__version__)\n",
    "print(\"KerasCV\",keras_cv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15925d8-6791-4d6f-9608-7197eba2208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciamos con la fase de configuraciÃ³n: DiseÃ±o de un data pipeline para un modelo con entradas y salidas mÃºltiples\n",
    "\n",
    "#ConfiguraciÃ³n de la clase CFG:\n",
    "\n",
    "class CFG:\n",
    "    verbose = 1 #ParÃ¡metro que configura la manera en que el proceso de entrenamiento es presentado para cada epoch. 0 = silencioso (no muestra nada), 1 = se muestra cada epoch separado por barra, 2 = se muestra solo el nÃºmero de epoch\n",
    "    seed = 42 #Semilla que se utiliza para poder reproducir resultados similares en cada ejecuciÃ³n\n",
    "    preset = \"efficientnetv2_b2_imagenet\" #Nombre del clasificador pre entrenado, ÃREA DE MEJORA. TambiÃ©n se puede utilizar modelos como CSPDarkNet, densenet, EfficientNetV2 (otras vertientes), MiT, MobileNetV3, ResNetV1, YOLO, VitDet\n",
    "    image_size = [224, 224] #TamaÃ±o de la imagen de entrada del modelo , ÃREA DE MEJORA, investigar que otros tamaÃ±os podemos utilizar y como es que cambia los resultados con otros tamaÃ±os -> posible efecto que tiene el tamaÃ±o de la imagen en los resultados del modelo\n",
    "    epochs: 12 #Epochs para el entrenamiento -> NÃºmero total de iteraciones que tuvo todo tu conjunto de entrenamiento para entrenar el modelo de ML, un \"paso\" de la data de entrenamiento se toma en cuenta cuando Ã©sta haya pasado tanto por delante como por detrÃ¡s en todos los procesos del modelo. Incrementando el nÃºmero de epochs mejora el performance del modelo al permitirle aprender comportamientos mÃ¡s complejos en la data, pero si existen muchos epochs, el modelo se sobre-entrena\n",
    "    batch_size: 96 #TamaÃ±o del batch -> Un batch es un subconjunto/grupo del conjunto completo de entrenamiento, tambiÃ©n se pueden definir como muestras del conjunto de entrenamiento, esto para mejorar el proceso de un conjunto enorme de datos en grupos \"pequeÃ±os\" de datos\n",
    "    lr_mode = \"step\" #Tipo de Learning Rate Scheduler y puede ser entre \"cos\", \"step\" o \"exp\". \"step\" = Step Decay (empieza a deacer el LR despuÃ©s de ciertos epochs mediante un factor lr_decay), \"exp\" = Exponential Decay (empieza a decaer el LR de manera exponencial), \"cos\" = Cosine Decay (el LR empieza a decaer mediante un factor de coseno). TambiÃ©n hay otros LRS, ÃREA DE MEJORA\n",
    "    drop_remainder = True #ParÃ¡metro lÃ³gico para desechar los batches incompletos. Sirve para tener un control sobre los batches en cuanto al tamaÃ±o de Ã©stos, si el parÃ¡metro es True entonces desecharÃ¡ el Ãºltimo batch si es que Ã©ste es de un tamaÃ±o menor (batch_size) a los demÃ¡s, por defualt es FALSE\n",
    "    num_classes = 6 #NÃºmero de clases que vienen en el dataset (las clases que vamos a predecir)\n",
    "    num_folds = 5 #NÃºmero de dobleces (batches) para dividir el conjunto de datos (Prueba y entrenamiento). Se utiliza dentro de la funciÃ³n StratifiedKFold y se refiere a que de los K batches que se crearon de la data, entrenamos el modelo con K-1 batches y probamos con el Ãºltimo batch, esto se repite para todas las permutaciones de batches (al final siempre se hace el entrenamiento dejando un batch diferente fuera cada vez), lo que lo hace estratificado es que tambiÃ©n nos fijamos en la distribuciÃ³n relativa de las clases, si una clase aparece mÃ¡s que otra entonces se toma en cuenta para la formaciÃ³n de batches. ÃREA DE MEJORA, podemos utilizar diferentes mÃ©todos de fold\n",
    "    fold = 0 #NÃºmero de fold que utilizaremos para la validaciÃ³n de la data. solo sirve para controlar los Ã­ndices del conjunto de datos y dividirlo en conjunto de prueba y entrenamiento. AquÃ­ definimos que el conjunto para validaciÃ³n serÃ¡ aquel del fold 0, lo demÃ¡s serÃ¡ para entrenmaiento\n",
    "    class_names = ['X4_mean', 'X11_mean', 'X18_mean',\n",
    "                   'X26_mean', 'X50_mean', 'X3112_mean'] #Nombre de las clases del conjunto de datos, son las variables que vamos a predecir\n",
    "    aux_class_names = list(map(lambda x: x.replace(\"mean\",\"sd\"), class_names)) #Nombre de las clases auxiliares para el entrenamiento de los datos, estas variables vienen tambiÃ©n en el conjunto de datos\n",
    "    num_classes = len(class_names) #NÃºmero total de clases que se tiene en el conjunto de entrenamiento\n",
    "    aux_num_classes = len(aux_class_names) #NÃºmero total de clases auxiliares que se tiene en el conjunto de entrenamiento\n",
    "\n",
    "#Ruta de donde se leerÃ¡n todos los archivos CSV\n",
    "\n",
    "ruta_data_csv = gf.get_data_path(\"csv\")\n",
    "\n",
    "#Ruta de donde se leerÃ¡n todas las imÃ¡genes de entrenamiento\n",
    "\n",
    "ruta_img_train = gf.get_data_path(\"train_images\")\n",
    "\n",
    "#Ruta de donde se leerÃ¡n todas las imÃ¡genes de prueba\n",
    "\n",
    "ruta_img_test = gf.get_data_path(\"test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae81c306-4f09-4952-808d-bbbb1fcc5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuramos el parÃ¡metro de seed\n",
    "\n",
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad6f707-8197-4d01-931c-2fad125f5099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>WORLDCLIM_BIO1_annual_mean_temperature</th>\n",
       "      <th>WORLDCLIM_BIO12_annual_precipitation</th>\n",
       "      <th>WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month</th>\n",
       "      <th>WORLDCLIM_BIO15_precipitation_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO4_temperature_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO7_temperature_annual_range</th>\n",
       "      <th>SOIL_bdod_0.5cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_100.200cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_15.30cm_mean_0.01_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>X26_mean</th>\n",
       "      <th>X50_mean</th>\n",
       "      <th>X3112_mean</th>\n",
       "      <th>X4_sd</th>\n",
       "      <th>X11_sd</th>\n",
       "      <th>X18_sd</th>\n",
       "      <th>X26_sd</th>\n",
       "      <th>X50_sd</th>\n",
       "      <th>X3112_sd</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192027691</td>\n",
       "      <td>12.235703</td>\n",
       "      <td>374.466675</td>\n",
       "      <td>62.524445</td>\n",
       "      <td>72.256844</td>\n",
       "      <td>773.592041</td>\n",
       "      <td>33.277779</td>\n",
       "      <td>125</td>\n",
       "      <td>149</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.243779</td>\n",
       "      <td>1.849375</td>\n",
       "      <td>50.216034</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>1.601473</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.153608</td>\n",
       "      <td>0.279610</td>\n",
       "      <td>15.045054</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195542235</td>\n",
       "      <td>17.270555</td>\n",
       "      <td>90.239998</td>\n",
       "      <td>10.351111</td>\n",
       "      <td>38.220940</td>\n",
       "      <td>859.193298</td>\n",
       "      <td>40.009777</td>\n",
       "      <td>124</td>\n",
       "      <td>144</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642940</td>\n",
       "      <td>1.353468</td>\n",
       "      <td>574.098472</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.258078</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>11.004477</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
       "0  192027691                               12.235703   \n",
       "1  195542235                               17.270555   \n",
       "\n",
       "   WORLDCLIM_BIO12_annual_precipitation  \\\n",
       "0                            374.466675   \n",
       "1                             90.239998   \n",
       "\n",
       "   WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
       "0                                          62.524445                       \n",
       "1                                          10.351111                       \n",
       "\n",
       "   WORLDCLIM_BIO15_precipitation_seasonality  \\\n",
       "0                                  72.256844   \n",
       "1                                  38.220940   \n",
       "\n",
       "   WORLDCLIM_BIO4_temperature_seasonality  \\\n",
       "0                              773.592041   \n",
       "1                              859.193298   \n",
       "\n",
       "   WORLDCLIM_BIO7_temperature_annual_range  SOIL_bdod_0.5cm_mean_0.01_deg  \\\n",
       "0                                33.277779                            125   \n",
       "1                                40.009777                            124   \n",
       "\n",
       "   SOIL_bdod_100.200cm_mean_0.01_deg  SOIL_bdod_15.30cm_mean_0.01_deg  ...  \\\n",
       "0                                149                              136  ...   \n",
       "1                                144                              138  ...   \n",
       "\n",
       "   X26_mean  X50_mean  X3112_mean     X4_sd    X11_sd    X18_sd    X26_sd  \\\n",
       "0  1.243779  1.849375   50.216034  0.008921  1.601473  0.025441  0.153608   \n",
       "1  0.642940  1.353468  574.098472  0.003102  0.258078  0.000866  0.034630   \n",
       "\n",
       "     X50_sd   X3112_sd                                         image_path  \n",
       "0  0.279610  15.045054  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "1  0.010165  11.004477  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "\n",
       "[2 rows x 177 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>WORLDCLIM_BIO1_annual_mean_temperature</th>\n",
       "      <th>WORLDCLIM_BIO12_annual_precipitation</th>\n",
       "      <th>WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month</th>\n",
       "      <th>WORLDCLIM_BIO15_precipitation_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO4_temperature_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO7_temperature_annual_range</th>\n",
       "      <th>SOIL_bdod_0.5cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_100.200cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_15.30cm_mean_0.01_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m04</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m05</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m06</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m07</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m08</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m09</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m10</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m11</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m12</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195066138</td>\n",
       "      <td>10.55810</td>\n",
       "      <td>961.500000</td>\n",
       "      <td>31.586735</td>\n",
       "      <td>13.728325</td>\n",
       "      <td>648.038208</td>\n",
       "      <td>25.351532</td>\n",
       "      <td>127</td>\n",
       "      <td>152</td>\n",
       "      <td>137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469694</td>\n",
       "      <td>0.455849</td>\n",
       "      <td>0.528211</td>\n",
       "      <td>0.555653</td>\n",
       "      <td>0.549882</td>\n",
       "      <td>0.542905</td>\n",
       "      <td>0.517507</td>\n",
       "      <td>0.462724</td>\n",
       "      <td>0.427107</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195524180</td>\n",
       "      <td>7.00287</td>\n",
       "      <td>1120.025513</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>7.258863</td>\n",
       "      <td>973.889404</td>\n",
       "      <td>39.135712</td>\n",
       "      <td>106</td>\n",
       "      <td>167</td>\n",
       "      <td>127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428838</td>\n",
       "      <td>0.456266</td>\n",
       "      <td>0.470074</td>\n",
       "      <td>0.468038</td>\n",
       "      <td>0.475943</td>\n",
       "      <td>0.483206</td>\n",
       "      <td>0.477197</td>\n",
       "      <td>0.432732</td>\n",
       "      <td>0.423728</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
       "0  195066138                                10.55810   \n",
       "1  195524180                                 7.00287   \n",
       "\n",
       "   WORLDCLIM_BIO12_annual_precipitation  \\\n",
       "0                            961.500000   \n",
       "1                           1120.025513   \n",
       "\n",
       "   WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
       "0                                          31.586735                       \n",
       "1                                          23.000000                       \n",
       "\n",
       "   WORLDCLIM_BIO15_precipitation_seasonality  \\\n",
       "0                                  13.728325   \n",
       "1                                   7.258863   \n",
       "\n",
       "   WORLDCLIM_BIO4_temperature_seasonality  \\\n",
       "0                              648.038208   \n",
       "1                              973.889404   \n",
       "\n",
       "   WORLDCLIM_BIO7_temperature_annual_range  SOIL_bdod_0.5cm_mean_0.01_deg  \\\n",
       "0                                25.351532                            127   \n",
       "1                                39.135712                            106   \n",
       "\n",
       "   SOIL_bdod_100.200cm_mean_0.01_deg  SOIL_bdod_15.30cm_mean_0.01_deg  ...  \\\n",
       "0                                152                              137  ...   \n",
       "1                                167                              127  ...   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m04  VOD_X_1997_2018_multiyear_mean_m05  \\\n",
       "0                            0.469694                            0.455849   \n",
       "1                            0.428838                            0.456266   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m06  VOD_X_1997_2018_multiyear_mean_m07  \\\n",
       "0                            0.528211                            0.555653   \n",
       "1                            0.470074                            0.468038   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m08  VOD_X_1997_2018_multiyear_mean_m09  \\\n",
       "0                            0.549882                            0.542905   \n",
       "1                            0.475943                            0.483206   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m10  VOD_X_1997_2018_multiyear_mean_m11  \\\n",
       "0                            0.517507                            0.462724   \n",
       "1                            0.477197                            0.432732   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m12  \\\n",
       "0                            0.427107   \n",
       "1                            0.423728   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "1  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "\n",
       "[2 rows x 165 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Leemos el archivo de entrenamiento\n",
    "df = pd.read_csv(f'{ruta_data_csv}/train.csv')\n",
    "#Creamos una nueva columna con la direcciÃ³n de cada imagen correspondiente\n",
    "df['image_path'] = f'{ruta_img_train}/'+df['id'].astype(str)+'.jpeg'\n",
    "#Sustituimos los NAs en las variables objetivo auxiliares con \"-1\"\n",
    "df.loc[:,CFG.aux_class_names] = df.loc[:,CFG.aux_class_names].fillna(-1)\n",
    "#Mostramos los primeros 2 renglones del conjunto de entrenamiento\n",
    "display(df.head(2))\n",
    "\n",
    "#Leemos el conjunto de prueba\n",
    "test_df = pd.read_csv(f'{ruta_data_csv}/test.csv')\n",
    "#Creamos una nueva columna con la direcciÃ³n de cada imagen correspondiente\n",
    "test_df['image_path'] = f'{ruta_img_train}/'+test_df['id'].astype(str)+'.jpeg'\n",
    "#Guardamos en una lista el nombre de todas las columnas del conjunto de prueba, estos son las columnas de las caracterÃ­sticas de las plantas\n",
    "FEATURE_COLS = test_df.columns[1:-1].tolist()\n",
    "#Mostramos los primeros 2 renglones del conjunto de prueba\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f423179-5d8d-4187-bb6d-64f22ed546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crearemos un data loader el cual podrÃ¡ leer imÃ¡genes y archivos CSV al mismo tiempo, tambiÃ©n maneja etiquetas para tareas principales y auxiliares\n",
    "#Este data loader tambiÃ©n aplicarÃ¡ modificaciones (augmentations) a las imÃ¡genes, tales como flip, rotation, brightness, etc. Todas estas modificaciones se aplicarÃ¡ solo a un subconjunto de imÃ¡genes. lo cual acelera el entrenamiento del modelo y reduce el cuello de botella del CPU\n",
    "\n",
    "#FunciÃ³n para contruir el modificador de las imÃ¡genes\n",
    "def build_augmenter():\n",
    "    #Definimos la modificaciones que se le harÃ¡n a las imÃ¡genes\n",
    "    aug_layers = [\n",
    "        #Brillo de la imagen que serÃ¡ ajustado de manera aleatoria durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_brightness/]\n",
    "        keras_cv.layers.RandomBrightness(factor = 0.1, value_range = (0, 1)),\n",
    "        #Contraste de la imagen que serÃ¡ ajustado de manera aleatoria durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_contrast/]\n",
    "        keras_cv.layers.RandomContrast(factor = 0.1, value_range = (0, 1)),\n",
    "        #SaturaciÃ³n de la imagen que serÃ¡ ajustado de manera aleatoria para reducir o aumentar la saturaciÃ³n de la imagen de input [https://keras.io/api/keras_cv/layers/augmentation/random_saturation/]\n",
    "        keras_cv.layers.RandomSaturation(factor = (0.45, 0.55)),\n",
    "        #Color a o matiz de la imagen que serÃ¡ ajustado de manera aleatoria [https://keras.io/api/keras_cv/layers/augmentation/random_hue/]\n",
    "        keras_cv.layers.RandomHue(factor = 0.1, value_range = (0, 1)),\n",
    "        #Recorte aleatorio de rectÃ¡ngulos de las imÃ¡genes para despuÃ©s rellenarlas. AquÃ­ se pueden hacer rellenos gausianos [https://keras.io/api/keras_cv/layers/augmentation/random_cutout/]\n",
    "        keras_cv.layers.RandomCutout(height_factor = (0.06, 0.15), width_factor = (0.06, 0.15)),\n",
    "        #Voltea las imÃ¡genes de manera aleatoria durante el proceso de entrenamiento [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/]\n",
    "        keras_cv.layers.RandomFlip(mode = \"horizontal_and_vertical\"),\n",
    "        #Hace zoom (de lejos o cerca) a las imÃ¡genes de manera aleatoria durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_zoom/]\n",
    "        keras_cv.layers.RandomZoom(height_factor = (0.05, 0.15)),\n",
    "        #RotaciÃ³n aleatoria de las imÃ¡genes durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/]\n",
    "        keras_cv.layers.RandomRotation(factor = (0.01, 0.05))\n",
    "    ]\n",
    "\n",
    "    #Aplicamos las modificaciones a muestras aleatorias \n",
    "    aug_layers = [keras_cv.layers.RandomApply(x, rate = 0.5) for x in aug_layers]\n",
    "\n",
    "    #Construimos la capa de modificaciones\n",
    "    augmenter = keras_cv.layers.Augmenter(aug_layers)\n",
    "\n",
    "    #Aplicamos las modificaciones\n",
    "    def augment(inp, label):\n",
    "        images = inp[\"images\"]\n",
    "        aug_data = {\"images\": images}\n",
    "        aug_data = augmenter(aug_data)\n",
    "        inp[\"images\"] = aug_data[\"images\"]\n",
    "        return inp, label\n",
    "    return augment\n",
    "\n",
    "    return augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50506ec8-e714-4fbc-b3dd-1905e77eb2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Augmenter' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/IPython/core/formatters.py:711\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    704\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    705\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    707\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    708\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    709\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    710\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 711\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/IPython/lib/pretty.py:411\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    410\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 411\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/IPython/lib/pretty.py:779\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(obj)\n\u001b[1;32m    780\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/keras/src/layers/layer.py:1274\u001b[0m, in \u001b[0;36mLayer.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, built=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1275\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Augmenter' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "x1 = build_augmenter()\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98858113-64d9-4be6-b20f-187525c36eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
