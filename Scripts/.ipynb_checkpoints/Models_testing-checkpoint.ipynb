{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b533ebb5-25f2-4de2-b526-3fd07bddd8fc",
   "metadata": {},
   "source": [
    "# PRUEBA DE MODELOS - EMBEDDING Y CLASIFICACI√ìN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6198e1-a0a1-4f3b-92e2-e9275b1cbce8",
   "metadata": {},
   "source": [
    "Lo que se pretende en este libro de Jupyter es empezar con las pruebas de algunos modelos que se pudieron obtener de referencia en investigaciones anteriores. Todo esto con el fin de poder tener una mejor noci√≥n de estos modelos y en el mejor de los casos, ya tener un modelo definido con el que se trabajar√° posteriormente. \n",
    "\n",
    "Algunos de estos modelos son simplemente de embedding, otros tantos si conllevan algunas t√©cnicas ya avanzadas de clasificaci√≥n o detecci√≥n de im√°genes, seg√∫n corresponda. La lista de modelos que se pretende probar es la siguiente:\n",
    "\n",
    "- OpenL3 (Solo de embedding)\n",
    "\n",
    "- ViT (de Google)\n",
    "\n",
    "- Contrastors: Aqu√≠ podemos hacer el uso a su vez de dos modelos, CLIP y MRL\n",
    "\n",
    "- Sports: Lista de varios modelos que nos pueden ayudar con la tarea asignada\n",
    "\n",
    "- ResNet: Modelo moldeable con el n√∫mero de capas que √©ste utiliza, es una red neuronal convolucional\n",
    "\n",
    "- InceptionV3: Modelo moldeable con el n√∫mero de capas y par√°metros que √©ste utiliza, es una red neuronal convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb999c8-ad49-443e-a579-0ba4778e1f6c",
   "metadata": {},
   "source": [
    "Ejemplos de modelos que algunas personas tomaron para la soluci√≥n del proyecto. Tomado de [aqu√≠](https://www.kaggle.com/competitions/planttraits2024/code)\n",
    "\n",
    "1. Primer ejemplo: [KerasCV - EfficientNetV2](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook#üîç-%7C-Loss-&-Metric)\n",
    "\n",
    "Aqu√≠ en este ejemplo podemos ver los detalles del significado de cada _plant trait_ y qu√© significa cada uno. Aqu√≠ es importante definir algunos par√°metros (que tambi√©n he visto dentro de ViT, por ejemplo, y que son el tam√±o de las im√°genes, epochs, seed, verbose, etc. Aqu√≠ tambi√©n se hace el augmentations de las im√°genes (flip, rotation, brightness, etc.), aqu√≠ los augmentations se hacen a un batch, lo cual acelera el entrenamiento y reduce el cuello de botella de CPU. \n",
    "\n",
    "Dentro del procedimiento de aplicar los augmenters, se ven funciones dentro de funciones, lo cual me confunde un poco. Despu√©s del proceso de augmenter, viene un proceso de la decodificaci√≥n de las im√°genes el cual involucra una modificaci√≥n en el tama√±o de la imagen y asignaci√≥n de etiquetas (aqu√≠ hay m√°s del proceso del mejoramiento de la imagen pero es un poco confuso)\n",
    "\n",
    "Divide la data en 5 conjuntos, despu√©s crea segmentos basados en las 6 _plant traits_ y las combina dentro de una columna final de segmento, al final, utiliza este segmento para balancear distribuciones similares de segmentos a trav√©s de los conjuntos. \n",
    "\n",
    "Antes de entrenar el modelo como tal, se aplica una normalizaci√≥n est√°ndar usando StandardScaler, esto asegura que los features tengan escalas consistentes, el cual es crucial para el desempe√±o √≥ptimo de las capas lineales o densas --**Build Train & Valid Dataset**--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb689b-a783-4210-887d-07519371ef3c",
   "metadata": {},
   "source": [
    "## Modelo 1: OpenL3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1456c3-f456-4ff5-ae08-bf6d6a1128ab",
   "metadata": {},
   "source": [
    "Las especificaciones de este modelo se pueden encontrar en la p√°gina de [GitHub](https://github.com/marl/openl3?tab=readme-ov-file) o su [Documentaci√≥n](https://openl3.readthedocs.io/en/latest/tutorial.html#introduction) oficial. [API Reference](https://openl3.readthedocs.io/en/latest/api.html) \n",
    "\n",
    "Papers de referencia:\n",
    "\n",
    "- [Paper 1: Look Listen and Learn](https://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf)\n",
    "\n",
    "- [Paper 2: Look, Listen and Learn more: design choices for deep audio embeddings](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf)\n",
    "\n",
    "\n",
    "Los par√°metros para los m√©todos de este modelo son los siguientes:\n",
    "\n",
    "- content_type: \"env\", \"music\" (default). \"music\" es para videos, im√°genes o m√∫sica\n",
    "\n",
    "- input_repr: \"linear\", \"mel128\" (default), \"mel256\"\n",
    "\n",
    "- embedding_size: 512, 8192 (default). Tama√±o del array resultante con el embedding de la imagen\n",
    "\n",
    "Si el embedding ya existe, entonces no crea uno nuevo, deja el \"original\"\n",
    "\n",
    "Para este modelo, existen 3 posibilidades:\n",
    "\n",
    "1. Puedes ejecutar el modelo directamente a una imagen (o lista de im√°genes) con el m√©todo \"get_image_embedding\"\n",
    "\n",
    "2. Puedes guardar el embedding en la misma carpeta de donde viene la imagen para un uso posterior. Para guardar el embedding es el m√©todo \"process_image_file\" y para cargarlo es el m√©todo \"np.load\" con np la librer√≠a \"numpy\"\n",
    "\n",
    "3. Puedes pre cargar desde un principio el modelo para que no est√©s cargandolo cada que lo requieras para una imagen. El m√©todo para pre cargar el modelo es \"openl3.models.load_image_embedding_model\", despu√©s, para usarlo en los m√©todos de los puntos anteriores, pasas el modelo con el argumento \"model\"\n",
    "\n",
    "- Del m√©todo \"imread\" obtuvimos un array de matrices sobre la imagen\n",
    "\n",
    "- De los m√©todos de openl3, los argumentos significan lo siguiente:\n",
    "\n",
    "     1. input_repr: Representaci√≥n del espectograma usado por el modelo. Es ignorado si el par√°metro \"modelo\" es un modelo de tipo Keras v√°lido. \"linear-frequency log-magnitude spectogram\", \"Mel-frequency log-magnitude spectogram\": este √∫ltimo captura informaci√≥n perceptivamente relevante de manera m√°s eficiente con menos bandas de frecuencia (128 o 256 bandas) que el espectograma lineal\n",
    "\n",
    "    2. content_type: Tipo de contenido utilizado para entrenar el modelo de embedding. Es ignorado si el par√°metro \"modelo\" es un modelo de tipo Keras v√°lido. \"music\" se refiere a contenido de m√∫sica como tal, instrumentos, tonos, etc; \"env\" es de environmental y se refiere a sonidos humanos o de la naturaleza, aquellos que son reproducidos de manera \"natural\"\n",
    "\n",
    "    3. embedding_size: Dimensi√≥n que tendr√° el embedding. Es ignorado si el par√°metro \"modelo\" es un modelo de tipo Keras v√°lido\n",
    "\n",
    "- Consideraciones extras sobre el modelo (obtenidas de los papers correspondientes):\n",
    "\n",
    "    1. El espectrograma \"Mel\" captura informaci√≥n de manera m√°s eficiente con menos bandas de frecuencia comparado con el espectrograma lineal, por eso siempre es mejor utilizar este argumento \"Mel\" a la hora de utilizar el modelo\n",
    "\n",
    "    2. En la realizaci√≥n del modelo se utiliz√≥ el optimizador Adam para minimizar la p√©rdida de la entropia cruzada binaria con regularizaci√≥n L2\n",
    " \n",
    "    3. Para las pruebas estad√≠sticas del modelo se utiliz√≥ la Prueba de rangos de Wilcoxon\n",
    " \n",
    "    4. El modelo L3 no requiere data con etiquetas, la representaci√≥n Mel es la mejor dentro de este modelo\n",
    " \n",
    "    5. El modelo L3 es no supervisado y aprende informaci√≥n a partir de entradas de de audio y visual al mismo tiempo\n",
    " \n",
    "    6. Dentro de este modelo se utiliza la t√©cnica de Activation Visualization el cual sirve para reconocer las regiones exactas de donde surgen los audios o videos, sus origenes.\n",
    " \n",
    "- Al final, se pudo ejecutar el modelo de manera correcta y se tiene ya un mejor y mayor entendimiento del rendimiento y uso del modelo.\n",
    "\n",
    "- **NOTA: Al ejecutar la funci√≥n me sale un aviso de que estoy ejecutando una funci√≥n costosa y me da los siguientes consejos para evitar estos cargos excesivos: Poner como argumento \"reduce_tracing = True\" o consultar documentaci√≥n de TensorFlow [Doc1](https://www.tensorflow.org/guide/function#controlling_retracing) [Doc2](https://www.tensorflow.org/api_docs/python/tf/function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2945cd4-835d-4077-bb31-6359ec2fd399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tentensorflow<1.14 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tentensorflow<1.14\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Librerias a utilizar en todo este proceso\n",
    "\n",
    "#!pip install openl3\n",
    "\n",
    "import openl3\n",
    "from skimage.io import imread\n",
    "import functions.general_functions as gf\n",
    "import os\n",
    "\n",
    "#Los embeddings se pueden leer con numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512dc05-3dfa-4892-8778-938211136968",
   "metadata": {},
   "source": [
    "### 1.1 Aplicaci√≥n directa del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9ba985-4708-4c9c-839f-5cb242f5d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre cargamos el modelo, al hacerlo solo una vez no es necesario pre cargar el modelo cada vez que se va a utilizar\n",
    "modelo = openl3.models.load_image_embedding_model(input_repr=\"mel256\", content_type=\"music\", embedding_size=512)\n",
    "\n",
    "#Variable global, de donde obtenemos la ruta de las im√°genes de entrenamiento y prueba\n",
    "ruta_imagenes_train = gf.get_data_path('train_images')\n",
    "ruta_imagenes_test = gf.get_data_path('test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf42efa-e983-4461-87e4-c9c6fcf90ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.175706  ,  1.5434446 ,  1.8470824 ,  1.6111743 ,  3.9059458 ,\n",
       "         1.6161033 ,  1.1251589 ,  1.8484113 ,  1.2820483 ,  1.2422009 ,\n",
       "         1.2345903 ,  0.83172244,  1.2606227 ,  1.6085217 ,  2.310515  ,\n",
       "         1.9807837 ,  2.63912   ,  1.8400848 ,  1.7005421 ,  1.7075837 ,\n",
       "         1.0354801 ,  2.1518202 ,  0.6044424 ,  1.4306686 ,  0.98116654,\n",
       "         0.777962  ,  3.3654976 ,  4.162442  ,  1.9882624 ,  0.7811913 ,\n",
       "         2.5927725 ,  1.8348336 ,  1.7911009 ,  1.8612864 ,  2.2643867 ,\n",
       "         2.5106506 ,  1.129749  ,  0.7803635 ,  1.5808517 ,  2.0452437 ,\n",
       "         0.7477303 ,  2.566805  ,  1.2202104 ,  2.673956  ,  1.3030437 ,\n",
       "         0.9613706 ,  1.4589942 ,  1.1933473 ,  1.6517575 ,  1.4095986 ,\n",
       "         1.3867158 ,  1.8570193 ,  3.5165267 ,  1.0719959 ,  0.7293594 ,\n",
       "         2.3112679 ,  0.84064364,  2.1612198 ,  3.0060468 ,  1.9224309 ,\n",
       "         1.1812272 ,  1.891209  ,  2.472405  ,  1.2888657 ,  1.6927787 ,\n",
       "         2.1506999 ,  1.3459386 ,  2.0038981 ,  1.5930213 ,  0.47655213,\n",
       "         2.6193697 ,  1.9717464 ,  4.062987  ,  1.1046907 ,  1.1110872 ,\n",
       "         2.361168  ,  2.197923  ,  2.2356052 ,  1.5180908 ,  1.4714471 ,\n",
       "         0.7321862 ,  2.3340888 ,  2.2026641 ,  2.900152  ,  1.8455327 ,\n",
       "         2.8180366 ,  3.579984  ,  1.7780606 ,  0.6885492 ,  0.74330634,\n",
       "         1.4548118 ,  1.7892987 ,  2.623982  ,  2.2822454 ,  2.6337693 ,\n",
       "         1.810327  ,  2.0195496 ,  1.8318466 ,  0.5800682 ,  1.9717218 ,\n",
       "         1.1088709 ,  2.4765005 ,  1.3142545 ,  2.3426805 ,  3.3546166 ,\n",
       "         1.8336228 ,  1.4230171 ,  1.3460764 ,  1.2419913 ,  1.6326134 ,\n",
       "         1.8541243 ,  3.3094888 ,  3.1520083 ,  1.1276817 ,  2.1059825 ,\n",
       "         1.4034028 ,  2.2198758 ,  1.3768513 ,  2.19128   ,  0.87318957,\n",
       "         1.9403172 ,  2.2162051 ,  2.1737297 ,  1.9482977 ,  3.2339306 ,\n",
       "         2.4979992 ,  3.8708446 ,  0.30502355,  1.1382095 ,  1.8605169 ,\n",
       "         2.1578095 ,  0.5733128 ,  0.98379403,  0.8280359 ,  0.8086305 ,\n",
       "         2.3863525 ,  2.9791481 ,  1.9045479 ,  1.9796925 ,  1.8616962 ,\n",
       "         3.3632848 ,  1.8634696 ,  1.107185  ,  1.4592223 ,  1.6343129 ,\n",
       "         1.3946321 ,  1.3572518 ,  0.35844022,  1.2527131 ,  3.150411  ,\n",
       "         1.6096046 ,  1.1554693 ,  2.1358314 ,  1.7029598 ,  1.141921  ,\n",
       "         1.6153053 ,  1.6055027 ,  3.1051896 ,  3.6214821 ,  1.6230621 ,\n",
       "         1.5834004 ,  1.6298965 ,  2.8244247 ,  1.1675341 ,  2.7904487 ,\n",
       "         1.7303566 ,  2.130632  ,  0.6963417 ,  1.751126  ,  1.3575698 ,\n",
       "         2.9695826 ,  1.3795626 ,  2.0887012 ,  2.9579601 ,  1.0759045 ,\n",
       "         0.6480726 ,  1.1689644 ,  2.195088  ,  0.7771737 ,  1.8708503 ,\n",
       "         1.973008  ,  2.593222  ,  1.3974954 ,  3.137685  ,  1.4733729 ,\n",
       "         2.9118552 ,  0.6655876 ,  1.1402754 ,  1.6189083 ,  2.1220033 ,\n",
       "         1.8366833 ,  1.8314385 ,  2.2287154 ,  3.9584134 ,  1.6328561 ,\n",
       "         2.620542  ,  1.4777068 ,  0.15441383,  1.1240487 ,  3.3462892 ,\n",
       "         1.7969214 ,  1.556514  ,  2.4305325 ,  0.8590141 ,  1.6037987 ,\n",
       "         2.3268068 ,  1.0716914 ,  2.353262  ,  0.7552768 ,  2.2437394 ,\n",
       "         1.8851045 ,  1.3443502 ,  1.2537194 ,  0.95854366,  1.3386508 ,\n",
       "         0.98735577,  2.4222198 ,  1.471299  ,  0.40153554,  0.9825933 ,\n",
       "         1.8274686 ,  2.1684446 ,  1.9626218 ,  3.0763166 ,  2.1571612 ,\n",
       "         1.7695723 ,  2.6577256 ,  1.9825742 ,  2.0900128 ,  1.4026421 ,\n",
       "         1.4734573 ,  1.1778079 ,  1.3406878 ,  1.0191549 ,  2.4203165 ,\n",
       "         0.9642346 ,  1.3775394 ,  1.2101616 ,  2.7640269 ,  2.3669791 ,\n",
       "         0.7761775 ,  2.118767  ,  1.8146111 ,  1.7414246 ,  1.2149911 ,\n",
       "         1.6906894 ,  1.9912071 ,  4.472443  ,  0.4392021 ,  1.4774317 ,\n",
       "         1.5085039 ,  0.56578064,  2.3826897 ,  1.9044176 ,  1.2921616 ,\n",
       "         0.982777  ,  1.7379091 ,  1.5467534 ,  0.9382145 ,  2.8912523 ,\n",
       "         2.1456141 ,  2.7281427 ,  2.3932753 ,  1.2604761 ,  2.3788674 ,\n",
       "         1.9280934 ,  2.3026574 ,  2.244354  ,  1.2846617 ,  1.1534966 ,\n",
       "         2.1021774 ,  1.657334  ,  1.0025334 ,  2.1684284 ,  1.6966815 ,\n",
       "         3.6313214 ,  1.7694956 ,  2.2314448 ,  0.88858825,  0.8019584 ,\n",
       "         0.42250943,  2.4249673 ,  2.2758982 ,  1.6080158 ,  1.3630352 ,\n",
       "         1.0489814 , -0.0961695 ,  0.0613366 ,  3.0018718 ,  2.657539  ,\n",
       "         1.7546085 ,  2.5809753 ,  2.8700643 ,  1.0275998 ,  2.5966926 ,\n",
       "         1.8877069 ,  0.92376304,  0.8094482 ,  1.3924993 ,  0.9060505 ,\n",
       "         0.7375886 ,  1.5616958 ,  0.9408794 ,  1.807155  ,  1.5401651 ,\n",
       "         2.1586545 ,  1.2999291 ,  1.3566748 ,  0.8188017 ,  0.20982033,\n",
       "         1.4128046 ,  2.732088  ,  2.1413565 ,  0.877346  ,  2.5168564 ,\n",
       "         2.8613496 ,  1.3230908 ,  1.6020036 ,  1.7247399 ,  1.008273  ,\n",
       "         1.827462  ,  3.3676603 ,  1.7086353 ,  0.81798834,  0.38888142,\n",
       "         1.4026177 ,  0.32413113,  1.3097413 ,  1.0313536 ,  1.7172253 ,\n",
       "         2.3557925 ,  1.3111204 ,  1.3517301 ,  0.45486632,  1.5576808 ,\n",
       "         2.4745257 ,  1.9757316 ,  1.4509815 ,  2.2598398 ,  0.9587231 ,\n",
       "         1.054393  ,  0.22588788,  1.3125238 ,  1.3522398 ,  1.6951782 ,\n",
       "         3.7570148 ,  2.4094355 ,  2.3221521 ,  0.7649872 ,  0.41615215,\n",
       "         2.2115865 ,  2.4118917 ,  2.3731947 ,  1.2105519 ,  0.14909582,\n",
       "         2.1226873 ,  1.071508  ,  1.3133154 ,  0.13713017,  1.3183657 ,\n",
       "         2.7107043 ,  3.4891021 ,  1.7466547 ,  2.1628833 ,  1.3982184 ,\n",
       "         0.69732356,  3.354535  ,  1.4641207 ,  1.2138011 ,  1.1258827 ,\n",
       "         2.3859258 ,  2.208928  ,  3.1228552 ,  0.7562111 ,  2.751453  ,\n",
       "         2.0329013 ,  2.5957615 ,  1.6002452 ,  1.8631349 ,  1.2944238 ,\n",
       "         4.089752  ,  1.2022967 ,  1.8000019 ,  3.009405  ,  0.61956596,\n",
       "         1.9118965 ,  1.7915144 ,  0.7789993 ,  0.9837974 ,  1.3568966 ,\n",
       "         0.3999674 ,  1.9177532 ,  0.95263   ,  2.9284015 ,  1.149992  ,\n",
       "         2.1916583 ,  1.2177802 ,  1.5857089 ,  2.1940875 ,  0.44687366,\n",
       "         2.3858454 ,  3.9504175 ,  0.5655163 ,  2.6389308 ,  0.15950087,\n",
       "         2.1763046 ,  0.97411096,  0.27542314,  1.9027846 ,  0.509019  ,\n",
       "         1.4849098 ,  1.5165368 ,  0.02513056,  1.8518919 ,  1.4084322 ,\n",
       "         2.115561  ,  3.5317254 ,  0.74647844,  1.6053514 ,  1.0835618 ,\n",
       "         1.3358855 ,  0.8422781 ,  1.0444161 ,  1.0711006 ,  3.2195356 ,\n",
       "         1.7400088 ,  3.472991  ,  2.0065453 ,  2.3838098 ,  1.6747029 ,\n",
       "         1.0819947 ,  2.4989796 ,  0.9762982 ,  0.6131284 ,  1.9837892 ,\n",
       "         1.5807332 ,  1.2274867 ,  1.3495158 ,  2.3833666 ,  2.5631557 ,\n",
       "         1.5764227 ,  2.9738996 ,  0.93225515,  2.347555  ,  0.77388114,\n",
       "         2.2777517 ,  2.3635151 ,  1.9934646 ,  3.1203682 ,  1.4682986 ,\n",
       "         1.6839352 ,  2.1998355 ,  0.62159514,  3.8899078 ,  1.1311991 ,\n",
       "         0.533277  ,  1.5396821 ,  1.8221093 ,  1.110822  ,  0.28853527,\n",
       "         1.3727562 ,  1.1685401 ,  1.3225605 ,  0.8580937 ,  1.8518877 ,\n",
       "         1.3909699 ,  3.2519639 ,  2.1847272 ,  0.913566  ,  0.7795896 ,\n",
       "         1.2857395 ,  1.1553231 ,  1.0589856 ,  1.4379913 ,  2.7278197 ,\n",
       "         1.6744288 ,  1.1484152 ,  0.85729074,  1.5635169 ,  1.4469551 ,\n",
       "         1.3510762 ,  2.0154874 ,  1.0346603 ,  2.4419563 ,  1.469019  ,\n",
       "         2.9397585 ,  1.2478424 ,  0.93469393,  0.9532465 ,  1.6992624 ,\n",
       "         2.7834325 ,  0.32698208,  1.1154722 ,  3.0087442 ,  1.8318268 ,\n",
       "         1.8996798 ,  1.71941   ,  0.7863625 ,  1.6181277 ,  1.6955439 ,\n",
       "         1.9642757 ,  1.7384559 ,  1.752031  ,  2.4207637 ,  2.0373354 ,\n",
       "         1.7210752 ,  1.866144  ,  1.0500181 ,  1.8593454 ,  1.6727269 ,\n",
       "         2.3892353 ,  1.1329664 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M√©todo para generar los embeddings de manera directa a una sola imagen\n",
    "\n",
    "#Nombre de la imagen a la cual aplicaremos el modelo\n",
    "imagen_name = '993123.jpeg'\n",
    "\n",
    "#Leemos la imagen\n",
    "imagen1 = imread(ruta_imagenes_train + imagen_name)\n",
    "#Generamos el embedding de la imagen de manera directa\n",
    "emb = openl3.get_image_embedding(imagen1, content_type=\"env\", input_repr=\"linear\", embedding_size=512)\n",
    "\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49b5a1-03cb-46de-bb64-dea1ea37768c",
   "metadata": {},
   "source": [
    "### 1.2 Guardar Embedding para uso futuro (Varisas im√°genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd6eab2-7011-44bf-9a3f-66677a1ca423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/998892.jpeg (1/2)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/998892.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/994535.jpeg (2/2)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/994535.npz exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "#Aplicaci√≥n del modelo a m√°s de una imagen\n",
    "\n",
    "#Rutas finales de las im√°genes a procesar\n",
    "\n",
    "imagen2 = ruta_imagenes_train + '998892.jpeg'\n",
    "imagen3 = ruta_imagenes_train + '994535.jpeg'\n",
    "#Lista para guardar todas las im√°genes\n",
    "imagen_array = [imagen2, imagen3]\n",
    "#M√©todo para guardar los embeddings de cada imagen en la misma carpeta de donde vienen las im√°genes\n",
    "openl3.process_image_file(imagen_array, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c128fd0a-7177-46bf-b751-598276d30784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17171964,  2.7841794 ,  0.58611256, ...,  0.55718577,\n",
       "        1.9009621 ,  1.2161709 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#En este m√©todo se leer√°n los embeddings de las im√°genes generadas en la secci√≥n anterior\n",
    "\n",
    "#Cargamos la data (embedding) de la imagen especificada\n",
    "data = np.load(ruta_imagenes_train + '998892.npz')\n",
    "#Obtenemos solo el embedding\n",
    "emb = data['embedding']\n",
    "\n",
    "emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f15cd4-5a29-450a-86e7-23b19270e59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/174383279.jpeg (1/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/174383279.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194747578.jpeg (2/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194747578.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/196588153.jpeg (3/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/196588153.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/8324721.jpeg (4/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/8324721.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/179983287.jpeg (5/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/179983287.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/56516675.jpeg (6/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/56516675.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194269576.jpeg (7/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/194269576.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/64653712.jpeg (8/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/64653712.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/195871735.jpeg (9/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/195871735.npz exists, skipping.\n",
      "openl3: Processing /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/188616414.jpeg (10/10)\n",
      "openl3: /Users/pedrovela/Docs/Datasets - ML/planttraits2024/train_images/188616414.npz exists, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.47553322,  2.9182515 , -0.14319089, ...,  0.74044186,\n",
       "        2.1243396 , -0.49356037], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#En esta secci√≥n se aplica el embedding a 10 im√°genes distintas dentro de la carpeta de train images de manera directa\n",
    "\n",
    "#Obtenemos la lista con los nombres (id's) de las im√°genes de entrenamiento\n",
    "train_file_names = os.listdir(ruta_imagenes_train)\n",
    "#Nos quedamos solo con las primeras 10 im√°genes\n",
    "train_file_names = train_file_names[:10]\n",
    "\n",
    "#Concatenamos el resto de la ruta del archivo al nombre de cada imagen\n",
    "train_complete_file_names = [ruta_imagenes_train + x for x in train_file_names]\n",
    "\n",
    "#M√©todo para guardar los embeddings de cada imagen en la misma carpeta de donde vienen las im√°genes\n",
    "openl3.process_image_file(train_complete_file_names, batch_size = 32)\n",
    "\n",
    "#Empezamos con el proceso de cargar los embeddings de cada imagen\n",
    "\n",
    "#Lista en donde se guardar√° cada embedding\n",
    "embs_files = list()\n",
    "#Loop para ir guardando cada embedding\n",
    "for imagen in train_file_names:\n",
    "    #Cargamos la data (embedding) de la imagen especificada\n",
    "    data = np.load(ruta_imagenes_train + imagen[:imagen.find('.')] + '.npz')\n",
    "    #Obtenemos solo el embedding\n",
    "    embs_files.append(data['embedding'][0])\n",
    "\n",
    "embs_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d10175-9b11-4e7e-8060-964125f85bff",
   "metadata": {},
   "source": [
    "### 1.3 Uso del modelo pre cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86c25a7-d2d9-43cf-a914-bc5f83688ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.20485121,  2.7495792 ,  1.1525512 ,  1.6354275 ,  1.5630054 ,\n",
       "         3.0043402 ,  1.1680847 ,  0.20437273,  0.47765186,  1.6069542 ,\n",
       "         0.99042135,  2.744746  ,  3.0113118 ,  0.697831  ,  2.5043488 ,\n",
       "         2.8977034 ,  2.1103582 ,  1.0919865 ,  2.5187943 ,  2.6752896 ,\n",
       "         2.5137546 ,  0.80452126,  0.9297262 ,  2.2770233 ,  2.8381968 ,\n",
       "         0.6253177 ,  0.7834051 ,  1.436738  ,  1.1495699 ,  1.3403784 ,\n",
       "         1.7564527 ,  1.0250019 ,  2.2259736 ,  0.5685906 ,  2.5769222 ,\n",
       "         0.8931727 ,  2.3390589 ,  1.2697175 ,  1.2542069 ,  1.3876815 ,\n",
       "         1.3700166 ,  1.7157243 ,  0.76253283,  1.8189112 ,  0.24554229,\n",
       "         1.335274  ,  1.7735906 ,  1.3587192 ,  1.4913703 ,  1.041074  ,\n",
       "         0.53341097,  0.9961289 ,  0.8008581 ,  1.6766714 ,  1.8453351 ,\n",
       "         1.4003036 ,  1.7122384 ,  1.1727496 ,  1.851693  ,  2.0431597 ,\n",
       "         2.2497199 ,  1.0162674 ,  2.1898563 ,  1.1334101 ,  0.64272827,\n",
       "         1.9252778 ,  2.6810825 ,  1.1118598 ,  2.4728882 ,  2.0378585 ,\n",
       "         0.18904294,  0.79025114,  2.4218001 ,  2.8007655 ,  1.5280496 ,\n",
       "         1.5474136 ,  1.9214152 ,  1.3252783 ,  1.7291185 ,  1.6519809 ,\n",
       "         1.6730304 ,  0.71860415,  1.2508231 ,  1.5200086 ,  2.0593126 ,\n",
       "         1.9230796 ,  1.7712485 ,  3.1574547 ,  1.2433559 ,  2.713454  ,\n",
       "         1.4150548 ,  0.8275679 ,  0.17761555,  1.4295996 ,  0.402724  ,\n",
       "         3.7560945 ,  2.9716074 ,  2.1054938 ,  1.4818022 ,  2.4419835 ,\n",
       "         1.5843626 ,  1.7393502 ,  1.9626493 ,  1.830607  ,  0.7150854 ,\n",
       "         1.1647476 ,  1.1310599 ,  1.9774538 ,  1.4355531 ,  1.5156869 ,\n",
       "         1.5979875 ,  0.5193798 ,  1.9295925 ,  3.1267235 ,  0.37254888,\n",
       "         1.7504405 ,  2.0851407 ,  1.2470307 ,  1.0443157 , -0.09026211,\n",
       "         3.181277  ,  0.9165494 ,  1.2001781 ,  2.5041108 ,  0.9305678 ,\n",
       "         0.09242424,  1.0307933 ,  1.8854965 ,  1.5079899 ,  1.8392226 ,\n",
       "         1.8071203 ,  0.6490128 ,  1.5491434 ,  2.1097796 ,  0.8568534 ,\n",
       "         1.0614096 ,  1.536929  ,  1.2888623 ,  2.3350706 ,  0.6976912 ,\n",
       "         0.8105583 ,  1.9423215 ,  0.6261319 ,  2.3393662 ,  1.6619316 ,\n",
       "         2.0141628 ,  1.1772301 ,  0.92156947,  1.8653541 ,  1.2902074 ,\n",
       "         2.5316038 ,  0.24059922,  2.2795978 ,  1.9879104 ,  1.425057  ,\n",
       "         0.92885405,  1.5962604 ,  1.719163  ,  0.79301786,  2.2725556 ,\n",
       "         1.0584172 ,  1.4747607 ,  2.0802364 ,  0.7287637 ,  0.02222875,\n",
       "         0.68939507,  1.3485879 ,  0.55696934,  1.7018901 ,  1.2952051 ,\n",
       "         0.9304137 ,  0.71346456,  1.0721709 ,  2.3936746 ,  1.3517103 ,\n",
       "         1.8400532 ,  3.0535743 ,  1.5248033 ,  2.1646209 ,  2.8433282 ,\n",
       "         0.7472565 ,  1.3872107 ,  1.4184986 ,  1.7283857 ,  1.9101846 ,\n",
       "         3.4059415 ,  1.0183837 ,  1.2752259 ,  0.52478236,  1.4572835 ,\n",
       "         1.8859854 ,  1.1009525 ,  2.018782  ,  2.5567327 ,  2.2113345 ,\n",
       "         1.599143  ,  1.7910511 ,  0.9189052 ,  1.5837723 ,  1.4791529 ,\n",
       "         1.2947991 ,  2.3006215 ,  2.7331173 ,  1.2402371 ,  0.98130715,\n",
       "         1.3301815 ,  1.9842585 ,  1.447342  ,  1.2027946 ,  1.9843901 ,\n",
       "         2.4563801 ,  1.3951555 ,  0.72004324,  1.7766222 ,  1.9360775 ,\n",
       "         3.3049457 ,  0.8194702 ,  1.4591975 ,  1.3551744 ,  2.109625  ,\n",
       "         2.322318  ,  2.5884986 ,  2.1426508 ,  0.669836  ,  3.2447402 ,\n",
       "         1.3465672 ,  1.6958401 ,  1.9680761 ,  1.6791688 ,  2.2260683 ,\n",
       "         0.70770514,  2.1221418 ,  1.3575913 ,  0.82252675,  1.5782276 ,\n",
       "         1.5124403 ,  0.96351004,  1.4934953 ,  1.4218223 ,  2.134638  ,\n",
       "         1.7454535 ,  2.6233408 ,  1.7791142 ,  2.0155365 ,  2.0383744 ,\n",
       "         1.2378262 ,  1.0985494 ,  1.5937773 ,  1.8554164 ,  0.17792201,\n",
       "         0.53817767,  0.98444164,  1.4498764 ,  1.3323634 ,  2.3908966 ,\n",
       "         1.3856658 ,  1.3186507 ,  0.9486842 ,  3.846665  ,  1.5070676 ,\n",
       "         0.33823866,  0.7877687 , -0.16274174,  1.0884465 ,  1.9007766 ,\n",
       "         2.5469697 ,  0.66021   ,  1.5684887 ,  1.2924713 ,  0.63552445,\n",
       "         1.5935831 ,  1.4875512 ,  1.5960518 ,  0.8582631 ,  2.1310747 ,\n",
       "         1.0317056 ,  1.0481809 ,  1.8717718 ,  2.0543463 ,  1.1496912 ,\n",
       "         2.888617  ,  1.7371292 ,  2.2067773 ,  3.1110735 ,  1.0411993 ,\n",
       "         0.27202043,  0.9156175 ,  0.5618163 ,  1.6236361 ,  1.6475532 ,\n",
       "         2.3125062 ,  0.49094838,  1.1980839 ,  1.3158065 ,  3.740868  ,\n",
       "         1.9677469 ,  1.0197357 ,  1.1762849 ,  0.7334433 ,  0.34872186,\n",
       "         0.7972447 ,  1.5106252 ,  2.366034  ,  2.192229  ,  1.6929114 ,\n",
       "         1.4890344 ,  3.0548801 ,  1.0835251 ,  2.1524706 ,  0.96201026,\n",
       "         1.0682323 ,  2.3660114 ,  1.1081941 ,  0.83726406,  1.2536136 ,\n",
       "         2.2398672 ,  2.112317  ,  2.3908331 ,  0.47984797,  2.062965  ,\n",
       "         0.38893068,  1.8647356 ,  1.670881  ,  0.6539917 ,  1.2611164 ,\n",
       "         1.5717337 ,  0.50576305,  2.5663433 ,  1.8686793 ,  2.172477  ,\n",
       "         1.0179551 ,  1.0023283 ,  1.2324402 ,  1.3330743 ,  1.8540252 ,\n",
       "         2.8859663 ,  1.2206006 ,  2.1910655 ,  0.19158404,  1.1950855 ,\n",
       "         1.1625215 ,  0.5881799 ,  1.9052645 ,  0.8708216 ,  1.4036076 ,\n",
       "         0.9691802 ,  0.95226324,  1.4744202 ,  1.4248235 ,  1.2367972 ,\n",
       "         2.808134  ,  2.4231102 ,  0.5292709 ,  2.915683  ,  0.8944221 ,\n",
       "         1.0192145 ,  3.3238676 ,  1.347873  ,  1.9790424 ,  0.4238795 ,\n",
       "         1.3618789 ,  2.133664  ,  1.2368494 ,  1.7652725 ,  1.6785655 ,\n",
       "         1.1529132 ,  3.577917  ,  1.3276899 ,  3.103002  ,  1.8847266 ,\n",
       "         0.86568314,  1.7473139 ,  2.0703397 ,  1.0410285 ,  1.8536546 ,\n",
       "         1.6400666 ,  0.85971665,  0.5454826 ,  1.9495049 ,  1.6841565 ,\n",
       "         2.3934028 ,  2.4245036 ,  1.7368637 ,  1.1532226 ,  3.5255368 ,\n",
       "         2.6359017 ,  2.0186172 ,  0.97741675,  3.3602288 ,  1.6372435 ,\n",
       "         1.0248289 ,  1.292603  ,  0.53276104,  2.1690738 ,  0.18544208,\n",
       "         1.077914  ,  1.9338853 ,  3.343952  ,  1.8495357 ,  2.0533946 ,\n",
       "         2.0159342 ,  1.9051903 ,  2.091736  ,  1.2920363 ,  1.8911469 ,\n",
       "         2.4060938 ,  2.1457841 ,  0.90916306,  2.5142744 ,  1.8741465 ,\n",
       "         1.0483673 ,  1.9130058 ,  1.5597029 ,  1.6793998 ,  0.8772772 ,\n",
       "         0.43882233,  2.4843915 ,  3.2221475 ,  1.9798054 ,  1.8952355 ,\n",
       "         1.946561  ,  1.1553272 ,  1.1124197 ,  0.7784294 ,  0.44111228,\n",
       "         1.2199454 ,  0.80838984,  1.7045665 ,  1.4436787 ,  0.6235423 ,\n",
       "         1.405282  ,  0.7928036 ,  2.8732536 ,  1.6124893 ,  1.6760724 ,\n",
       "         0.7731418 ,  1.2888249 ,  1.5158899 ,  1.500979  ,  0.69608504,\n",
       "         2.3611884 ,  1.0385586 ,  2.2581875 ,  1.2921201 ,  2.0748973 ,\n",
       "         1.8155406 ,  2.4921267 ,  2.6781292 ,  1.3267325 ,  2.2051191 ,\n",
       "         0.9098008 ,  0.9012551 ,  0.12941791,  2.409787  ,  0.6900594 ,\n",
       "         2.1228986 ,  2.2754078 ,  1.1853325 ,  0.4757692 ,  1.8257791 ,\n",
       "         0.93363845,  0.8266524 ,  1.4521326 ,  1.8899059 ,  1.731703  ,\n",
       "         1.3749826 ,  2.3283339 ,  1.0289295 ,  0.7492265 ,  2.640315  ,\n",
       "         0.99254185,  1.5912545 ,  1.7530429 ,  1.5952451 ,  0.78447586,\n",
       "         2.51337   ,  2.1093833 ,  1.606523  ,  1.1113402 ,  0.39814588,\n",
       "         1.3742307 ,  0.92801857,  2.9369183 ,  1.7763023 ,  1.0433584 ,\n",
       "         0.09030688,  0.3081057 ,  1.561068  ,  1.2821492 ,  1.9407334 ,\n",
       "         2.2916896 ,  1.6688268 ,  0.95677584,  2.0200806 ,  1.8097007 ,\n",
       "         0.8366819 , -0.2874099 ,  1.8230293 ,  1.1122035 ,  2.466584  ,\n",
       "         3.9499621 ,  1.5631782 ,  1.0250038 ,  1.4679947 ,  2.22061   ,\n",
       "         0.03978098,  3.3677886 ,  1.2276036 ,  2.0392818 ,  1.563246  ,\n",
       "         1.9572912 ,  1.6245346 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M√©todo en el cual generamos el embedding de la imagen ya con el modelo pre cargado\n",
    "\n",
    "#Obtenemos el embedding final\n",
    "emb = openl3.get_image_embedding(imagen1, model=modelo)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9465fa9-0af9-4ea4-bdbd-eca5ea6be2ee",
   "metadata": {},
   "source": [
    "## Modelo 2: ViT (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be12689-b62f-4b51-af77-dd7016b27707",
   "metadata": {},
   "source": [
    "[GitHub](https://github.com/lukemelas/PyTorch-Pretrained-ViT?tab=readme-ov-file) , [Colab](https://colab.research.google.com/drive/1muZ4QFgVfwALgqmrfOkp7trAvqDemckO?usp=sharing)\n",
    "\n",
    "Papers de referencia:\n",
    "\n",
    "- [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://openreview.net/pdf?id=YicbFdNTTy)\n",
    "\n",
    "- [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)\n",
    " \n",
    "- [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/pdf/2106.10270)\n",
    "   \n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v7)\n",
    "\n",
    "Proceso general del modelo:\n",
    "\n",
    "1. Considera una imagen como una secuencia mono-dimensional (1 dimensi√≥n) de parches (token embeddings)\n",
    "  \n",
    "2. Antepone un token de clasificaci√≥n a la secuencia generada en el punto 1\n",
    "\n",
    "3. Pone estos parches a trav√©s de un codificador transformador (como BERT)\n",
    "\n",
    "4. Pasa el primer token del output generado por el transformador a trav√©s de un peque√±o MLP (Multilayer Perceptron) para obtener los logits de clasificaci√≥n. Deja una capa escondida para pre-entrenamiento y una sola capa lineal para el fine-tunning\n",
    "\n",
    "Puntos sobre la lectura:\n",
    "\n",
    "- Se divide una imagen en parches y provee la secuencia de embeddings lineales de esos parches como entrada a un Transformador\n",
    "\n",
    "- Los parches son tratados como Tokens (palabras) en una aplicaci√≥n NLP\n",
    "\n",
    "- El encoder Transformer consiste en alternar capas de multiheaded self-attention (MSA) y bloques MLP\n",
    "\n",
    "- En este modelo existen dos tipos de capas: las capas MLP y MSA\n",
    "\n",
    "- La longitud de la secuencia del Transformador es inversamente proporcional al cuadrado del tama√±o del parche, por lo que modelos con tama√±os peque√±os de parches son computacionalmente m√°s caros\n",
    "\n",
    "- Aqu√≠ tambi√©n se utiliza el optmizador Adam (la otra alternativa es SGD), es un optimizador estoc√°stico. Tambi√©n existe el optmizador RAdam que brinda mejor control en la varianza del gradiente, el cual es necesario cuando el modelo se entrena a un valor alto de LR; RAdam detecta inestabilidad en la varianza y cambia el LR de manera suave para evitar la divergencia en las etapas iniciales del entrenamiento.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation)**: Combina el optimizador RMSProp (Root Mean Square Propagation) con el Momentum. $\\beta_1$ es el factor de olvido del gradiente y $\\beta_2$ es el factor de olvido del segundo momento del gradiente\n",
    "\n",
    "- **FLOPs (Floating Point Operations)**: M√©tricas que se utilizan com√∫nmente para calcular la complejidad computacional de los modelos de deep learning, sirve para entender f√°cilmente el n√∫mero de operaciones aritm√©ticas requeridas para realizar cierta operacion computacional. N√∫mero de operaciones de punto flotante (suma, resta, multiplicaci√≥n y divisi√≥n en n√∫meros de punto flotante). Al optimizar esta m√©trica podemos reducir la energ√≠a requerida para correr nuestra red neuronal [Ref 1](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html) [Ref 2](https://medium.com/@pashashaik/a-guide-to-hand-calculating-flops-and-macs-fa5221ce5ccc)\n",
    "\n",
    "- **EPOCH**: Se refiere al pase entero del conjunto de datos de entrenamientoa trav√©s de los algoritmos. Es un hiper-parametro que determina el proceso de entrenar el modelo entero de ML. Es cuando pasas TODO tu conjunto de entrenamiento a trav√©s del modelo y entonces se define como el n√∫mero total de iteraciones que tuvo todo tu conjunto de entrenamiento para entrenar el modelo de ML, un \"paso\" de la data de entrenamiento se toma en cuenta cuando √©sta haya pasado tanto por delante como por detr√°s en todos los procesos del modelo. Un epoch est√° completo cuando ha procesado todos los _batches_ y ha actualizado sus par√°metros basados en el c√°lculo de p√©rdida. De manera general, incrementando el n√∫mero de epochs mejora el performance del modelo al permitirle aprender comportamientos m√°s complejos en la data, pero si existen muchos epochs, el modelo se sobre-entrena. Iteraci√≥n completa: Procesar cada batch de la data de entrenamiento, calcular la p√©rdida y actualizar los par√°metros del modelo. Ayuda a procesar o entrenar el modelo con un conjunto enorme de datos, si se tiene un conjunto enorme de datos y no es posible entrenar todos los datos de una sola vez (debido a memoria), los epochs te permiten entrenar el modelo en mini batches independientes que se realiza uno por uno. [Ref 1](https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-epoch-in-machine-learning#:~:text=Machine%20learning%20models%20are%20trained,training%20data%20through%20the%20algorithm.) [Ref 2](https://www.geeksforgeeks.org/epoch-in-machine-learning/)\n",
    "\n",
    "- **LEARNING RATE WARMUP**: Proceso para mejorar la optimizaci√≥n del modelo; incrementa gradualmente la taza (rate) de aprendizaje (tama√±o del paso tomado en cada iteraci√≥n para actualizar los par√°metros del modelo) durante las etapas iniciales del entrenamiento antes de iniciar con las tazas de aprendizaje programadas; se incrementa linealmente la taza de aprendizaje durante las iteraciones iniciales y despu√©s se continua con las tazas programadas de aprendizaje. Se necesita ajustar los par√°metros del calentamiento (warm-up) basado en caracter√≠sticas espec√≠ficas del modelo y conjunto de datos. [Ref 1](https://techkluster.com/technology/learning-rate-warm-up/#:~:text=Learning%20rate%20warm%2Dup%20involves,how%20it%20can%20be%20implemented.) [Ref 2](https://www.baeldung.com/cs/learning-rate-warm-up)\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent)**: M√©todo iterativo para optimizar la funci√≥n objetivo con propiedades de suavidad adecuadas (diferenciabilidad, sub-diferenciabilidad), puede ser tomado como la aproximaci√≥n estoc√°stica de la t√©cnica gradiente descendiente, ya que reemplaza el actual gradiente por otro gradiente estimado a partir de un subconjunto aleatorio de la data. Aqu√≠, dentro del proceso de optimizaci√≥n de la funci√≥n objetivo, en vez de calcular el verdadero gradiente de la funci√≥n, lo que se hace es estimar ese gradiente con el gradiente de un solo registro o muestra, despu√©s actualiza este valor para cada entrada (muestra) del conjunto de entrenamiento hasta que se logra la conversi√≥n. Aqu√≠ se beneficia el costo computacional por iteraci√≥n, aunque de manera general toma un n√∫mero m√°s alto de iteraciones que el m√©todo tradicional de gradiente descendiente. Este m√©todo es mejor para casos en donde el conjunto de datos es grande. Una elecci√≥n de un valor alto de LR puede llevar a que el modelo tome pasos grandes en cada iteraci√≥n y \"pase\" de largo el valor m√≠nimo y una elecci√≥n de LR baja puede ocasionar que el modelo converga lentamente. [Ref 1](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) [Ref 2](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/) [Ref 3](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31) [Ref 4](https://scikit-learn.org/stable/modules/sgd.html)\n",
    "\n",
    "- fine-tunning con SGD con momentum de 0.9, el fine-tunning general del modelo se hace con una resoluci√≥n de 384 (a menos que se estipule otra cosa). EL Vision Transformer se beneficia con mejores resoluciones.\n",
    "\n",
    "- SGD con momentum recuerda la actualizaci√≥n del cambio en el par√°metro w que minimiza la funci√≥n objetivo en cada iteraci√≥n y determina la sig. actualizaci√≥n del par√°metro como una combinaci√≥n lineal del gradiente y la actualizaci√≥n anterior. Utiliza un factor de decadencia entre 0 y 1 que determina la contribuci√≥n relativa del actual gradiente y gradientes anteriores con respecto al cambio en los pesos. \n",
    "\n",
    "R√°pida descripci√≥n de la inspecci√≥n interna de ViT:\n",
    "\n",
    "1. La primera capa de ViT proyecta linealmente los parches aplanados a un espacio con dimensi√≥n menor\n",
    "\n",
    "2. Se agrega una posici√≥n de embedding a cada parche. El modelo aprende a calcular la distancia (similitud) entre las imagenes con respecto a su posici√≥n de embedding; parches cercanos tienden a tener posiciones de embeddings m√°s similares. Esta distancia es de coseno entre la posici√≥n del embedding del parche de un determinado rengl√≥n y columna y la posici√≥n del embedding de los dem√°s parches. Parches que se encuentran en la misma columna o rengl√≥n tienen embeddings similares\n",
    "\n",
    "3. Una estructura sinusoidal (de tipo seno) se utiliza t√≠picamente para mallas grandes\n",
    "\n",
    "4. Self-attention permite a ViT integrar informaci√≥n a trav√©s de toda la imagen, incluso en las capas m√°s bajas. Es una distancia que se mide entre el pixel de consulta (query) y los dem√°s pixeles, todas con un peso de atenci√≥n (weight). Mientras la capa sea m√°s baja (profunda), la atenci√≥n que se le da a la imagen es mayor.\n",
    "\n",
    "5. El modelo atiende a regiones de la imagen que son sem√°nticamente relevantes para la clasificaci√≥n\n",
    "\n",
    "\n",
    "**TRATAR DE REPLICAR ESTOS EJEMPLOS:** [Kaggle Example](https://www.kaggle.com/code/raufmomin/vision-transformer-vit-from-scratch) [Medium Example](https://medium.com/@diego.machado/fine-tuning-vit-for-image-classification-with-hugging-face-48c4be31e367) [Busqueda de Google (referencias)](https://www.google.com/search?q=using+vit+model+with+csv+file&client=safari&sca_esv=41a53a54b5bc3fc7&sca_upv=1&sxsrf=ADLYWIL7bNtclYzSlPXpYVpatUTD_zwpYw%3A1723677174417&source=hp&ei=9jm9Zry5F-7HkPIPiLy8iAo&iflsig=AL9hbdgAAAAAZr1IBha6iJ9JBpkWgf9h1nv91qK1KZIl&ved=0ahUKEwi8jL_tzfWHAxXuI0QIHQgeD6EQ4dUDCBY&uact=5&oq=using+vit+model+with+csv+file&gs_lp=Egdnd3Mtd2l6Ih11c2luZyB2aXQgbW9kZWwgd2l0aCBjc3YgZmlsZTIFECEYoAFIsj9QAFjZPnAAeACQAQCYAZ0BoAG_GKoBBDYuMjK4AQPIAQD4AQGYAhugArIYwgIKECMYgAQYJxiKBcICBBAjGCfCAg4QABiABBixAxiDARiKBcICERAuGIAEGLEDGNEDGIMBGMcBwgILEC4YgAQYsQMYgwHCAgsQABiABBixAxiDAcICBRAAGIAEwgIREC4YgAQYsQMYxwEYjgUYrwHCAggQLhiABBixA8ICCBAAGIAEGLEDwgIFEC4YgATCAg4QLhiABBjHARiOBRivAcICChAAGIAEGLEDGArCAgsQLhiABBjHARivAcICDRAAGIAEGLEDGEYY_wHCAgYQABgWGB7CAgcQABiABBgTwgIJEAAYgAQYExgKwgIIEAAYExgWGB7CAgQQIRgVmAMAkgcEMy4yNKAH734&sclient=gws-wiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f46b1-10d9-42b9-b227-c36c5710dbe7",
   "metadata": {},
   "source": [
    "El siguiente ejemplo se tom√≥ de [GitHub](https://github.com/lukemelas/PyTorch-Pretrained-ViT?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb8d86c-6edf-4ef0-8623-75b4e5c7bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_vit\n",
      "  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch (from pytorch_pretrained_vit)\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch->pytorch_pretrained_vit)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from torch->pytorch_pretrained_vit) (4.12.2)\n",
      "Collecting sympy (from torch->pytorch_pretrained_vit)\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from torch->pytorch_pretrained_vit) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from torch->pytorch_pretrained_vit) (3.1.4)\n",
      "Collecting fsspec (from torch->pytorch_pretrained_vit)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pedrovela/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages (from jinja2->torch->pytorch_pretrained_vit) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->pytorch_pretrained_vit)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pytorch_pretrained_vit\n",
      "  Building wheel for pytorch_pretrained_vit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorch_pretrained_vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11114 sha256=9f5a18f9e5c23c5849b1b1a6b48aaaf0e5b85aafac1e211526c97205ad4ea5e6\n",
      "  Stored in directory: /Users/pedrovela/Library/Caches/pip/wheels/79/c8/4f/9ad72c6f247a7e6888cc7767db9632675cf82656fffec85518\n",
      "Successfully built pytorch_pretrained_vit\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch, pytorch_pretrained_vit\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 mpmath-1.3.0 pytorch_pretrained_vit-0.0.7 sympy-1.13.2 torch-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\" to /Users/pedrovela/.cache/torch/hub/checkpoints/B_16_imagenet1k.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 331M/331M [00:08<00:00, 38.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "#Librerias a importar\n",
    "\n",
    "#!pip3 install pytorch_pretrained_vit\n",
    "\n",
    "from pytorch_pretrained_vit import ViT\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dadbf64-964c-4820-a00e-6802fb515b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n",
      "torch.Size([1, 3, 384, 384])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install torchvision\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from functions import general_functions as gf\n",
    "\n",
    "ruta_imagenes_train = gf.get_data_path('train_images')\n",
    "img_name = ruta_imagenes_train + '993123.jpeg'\n",
    "\n",
    "# Load ViT\n",
    "from pytorch_pretrained_vit import ViT\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load image\n",
    "# NOTE: Assumes an image `img.jpg` exists in the current directory\n",
    "img = transforms.Compose([\n",
    "    transforms.Resize((384, 384)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])(Image.open(img_name)).unsqueeze(0)\n",
    "print(img.shape) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "# Classify\n",
    "with torch.no_grad():\n",
    "    outputs = model(img).squeeze(0)\n",
    "print(outputs.shape)  # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4af481-bb40-4adb-96dd-4be416557bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1168e-02, -3.6647e-01, -8.2826e-02, -5.2003e-03, -6.9432e-01,\n",
       "         6.3112e-01,  1.1538e-01, -9.5176e-01, -2.8889e-01,  1.1204e-01,\n",
       "         9.6513e-01, -3.8003e-01, -2.1552e-01, -6.1709e-01,  8.0917e-01,\n",
       "        -2.5889e-01,  5.4547e-02, -1.1185e+00, -5.8974e-01, -3.2512e-01,\n",
       "        -2.5198e-01,  5.5554e-01, -2.1421e-02,  3.8290e-01, -2.5087e-02,\n",
       "         1.9608e+00,  3.3388e+00,  2.1743e+00,  1.2444e+00,  8.5103e-01,\n",
       "         9.4214e-01,  1.1039e+00,  2.9827e+00, -1.2655e+00,  3.1554e-01,\n",
       "         9.3914e-01,  8.1178e-01,  4.6449e-01,  2.8589e+00,  3.0353e-01,\n",
       "         1.3648e+00,  3.1621e+00,  7.6384e-01,  1.6017e+00,  2.9447e+00,\n",
       "         1.9694e+00,  1.7545e+00,  2.4520e+00, -8.3982e-03,  6.9470e-02,\n",
       "        -5.9563e-01, -4.6310e-01,  3.2046e+00,  2.7494e+00,  2.3458e+00,\n",
       "         1.7325e+00,  2.8559e+00,  1.9144e+00,  9.2152e-01,  2.1196e+00,\n",
       "         1.6230e+00,  9.6469e-01,  1.4489e+00,  1.3848e+00,  1.8677e+00,\n",
       "         6.0899e-01,  1.9174e+00,  2.1616e+00,  2.7669e+00, -9.9660e-01,\n",
       "         3.9333e+00,  1.6711e+00,  2.3127e+00,  3.5069e+00,  2.0710e+00,\n",
       "         1.6983e+00,  1.0906e+00,  2.0403e+00,  3.1204e+00,  2.2113e+00,\n",
       "         1.3856e+00,  1.9282e+00,  1.4429e+00,  5.3731e-01,  3.4887e-01,\n",
       "        -2.4345e-01,  1.3111e+00, -1.1046e+00,  4.2548e-02,  1.0457e+00,\n",
       "         4.3298e-01,  1.3300e-01,  4.2804e-01, -9.9829e-01,  6.1586e-01,\n",
       "         6.7688e-01, -1.7117e+00, -7.2654e-01,  3.1594e-01, -4.0683e-01,\n",
       "         3.2949e-01, -1.0752e+00,  1.6184e+00, -1.2399e+00, -6.2792e-01,\n",
       "        -1.0170e+00, -1.4297e-01, -1.3180e-01,  8.4345e-01, -8.0051e-01,\n",
       "         1.5062e+00,  2.0540e+00, -1.0911e-01,  2.1944e+00,  3.2397e+00,\n",
       "         7.3684e-01, -2.1486e-01, -1.9343e-01,  4.1691e-01,  1.4511e+00,\n",
       "         1.4117e+00,  5.7832e-02, -7.1749e-03, -8.0117e-01,  3.1938e-01,\n",
       "         2.5990e-01,  2.2331e+00, -1.1631e+00, -7.5400e-02, -1.7564e+00,\n",
       "         9.8538e-03, -9.5136e-01, -5.4334e-01,  5.3501e-03, -1.0813e+00,\n",
       "         1.1229e-01, -5.8678e-01, -8.3818e-01,  1.0756e+00,  8.1807e-01,\n",
       "         2.8066e-01,  5.0494e-01,  5.5894e-01,  6.9712e-01, -9.8801e-01,\n",
       "        -2.4797e-01,  3.9322e-01, -2.0777e-01,  2.6230e-02, -7.3113e-01,\n",
       "         4.2627e-01, -1.0014e-01, -8.3944e-01, -7.1207e-01, -1.3300e+00,\n",
       "        -1.0026e+00, -1.0011e+00, -3.5355e-01, -2.7932e-01, -6.3326e-01,\n",
       "        -2.0266e+00, -5.0000e-01,  7.0404e-02, -2.1838e-01, -5.8161e-01,\n",
       "        -2.7019e-01,  4.4338e-01, -9.5197e-01, -8.2223e-02, -9.1261e-01,\n",
       "        -1.6013e+00,  9.7389e-01, -5.9385e-01, -1.1468e+00, -5.5552e-01,\n",
       "        -1.7261e+00, -1.1529e+00, -5.5075e-01,  6.5345e-01, -5.6858e-02,\n",
       "        -1.4461e-01,  3.4404e-01, -1.7313e+00, -1.1631e+00, -1.4553e+00,\n",
       "        -8.3759e-01, -1.4087e+00, -7.5500e-01, -1.0823e+00, -1.1800e+00,\n",
       "        -1.3814e+00, -1.3580e+00, -1.1169e+00, -1.0051e+00, -7.8645e-01,\n",
       "         4.0795e-01, -8.8525e-01, -2.0180e+00, -1.8984e+00, -1.5952e+00,\n",
       "        -1.8436e+00, -8.4293e-01, -2.2927e+00, -8.8634e-01, -1.0539e+00,\n",
       "        -1.0166e+00, -7.7361e-01, -1.1645e+00, -5.4145e-02, -9.3864e-01,\n",
       "         2.2528e-01,  4.5755e-01, -1.0382e+00, -1.7036e+00, -2.0323e+00,\n",
       "        -1.1887e+00, -1.7114e+00, -9.9984e-01, -7.3405e-01, -1.9381e+00,\n",
       "        -1.8301e+00, -6.4421e-01, -7.5570e-01, -2.8457e-01, -1.4033e+00,\n",
       "        -1.3007e+00, -1.8446e+00, -1.3030e+00, -1.4239e+00, -1.4555e+00,\n",
       "        -8.3883e-01, -1.4656e+00, -9.3830e-01, -1.3109e+00, -7.3162e-01,\n",
       "        -1.5076e+00, -2.4312e-01,  4.6379e-03, -4.6894e-01, -1.0337e+00,\n",
       "        -6.1888e-02, -3.3572e-01, -3.0150e-01, -4.6039e-01, -1.5975e+00,\n",
       "        -7.4560e-01,  3.8803e-01, -1.1875e+00, -7.2280e-01,  1.3671e-01,\n",
       "         6.4118e-01, -2.0195e-01, -8.4569e-01, -6.0579e-01, -2.5960e-01,\n",
       "        -1.0477e+00, -1.9313e+00, -1.9360e-02, -8.6606e-02, -6.2941e-01,\n",
       "        -7.5817e-01, -5.7888e-01, -7.8532e-01, -2.4488e-01, -3.2668e-01,\n",
       "        -6.2050e-02,  4.1200e-01, -1.0372e+00, -3.7898e-01,  3.2074e-01,\n",
       "         2.1915e-01, -2.2707e-01,  2.2745e-01, -3.5002e-01,  2.9384e-02,\n",
       "        -4.9763e-01, -7.3610e-01,  3.9763e-01,  2.3249e-01,  5.8555e-01,\n",
       "         1.3995e+00, -3.4781e-01, -7.7838e-01, -7.8467e-01, -7.4402e-01,\n",
       "        -3.2940e-01, -3.9144e-01,  3.2016e-01, -1.6993e-01,  1.8216e-01,\n",
       "        -4.5186e-01, -7.0215e-01, -5.4624e-01, -1.3429e-01, -7.4436e-01,\n",
       "        -1.0177e+00, -5.1993e-01, -1.1596e+00,  3.5700e-01,  7.1961e-01,\n",
       "         3.5158e+00,  3.1373e+00,  3.2926e+00,  2.8622e+00,  2.0885e+00,\n",
       "         3.4101e+00,  1.4803e+00,  1.8022e+00,  1.4543e+00,  2.9384e+00,\n",
       "         3.4243e+00,  4.0282e+00,  3.0364e+00,  2.4017e+00,  1.8663e+00,\n",
       "         3.3399e+00,  2.8954e+00,  1.8773e+00,  3.1626e+00,  2.7417e+00,\n",
       "         3.7851e+00,  2.4693e+00,  4.6723e+00,  2.9071e+00,  3.0573e+00,\n",
       "         3.5883e+00,  3.7828e+00,  6.7367e-01,  7.0693e-01,  6.1704e-02,\n",
       "         6.1139e-01, -4.2987e-03, -2.8763e-01,  8.3422e-01,  8.0594e-01,\n",
       "         8.1811e-01,  1.3735e+00, -3.7762e-01, -1.0273e+00, -1.6728e-01,\n",
       "         3.3887e-01, -1.0877e+00, -7.0069e-01, -7.8717e-01, -9.8512e-01,\n",
       "        -8.7440e-01, -1.2665e+00, -6.5122e-01,  1.9033e-01,  6.9602e-01,\n",
       "         1.0764e+00, -8.5651e-01, -8.4488e-01, -2.3287e-01, -7.7346e-01,\n",
       "        -4.4957e-01,  1.5453e+00,  8.9512e-01,  1.1968e+00,  1.1509e-01,\n",
       "         3.3873e-01,  3.1633e-01,  4.3355e-01, -3.5741e-01, -5.0220e-01,\n",
       "        -8.9215e-01, -1.3151e+00, -3.0366e-01, -1.3485e+00, -1.4128e+00,\n",
       "        -2.3350e-01,  4.2517e-01, -1.0047e+00, -7.9467e-01, -2.1988e-01,\n",
       "        -1.1085e+00, -1.5861e+00, -8.7015e-02, -6.6336e-01, -1.7336e-01,\n",
       "         1.3440e-01, -7.0766e-01, -4.8446e-01,  1.1174e+00,  1.2331e+00,\n",
       "        -3.3156e-01, -2.6324e-01, -3.0282e-01, -2.0501e+00,  3.2185e-02,\n",
       "         9.1381e-01,  2.4483e-01,  2.0996e-01,  2.4459e-02, -3.8633e-01,\n",
       "         9.3509e-01,  9.1750e-02, -1.9178e-01, -9.0969e-01, -6.0290e-01,\n",
       "        -2.0366e-01, -2.1135e-01, -6.3131e-01, -6.4213e-01,  2.5490e-01,\n",
       "        -1.3417e+00, -9.5330e-01,  4.3120e-01,  8.2717e-01, -7.3563e-01,\n",
       "         2.7190e-01,  5.7451e-01,  1.9557e-01,  4.0642e-01,  2.4713e-02,\n",
       "        -5.8788e-01,  1.4998e-01, -8.7239e-02, -5.4671e-01,  2.5241e-01,\n",
       "         3.1235e-03, -8.5227e-01, -1.8259e-01, -6.8012e-01,  6.2426e-02,\n",
       "         3.7516e-01,  3.3387e-01,  1.2676e+00,  2.9962e-01,  1.3741e+00,\n",
       "        -3.8969e-01, -3.9703e-01,  1.1737e-01, -9.2184e-01, -9.6948e-02,\n",
       "        -4.9524e-01,  5.3546e-01, -3.5336e-01, -4.3691e-01, -6.5207e-01,\n",
       "         5.3269e-02, -1.3944e-01, -1.0861e+00, -3.7627e-01, -3.9291e-01,\n",
       "         1.4247e-01, -6.7763e-01,  2.7155e-01,  3.9185e-02, -6.5577e-02,\n",
       "        -8.1487e-01,  5.1319e-01, -4.2864e-01, -3.1421e-01, -1.2266e+00,\n",
       "         1.2333e+00, -4.4900e-02, -5.3717e-01, -5.2242e-01, -7.3242e-01,\n",
       "        -3.4712e-01, -3.9096e-01,  5.9648e-02,  4.3788e-01, -1.0977e+00,\n",
       "         4.4682e-02, -4.5545e-01, -3.0675e-01, -3.0604e-01,  1.2009e-01,\n",
       "        -4.0047e-01, -1.0223e+00,  3.3356e-01,  1.0372e-02, -5.2635e-01,\n",
       "        -5.5119e-01, -7.2843e-02,  4.2490e-01, -1.6256e-01,  1.1271e+00,\n",
       "         6.9395e-01, -6.2672e-03, -2.0958e-01,  1.9731e-01, -1.4454e+00,\n",
       "        -1.0495e+00, -1.8070e-01, -5.9833e-01, -1.9903e-01,  1.1636e+00,\n",
       "        -3.7101e-01,  2.4326e-01,  1.7736e-01, -1.8066e-01, -1.0157e+00,\n",
       "        -1.0136e+00, -4.7563e-01, -5.7059e-01, -9.3047e-01, -5.2690e-02,\n",
       "         5.2338e-01, -1.0183e+00,  1.6415e-01, -1.9810e-01,  1.7473e-02,\n",
       "         4.0204e-01, -1.4003e-01,  3.6955e-01, -5.1234e-01, -2.7032e-01,\n",
       "        -3.6590e-01,  1.4747e-03, -2.1043e-01, -7.6419e-01,  2.5213e-01,\n",
       "        -5.5918e-02,  6.6711e-01,  3.6876e-02, -1.0096e-01,  4.8464e-01,\n",
       "         3.8541e-01, -1.3456e+00,  5.7231e-01, -4.2470e-02,  4.3133e-01,\n",
       "        -8.4049e-02, -2.6305e-01, -4.6414e-01, -1.1372e-01,  2.2947e-01,\n",
       "        -5.2887e-01, -3.4392e-01, -2.2255e-01,  3.6742e-01, -1.4386e+00,\n",
       "         7.2409e-01, -2.3698e-01, -1.6448e-01, -1.5687e+00, -1.3258e+00,\n",
       "        -3.8216e-01, -1.0692e+00, -7.2697e-01, -3.7392e-01, -1.4315e-01,\n",
       "        -9.3109e-01, -6.5581e-01, -6.3122e-01, -6.2383e-01, -2.4160e+00,\n",
       "        -7.5089e-01,  8.1680e-02,  5.2389e-02,  6.0548e-01, -5.5790e-01,\n",
       "         1.6947e-01, -4.6718e-02, -1.1821e+00, -6.0695e-01, -7.6135e-01,\n",
       "        -1.0650e+00, -5.6378e-01, -2.7429e-01, -5.5994e-01, -1.7744e-01,\n",
       "        -2.0655e-01, -4.8465e-01, -1.4198e+00, -8.8112e-01, -9.6030e-02,\n",
       "        -4.2952e-01, -4.7959e-01, -1.0171e-01,  1.2004e-01, -1.2173e-01,\n",
       "        -1.3749e+00, -1.0508e+00, -6.6805e-01, -3.7798e-01, -9.8350e-01,\n",
       "         6.3704e-01,  3.6199e-01, -2.0396e-02,  3.0103e-01, -4.9200e-01,\n",
       "         2.0001e-01,  3.1668e-01,  5.9360e-01,  9.7401e-01, -5.2448e-01,\n",
       "        -1.5190e-01,  1.2539e-01, -2.6305e-01, -7.9693e-01,  2.9097e-01,\n",
       "        -3.0721e-01,  6.4088e-01,  1.0766e-01, -1.7094e+00,  1.2725e+00,\n",
       "        -8.6696e-02, -5.3121e-01,  4.1654e-01, -3.0984e-01,  1.6124e-01,\n",
       "        -4.5992e-01, -1.4941e-01,  1.2842e-01, -3.0104e-02,  8.2607e-01,\n",
       "         3.3000e-01, -1.1985e+00, -4.4296e-01, -2.7740e-01, -4.0548e-01,\n",
       "        -3.4770e-01,  6.9184e-01, -8.0681e-02,  8.6727e-01, -9.9535e-01,\n",
       "        -2.0594e+00, -7.4443e-02,  7.7793e-01, -1.5041e+00, -1.4131e+00,\n",
       "         4.7774e-01,  7.1617e-01, -1.5353e+00, -1.2535e+00, -1.3690e-01,\n",
       "         2.9112e-01,  1.1169e+00, -1.7713e+00, -1.1103e+00,  3.8009e-01,\n",
       "        -6.9413e-01, -1.9473e-01,  3.4498e-02, -3.5046e-01, -1.4358e-01,\n",
       "        -6.1537e-03, -1.9874e-02, -2.5464e-01, -1.0155e+00,  2.1997e-01,\n",
       "         2.8780e-01, -4.7487e-01, -2.8096e-01, -7.1042e-01,  6.8230e-01,\n",
       "        -3.3568e-01, -1.2166e+00, -9.0697e-01,  1.2215e-01, -1.7390e+00,\n",
       "        -3.2139e-01, -4.7601e-01, -4.7051e-01, -2.5235e-01, -7.3702e-01,\n",
       "        -1.7650e-01,  2.4883e-01,  5.3045e-01,  2.6200e-01, -1.7633e+00,\n",
       "        -1.0401e+00, -5.4094e-01, -9.2651e-01, -8.4484e-01, -5.3297e-01,\n",
       "        -5.4115e-01,  1.0649e-01, -2.9897e-02, -7.3113e-01,  4.6768e-01,\n",
       "        -6.1681e-01, -1.2394e+00,  1.3555e+00, -1.4528e+00, -3.9816e-01,\n",
       "        -3.7695e-01, -1.3179e+00, -3.4122e-01, -2.7023e-01, -1.3537e-01,\n",
       "         2.9659e-01,  2.9337e-01, -7.9024e-01, -7.6011e-01,  2.1546e-01,\n",
       "         3.6362e-01,  8.0484e-02,  9.7451e-01,  1.7527e+00,  4.0523e-01,\n",
       "        -6.5913e-02,  4.7158e-01, -1.2273e-01, -5.4626e-01, -3.2069e-01,\n",
       "         5.9521e-02, -2.8239e-01,  9.3321e-02,  5.4704e-01,  1.2708e+00,\n",
       "         3.7720e-02, -4.4450e-01, -2.6901e-01, -4.4626e-01, -3.4592e-02,\n",
       "         1.0194e-01, -2.3339e-01,  4.4487e-01,  3.3936e-01,  2.4321e-01,\n",
       "         2.0211e-01,  3.3796e-01, -7.8064e-01, -1.6992e+00, -9.5757e-01,\n",
       "         2.6221e-01, -4.1715e-01,  4.7921e-01, -4.4031e-02, -1.1713e+00,\n",
       "         1.4446e-01,  7.2909e-01, -1.6373e+00,  3.3419e-01,  5.6544e-02,\n",
       "         1.8892e-01,  1.0617e-01, -1.0292e+00,  6.8586e-01,  1.3947e-02,\n",
       "         7.5578e-01, -4.1465e-02, -5.7326e-02,  2.3749e+00, -8.7912e-01,\n",
       "         9.9355e-01, -9.0319e-01,  5.6646e-01,  4.6569e-01, -1.1790e-01,\n",
       "        -1.1215e+00,  8.5247e-04, -1.1088e-01, -3.0665e-01,  1.5602e+00,\n",
       "        -8.9040e-01,  3.2294e-01, -5.7312e-01, -2.7327e-01, -4.5776e-01,\n",
       "        -3.5135e-01,  1.2604e+00, -1.1591e+00,  6.0895e-01, -2.3839e-02,\n",
       "        -1.2414e+00, -5.6151e-01, -9.8610e-02, -5.2664e-02, -1.1397e-01,\n",
       "        -2.2334e-01, -1.2210e+00,  4.4270e-01, -2.9248e-01,  4.0192e-01,\n",
       "         5.1236e-01, -7.5999e-01,  6.9734e-01, -6.5716e-01, -4.0890e-02,\n",
       "        -1.0566e-01, -1.2025e-01,  3.3041e-01, -3.6257e-01,  8.9573e-01,\n",
       "        -7.5122e-01, -7.2522e-01, -9.3669e-01,  5.3541e-01,  9.8458e-01,\n",
       "        -6.6255e-01,  2.1266e-01, -6.7777e-01, -5.4911e-01,  6.6891e-01,\n",
       "         4.8033e-01, -2.7051e-01,  3.4499e-01, -2.7397e-01, -9.7528e-01,\n",
       "        -3.3758e-01, -1.6120e+00, -1.5429e-01, -3.0252e-01,  3.7784e-01,\n",
       "        -8.0197e-01, -6.7167e-01, -3.0710e-01, -5.1659e-02, -2.8810e-01,\n",
       "         1.1968e-02,  4.8108e-01,  1.9145e-02,  6.8697e-02, -1.1712e+00,\n",
       "         8.4081e-01, -1.1797e+00,  8.3842e-02, -7.7231e-01,  3.1158e-01,\n",
       "         1.8174e-01,  1.7131e-01,  3.0156e-01,  1.0160e-01, -6.3591e-01,\n",
       "         5.3666e-01, -1.7055e+00, -8.5954e-01, -7.6677e-01, -6.1682e-02,\n",
       "         1.1917e+00,  9.5173e-02, -9.0544e-01, -5.4406e-01,  7.9547e-02,\n",
       "        -1.3502e-01, -9.4616e-01, -9.9362e-01, -3.8204e-01, -2.5304e-01,\n",
       "        -1.3443e+00, -2.4780e-02, -4.9610e-01,  7.8225e-01, -9.9303e-01,\n",
       "        -3.7499e-01, -1.0988e+00,  1.0256e+00,  7.9206e-01, -1.1220e+00,\n",
       "         1.8751e-01, -2.7323e-01,  7.1185e-01, -1.1991e-01, -4.0646e-01,\n",
       "        -6.8417e-01, -7.3193e-01,  6.7616e-01, -4.9689e-01, -1.4372e+00,\n",
       "        -1.4862e-02,  7.9633e-01, -6.0245e-01, -1.5254e-01, -1.1453e+00,\n",
       "         5.5803e-01, -3.1614e-01, -3.2544e-02, -8.8137e-01, -3.6552e-02,\n",
       "        -9.9482e-01,  1.0695e-01,  2.5453e-01,  6.5065e-01, -3.9501e-01,\n",
       "         9.9171e-02, -5.0370e-01,  3.6602e-01, -4.7946e-01,  1.0609e-01,\n",
       "        -1.0575e+00, -1.6334e-01,  5.9121e-01,  1.4640e-01, -6.5209e-01,\n",
       "         2.0023e-01, -4.3255e-01, -2.2385e-01,  1.7217e-01, -1.4867e-01,\n",
       "        -7.2663e-01,  2.2605e-01, -1.0006e+00, -2.8136e-01,  1.4472e-01,\n",
       "         5.8588e-01, -4.4207e-01, -7.0224e-01, -1.0914e+00, -7.1152e-01,\n",
       "        -2.0456e-02, -6.5664e-01, -2.7428e-03, -1.5722e-01,  5.0217e-01,\n",
       "         7.1508e-01,  3.5123e-01,  7.9195e-02, -1.0162e+00,  7.6332e-01,\n",
       "        -9.2266e-01, -1.4160e-01, -1.4416e-01, -1.9193e-01, -1.2449e+00,\n",
       "        -3.4593e-01,  2.8422e-01,  1.3690e+00, -9.5024e-01, -1.0418e-01,\n",
       "        -2.1390e-01, -1.0808e+00, -1.2149e+00, -7.3044e-01,  4.9418e-01,\n",
       "        -5.3101e-03,  7.1854e-01, -1.0582e+00,  1.5716e-01,  2.4118e-01,\n",
       "        -1.1823e-01, -8.6130e-01, -9.6115e-02, -3.2542e-02, -5.5819e-01,\n",
       "         1.2366e-01,  3.0340e-01,  5.5899e-01,  5.6439e-01, -4.1328e-02,\n",
       "        -5.5495e-02,  1.4490e+00, -1.6481e-01,  2.5917e-01,  2.8859e-01,\n",
       "         1.5971e+00,  1.3329e+00,  8.3958e-01,  3.6907e-01,  4.7855e-01,\n",
       "        -4.1180e-01,  1.9639e+00,  1.6070e+00,  1.3636e-02,  5.7129e-01,\n",
       "         2.6093e-01, -6.0772e-02,  1.1129e+00, -1.3175e+00, -3.2930e-01,\n",
       "         1.5176e-01,  6.3380e-01,  2.3787e-01,  6.0232e-01, -4.5577e-01,\n",
       "         9.7763e-01, -8.3347e-01,  3.0875e-01, -8.9208e-01, -8.8099e-01,\n",
       "         7.7705e-01, -2.5459e-01, -4.6234e-02, -3.0294e-01, -4.5818e-01,\n",
       "         3.3745e-01,  3.9379e-01,  6.6701e-01, -7.0002e-01,  3.0053e-01,\n",
       "         9.6884e-01,  4.9355e-02,  2.7887e-01, -1.6099e-01,  1.0769e+00,\n",
       "         6.6698e-01, -3.5536e-01, -6.1739e-01, -2.9499e-01, -5.9342e-01,\n",
       "         2.3594e+00,  1.9801e+00,  1.9181e+00,  2.1723e+00,  2.0243e+00,\n",
       "         1.3820e+00,  1.7312e+00,  6.1605e-01,  1.0870e+00,  2.3154e+00,\n",
       "         2.5561e+00,  2.2475e+00,  1.0393e+00,  3.3476e+00, -3.8315e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b55d3d-b5d7-4f73-a010-c19b5c7a15e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[322, 311, 70, 320]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(outputs, k=4).indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c42fe7-ae24-4716-97f6-46fac72daaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04724839702248573"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(outputs, -1)[322].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ab57c-68b7-44e6-ba63-65803a98033d",
   "metadata": {},
   "source": [
    "El siguiente ejemplo se tom√≥ de [Kaggle](https://www.kaggle.com/code/raufmomin/vision-transformer-vit-from-scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c86abe-8a78-4cbb-aa91-46b584544d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.13.0\n"
     ]
    }
   ],
   "source": [
    "#Importamos las librerias a utlizar\n",
    "\n",
    "#!pip3 install tensorflow_addons\n",
    "#!pip3 install --upgrade typing-extensions==4.5\n",
    "#!pip3 install --upgrade tensorflow==2.13\n",
    "#!pip3 install 'keras<3.0.0' mediapipe-model-maker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593993c-3999-42b8-9694-7a0a936ed193",
   "metadata": {},
   "source": [
    "FUNCION DE PARECIDO, PUEDE SER LA FUNCION EUCLIDIANA, COSENO (TE QUEDA AL REV√àS) CUANDO COINCIDE VALE 1 Y CUANDO NO, VALE 0. TENGO LOS EMBEDDINGS. EN UNA CIERTA CAPA DE LA RED TENGO CIERTOS COMPONENTES PRINCIPALES. RED DE REGULARIZACIONES PARA QUE NO SE TE SALTEN EN LAS CAPAS.\n",
    "\n",
    "PROBLEMA DE CLASIFICACION, MUESTRA DE TRAIN, UNA DE TEST. TENGO UNA IMAGEN DE TEST, LA TRUNCO, SACO LOS RESULTADOS Y OBTENDO SIMILITUDES. SE PUEDE METER EN EL ARQUETIPO A PRIORI. COMO HACER UNA METRICA PARA QUE SE PAREZCA \n",
    "\n",
    "TEXTO DESTRUCTURADO, BASE DE DATOS DE GRAFOS (GRAFOS), LO METES RFLIP.\n",
    "\n",
    "CONTRUIR LOS ARQUETIPOS, NIVEL AL CUAL ESTOY CRTANDO LA RED Y LA METRICA PARA LA COMPARACION. ETIQUETAS: LO QUE ESTOY VIENDO ES QUE ESA MUESTRA A QUE CLUSTER CORRESPONDE. ENTRENAR EL MODELO CON IMAGENES \"EQUIS\" DE PERROS, ANIMALES, ETC, ETC. PARA TENERLO SUPERVISADO EN EL ENTRENAMIENTO, IR POR FASES Y UNA VEZ QUE HAYA AGARRADO VELOCIDAD ENTONCES SI METERLE LO DE LAS PLANTAS. SEMISUPERVISADO -> IR ALARGANDOLO PARA QUE VAYA APRENDIENDO DE DIFERENTES RAMAS. \n",
    "\n",
    "DEEP LEARNING MUY PODEROSO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fcec1-0cfd-4a2e-b4fb-526365c03e72",
   "metadata": {},
   "source": [
    "## Prueba extra: Soluci√≥n de Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbebb2-87cc-4824-a291-57c4e1363ad4",
   "metadata": {},
   "source": [
    "Aqu√≠ se va a probar una soluci√≥n prove√≠da por el equipo de Keras dentro del mismo reto de Kaggle. Esto con el fin de poder entender como se puede ir aplicando el modelo y el prop√≥sito es que de aqu√≠ se van obteniendo vertientes diferentes.\n",
    "\n",
    "Primero, se probar√° si es que se puede replicar con mis propios recursos la misma soluci√≥n para ver si es factible desde mis recursos.\n",
    "\n",
    "Una vez que tenga la confirmaci√≥n de que puedo replicar la soluci√≥n con mis recursos, entonces se investigar√° que parte(s) es la que puedo editar para tener algo diferente y de ah√≠ ir obteniendo mi proyecto.\n",
    "\n",
    "Ya que tengo la teor√≠a investigada de mi lado entonces si entrar√≠amos en la fase de experimentaci√≥n\n",
    "\n",
    "[Referencia en p√°gina de Kaggle](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook#üîç-%7C-Loss-&-Metric)\n",
    "\n",
    "**OTRAS REFERENCIAS:**\n",
    "\n",
    "[Keras CV Models](https://keras.io/api/keras_cv/models/)\n",
    "\n",
    "[Train an image classifier from scratch - Keras CV](https://keras.io/guides/keras_cv/classification_with_keras_cv/)\n",
    "\n",
    "[Learning Rate Schedulers](https://github.com/Mr-TalhaIlyas/Learning-Rate-Schedulers-Packege-Tensorflow-PyTorch-Keras?tab=readme-ov-file)\n",
    "\n",
    "[SGDR](https://arxiv.org/pdf/1608.03983)\n",
    "\n",
    "[batch function doc](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)\n",
    "\n",
    "[StratifiedKFold - Medium](https://medium.com/@literallywords/stratified-k-fold-with-keras-e57c487b1416)\n",
    "\n",
    "[StratifiedKFold - scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n",
    "\n",
    "[StratifiedKFold - Visualization](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2df8d-8f28-428c-98ab-8d8554dabfae",
   "metadata": {},
   "source": [
    "### Replicaci√≥n de la soluci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e553ab-2a0f-455c-8477-647bb0cd8863",
   "metadata": {},
   "source": [
    "En la soluci√≥n publicada de Kaggle, el modelo que utlizan es **EfficientNev2** (√°re ade oportunidad para cambiar de modelo?). \n",
    "\n",
    "Partes (pasos) esenciales para la ejecuci√≥n de este proyecto:\n",
    "\n",
    "- Designing a data pipeline for a multi-input and multi-output model.\n",
    "- Creating random augmentation pipeline with KerasCV.\n",
    "- Loading the data efficiently using tf.data.\n",
    "- Creating the model using KerasCV presets.\n",
    "- Training the model.\n",
    "- Inference and Submission on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d07aa3-6c0c-4175-8fde-1cc6bad43e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 20:15:53.440906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Instalaci√≥n de las librer√≠as necesarias a utilizar\n",
    "\n",
    "#!pip3 install keras_cv==0.8.2 --no-deps\n",
    "#!pip3 install tensorflow==2.15.0 --no-deps\n",
    "#!pip3 install keras==3.0.4 --no-deps\n",
    "\n",
    "#Importamos las librerias que vayamos a necesitar\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" #Aqui el autor dice que tambi√©n podemos utilizar tensorflow o torch (√ÅREA DE OPORTUNIDAD)\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Funciones creadas para facilitar algunos procesos\n",
    "import functions.general_functions as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2448e526-81ff-4561-aab6-484e8426e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.15.0\n",
      "Keras 3.0.4\n",
      "KerasCV 0.8.2\n"
     ]
    }
   ],
   "source": [
    "#Imprimimos las versiones de las librerias a utilizar \n",
    "\n",
    "print(\"TensorFlow\", tf.__version__)\n",
    "print(\"Keras\",keras.__version__)\n",
    "print(\"KerasCV\",keras_cv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15925d8-6791-4d6f-9608-7197eba2208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciamos con la fase de configuraci√≥n: Dise√±o de un data pipeline para un modelo con entradas y salidas m√∫ltiples\n",
    "\n",
    "#Configuraci√≥n de la clase CFG:\n",
    "\n",
    "class CFG:\n",
    "    verbose = 1 #Par√°metro que configura la manera en que el proceso de entrenamiento es presentado para cada epoch. 0 = silencioso (no muestra nada), 1 = se muestra cada epoch separado por barra, 2 = se muestra solo el n√∫mero de epoch\n",
    "    seed = 42 #Semilla que se utiliza para poder reproducir resultados similares en cada ejecuci√≥n\n",
    "    preset = \"efficientnetv2_b2_imagenet\" #Nombre del clasificador pre entrenado, √ÅREA DE MEJORA. Tambi√©n se puede utilizar modelos como CSPDarkNet, densenet, EfficientNetV2 (otras vertientes), MiT, MobileNetV3, ResNetV1, YOLO, VitDet\n",
    "    image_size = [224, 224] #Tama√±o de la imagen de entrada del modelo , √ÅREA DE MEJORA, investigar que otros tama√±os podemos utilizar y como es que cambia los resultados con otros tama√±os -> posible efecto que tiene el tama√±o de la imagen en los resultados del modelo\n",
    "    epochs: 12 #Epochs para el entrenamiento -> N√∫mero total de iteraciones que tuvo todo tu conjunto de entrenamiento para entrenar el modelo de ML, un \"paso\" de la data de entrenamiento se toma en cuenta cuando √©sta haya pasado tanto por delante como por detr√°s en todos los procesos del modelo. Incrementando el n√∫mero de epochs mejora el performance del modelo al permitirle aprender comportamientos m√°s complejos en la data, pero si existen muchos epochs, el modelo se sobre-entrena\n",
    "    batch_size: 96 #Tama√±o del batch -> Un batch es un subconjunto/grupo del conjunto completo de entrenamiento, tambi√©n se pueden definir como muestras del conjunto de entrenamiento, esto para mejorar el proceso de un conjunto enorme de datos en grupos \"peque√±os\" de datos\n",
    "    lr_mode = \"step\" #Tipo de Learning Rate Scheduler y puede ser entre \"cos\", \"step\" o \"exp\". \"step\" = Step Decay (empieza a deacer el LR despu√©s de ciertos epochs mediante un factor lr_decay), \"exp\" = Exponential Decay (empieza a decaer el LR de manera exponencial), \"cos\" = Cosine Decay (el LR empieza a decaer mediante un factor de coseno). Tambi√©n hay otros LRS, √ÅREA DE MEJORA\n",
    "    drop_remainder = True #Par√°metro l√≥gico para desechar los batches incompletos. Sirve para tener un control sobre los batches en cuanto al tama√±o de √©stos, si el par√°metro es True entonces desechar√° el √∫ltimo batch si es que √©ste es de un tama√±o menor (batch_size) a los dem√°s, por defualt es FALSE\n",
    "    num_classes = 6 #N√∫mero de clases que vienen en el dataset (las clases que vamos a predecir)\n",
    "    num_folds = 5 #N√∫mero de dobleces (batches) para dividir el conjunto de datos (Prueba y entrenamiento). Se utiliza dentro de la funci√≥n StratifiedKFold y se refiere a que de los K batches que se crearon de la data, entrenamos el modelo con K-1 batches y probamos con el √∫ltimo batch, esto se repite para todas las permutaciones de batches (al final siempre se hace el entrenamiento dejando un batch diferente fuera cada vez), lo que lo hace estratificado es que tambi√©n nos fijamos en la distribuci√≥n relativa de las clases, si una clase aparece m√°s que otra entonces se toma en cuenta para la formaci√≥n de batches. √ÅREA DE MEJORA, podemos utilizar diferentes m√©todos de fold\n",
    "    fold = 0 #N√∫mero de fold que utilizaremos para la validaci√≥n de la data. solo sirve para controlar los √≠ndices del conjunto de datos y dividirlo en conjunto de prueba y entrenamiento. Aqu√≠ definimos que el conjunto para validaci√≥n ser√° aquel del fold 0, lo dem√°s ser√° para entrenmaiento\n",
    "    class_names = ['X4_mean', 'X11_mean', 'X18_mean',\n",
    "                   'X26_mean', 'X50_mean', 'X3112_mean'] #Nombre de las clases del conjunto de datos, son las variables que vamos a predecir\n",
    "    aux_class_names = list(map(lambda x: x.replace(\"mean\",\"sd\"), class_names)) #Nombre de las clases auxiliares para el entrenamiento de los datos, estas variables vienen tambi√©n en el conjunto de datos\n",
    "    num_classes = len(class_names) #N√∫mero total de clases que se tiene en el conjunto de entrenamiento\n",
    "    aux_num_classes = len(aux_class_names) #N√∫mero total de clases auxiliares que se tiene en el conjunto de entrenamiento\n",
    "\n",
    "#Ruta de donde se leer√°n todos los archivos CSV\n",
    "\n",
    "ruta_data_csv = gf.get_data_path(\"csv\")\n",
    "\n",
    "#Ruta de donde se leer√°n todas las im√°genes de entrenamiento\n",
    "\n",
    "ruta_img_train = gf.get_data_path(\"train_images\")\n",
    "\n",
    "#Ruta de donde se leer√°n todas las im√°genes de prueba\n",
    "\n",
    "ruta_img_test = gf.get_data_path(\"test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae81c306-4f09-4952-808d-bbbb1fcc5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuramos el par√°metro de seed\n",
    "\n",
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad6f707-8197-4d01-931c-2fad125f5099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>WORLDCLIM_BIO1_annual_mean_temperature</th>\n",
       "      <th>WORLDCLIM_BIO12_annual_precipitation</th>\n",
       "      <th>WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month</th>\n",
       "      <th>WORLDCLIM_BIO15_precipitation_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO4_temperature_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO7_temperature_annual_range</th>\n",
       "      <th>SOIL_bdod_0.5cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_100.200cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_15.30cm_mean_0.01_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>X26_mean</th>\n",
       "      <th>X50_mean</th>\n",
       "      <th>X3112_mean</th>\n",
       "      <th>X4_sd</th>\n",
       "      <th>X11_sd</th>\n",
       "      <th>X18_sd</th>\n",
       "      <th>X26_sd</th>\n",
       "      <th>X50_sd</th>\n",
       "      <th>X3112_sd</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192027691</td>\n",
       "      <td>12.235703</td>\n",
       "      <td>374.466675</td>\n",
       "      <td>62.524445</td>\n",
       "      <td>72.256844</td>\n",
       "      <td>773.592041</td>\n",
       "      <td>33.277779</td>\n",
       "      <td>125</td>\n",
       "      <td>149</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.243779</td>\n",
       "      <td>1.849375</td>\n",
       "      <td>50.216034</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>1.601473</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.153608</td>\n",
       "      <td>0.279610</td>\n",
       "      <td>15.045054</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195542235</td>\n",
       "      <td>17.270555</td>\n",
       "      <td>90.239998</td>\n",
       "      <td>10.351111</td>\n",
       "      <td>38.220940</td>\n",
       "      <td>859.193298</td>\n",
       "      <td>40.009777</td>\n",
       "      <td>124</td>\n",
       "      <td>144</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642940</td>\n",
       "      <td>1.353468</td>\n",
       "      <td>574.098472</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.258078</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>11.004477</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
       "0  192027691                               12.235703   \n",
       "1  195542235                               17.270555   \n",
       "\n",
       "   WORLDCLIM_BIO12_annual_precipitation  \\\n",
       "0                            374.466675   \n",
       "1                             90.239998   \n",
       "\n",
       "   WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
       "0                                          62.524445                       \n",
       "1                                          10.351111                       \n",
       "\n",
       "   WORLDCLIM_BIO15_precipitation_seasonality  \\\n",
       "0                                  72.256844   \n",
       "1                                  38.220940   \n",
       "\n",
       "   WORLDCLIM_BIO4_temperature_seasonality  \\\n",
       "0                              773.592041   \n",
       "1                              859.193298   \n",
       "\n",
       "   WORLDCLIM_BIO7_temperature_annual_range  SOIL_bdod_0.5cm_mean_0.01_deg  \\\n",
       "0                                33.277779                            125   \n",
       "1                                40.009777                            124   \n",
       "\n",
       "   SOIL_bdod_100.200cm_mean_0.01_deg  SOIL_bdod_15.30cm_mean_0.01_deg  ...  \\\n",
       "0                                149                              136  ...   \n",
       "1                                144                              138  ...   \n",
       "\n",
       "   X26_mean  X50_mean  X3112_mean     X4_sd    X11_sd    X18_sd    X26_sd  \\\n",
       "0  1.243779  1.849375   50.216034  0.008921  1.601473  0.025441  0.153608   \n",
       "1  0.642940  1.353468  574.098472  0.003102  0.258078  0.000866  0.034630   \n",
       "\n",
       "     X50_sd   X3112_sd                                         image_path  \n",
       "0  0.279610  15.045054  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "1  0.010165  11.004477  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "\n",
       "[2 rows x 177 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>WORLDCLIM_BIO1_annual_mean_temperature</th>\n",
       "      <th>WORLDCLIM_BIO12_annual_precipitation</th>\n",
       "      <th>WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month</th>\n",
       "      <th>WORLDCLIM_BIO15_precipitation_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO4_temperature_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO7_temperature_annual_range</th>\n",
       "      <th>SOIL_bdod_0.5cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_100.200cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_15.30cm_mean_0.01_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m04</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m05</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m06</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m07</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m08</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m09</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m10</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m11</th>\n",
       "      <th>VOD_X_1997_2018_multiyear_mean_m12</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195066138</td>\n",
       "      <td>10.55810</td>\n",
       "      <td>961.500000</td>\n",
       "      <td>31.586735</td>\n",
       "      <td>13.728325</td>\n",
       "      <td>648.038208</td>\n",
       "      <td>25.351532</td>\n",
       "      <td>127</td>\n",
       "      <td>152</td>\n",
       "      <td>137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469694</td>\n",
       "      <td>0.455849</td>\n",
       "      <td>0.528211</td>\n",
       "      <td>0.555653</td>\n",
       "      <td>0.549882</td>\n",
       "      <td>0.542905</td>\n",
       "      <td>0.517507</td>\n",
       "      <td>0.462724</td>\n",
       "      <td>0.427107</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195524180</td>\n",
       "      <td>7.00287</td>\n",
       "      <td>1120.025513</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>7.258863</td>\n",
       "      <td>973.889404</td>\n",
       "      <td>39.135712</td>\n",
       "      <td>106</td>\n",
       "      <td>167</td>\n",
       "      <td>127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428838</td>\n",
       "      <td>0.456266</td>\n",
       "      <td>0.470074</td>\n",
       "      <td>0.468038</td>\n",
       "      <td>0.475943</td>\n",
       "      <td>0.483206</td>\n",
       "      <td>0.477197</td>\n",
       "      <td>0.432732</td>\n",
       "      <td>0.423728</td>\n",
       "      <td>/Users/pedrovela/Docs/Datasets - ML/planttrait...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
       "0  195066138                                10.55810   \n",
       "1  195524180                                 7.00287   \n",
       "\n",
       "   WORLDCLIM_BIO12_annual_precipitation  \\\n",
       "0                            961.500000   \n",
       "1                           1120.025513   \n",
       "\n",
       "   WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
       "0                                          31.586735                       \n",
       "1                                          23.000000                       \n",
       "\n",
       "   WORLDCLIM_BIO15_precipitation_seasonality  \\\n",
       "0                                  13.728325   \n",
       "1                                   7.258863   \n",
       "\n",
       "   WORLDCLIM_BIO4_temperature_seasonality  \\\n",
       "0                              648.038208   \n",
       "1                              973.889404   \n",
       "\n",
       "   WORLDCLIM_BIO7_temperature_annual_range  SOIL_bdod_0.5cm_mean_0.01_deg  \\\n",
       "0                                25.351532                            127   \n",
       "1                                39.135712                            106   \n",
       "\n",
       "   SOIL_bdod_100.200cm_mean_0.01_deg  SOIL_bdod_15.30cm_mean_0.01_deg  ...  \\\n",
       "0                                152                              137  ...   \n",
       "1                                167                              127  ...   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m04  VOD_X_1997_2018_multiyear_mean_m05  \\\n",
       "0                            0.469694                            0.455849   \n",
       "1                            0.428838                            0.456266   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m06  VOD_X_1997_2018_multiyear_mean_m07  \\\n",
       "0                            0.528211                            0.555653   \n",
       "1                            0.470074                            0.468038   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m08  VOD_X_1997_2018_multiyear_mean_m09  \\\n",
       "0                            0.549882                            0.542905   \n",
       "1                            0.475943                            0.483206   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m10  VOD_X_1997_2018_multiyear_mean_m11  \\\n",
       "0                            0.517507                            0.462724   \n",
       "1                            0.477197                            0.432732   \n",
       "\n",
       "   VOD_X_1997_2018_multiyear_mean_m12  \\\n",
       "0                            0.427107   \n",
       "1                            0.423728   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "1  /Users/pedrovela/Docs/Datasets - ML/planttrait...  \n",
       "\n",
       "[2 rows x 165 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Leemos el archivo de entrenamiento\n",
    "df = pd.read_csv(f'{ruta_data_csv}/train.csv')\n",
    "#Creamos una nueva columna con la direcci√≥n de cada imagen correspondiente\n",
    "df['image_path'] = f'{ruta_img_train}/'+df['id'].astype(str)+'.jpeg'\n",
    "#Sustituimos los NAs en las variables objetivo auxiliares con \"-1\"\n",
    "df.loc[:,CFG.aux_class_names] = df.loc[:,CFG.aux_class_names].fillna(-1)\n",
    "#Mostramos los primeros 2 renglones del conjunto de entrenamiento\n",
    "display(df.head(2))\n",
    "\n",
    "#Leemos el conjunto de prueba\n",
    "test_df = pd.read_csv(f'{ruta_data_csv}/test.csv')\n",
    "#Creamos una nueva columna con la direcci√≥n de cada imagen correspondiente\n",
    "test_df['image_path'] = f'{ruta_img_train}/'+test_df['id'].astype(str)+'.jpeg'\n",
    "#Guardamos en una lista el nombre de todas las columnas del conjunto de prueba, estos son las columnas de las caracter√≠sticas de las plantas\n",
    "FEATURE_COLS = test_df.columns[1:-1].tolist()\n",
    "#Mostramos los primeros 2 renglones del conjunto de prueba\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f423179-5d8d-4187-bb6d-64f22ed546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crearemos un data loader el cual podr√° leer im√°genes y archivos CSV al mismo tiempo, tambi√©n maneja etiquetas para tareas principales y auxiliares\n",
    "#Este data loader tambi√©n aplicar√° modificaciones (augmentations) a las im√°genes, tales como flip, rotation, brightness, etc. Todas estas modificaciones se aplicar√° solo a un subconjunto de im√°genes. lo cual acelera el entrenamiento del modelo y reduce el cuello de botella del CPU\n",
    "\n",
    "#Funci√≥n para contruir el modificador de las im√°genes\n",
    "def build_augmenter():\n",
    "    #Definimos la modificaciones que se le har√°n a las im√°genes\n",
    "    aug_layers = [\n",
    "        #Brillo de la imagen que ser√° ajustado de manera aleatoria durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_brightness/]\n",
    "        keras_cv.layers.RandomBrightness(factor = 0.1, value_range = (0, 1)),\n",
    "        #Contraste de la imagen que ser√° ajustado de manera aleatoria durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_contrast/]\n",
    "        keras_cv.layers.RandomContrast(factor = 0.1, value_range = (0, 1)),\n",
    "        #Saturaci√≥n de la imagen que ser√° ajustado de manera aleatoria para reducir o aumentar la saturaci√≥n de la imagen de input [https://keras.io/api/keras_cv/layers/augmentation/random_saturation/]\n",
    "        keras_cv.layers.RandomSaturation(factor = (0.45, 0.55)),\n",
    "        #Color a o matiz de la imagen que ser√° ajustado de manera aleatoria [https://keras.io/api/keras_cv/layers/augmentation/random_hue/]\n",
    "        keras_cv.layers.RandomHue(factor = 0.1, value_range = (0, 1)),\n",
    "        #Recorte aleatorio de rect√°ngulos de las im√°genes para despu√©s rellenarlas. Aqu√≠ se pueden hacer rellenos gausianos [https://keras.io/api/keras_cv/layers/augmentation/random_cutout/]\n",
    "        keras_cv.layers.RandomCutout(height_factor = (0.06, 0.15), width_factor = (0.06, 0.15)),\n",
    "        #Voltea las im√°genes de manera aleatoria durante el proceso de entrenamiento [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/]\n",
    "        keras_cv.layers.RandomFlip(mode = \"horizontal_and_vertical\"),\n",
    "        #Hace zoom (de lejos o cerca) a las im√°genes de manera aleatoria durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_zoom/]\n",
    "        keras_cv.layers.RandomZoom(height_factor = (0.05, 0.15)),\n",
    "        #Rotaci√≥n aleatoria de las im√°genes durante el proceso de entrenamiento del modelo [https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/]\n",
    "        keras_cv.layers.RandomRotation(factor = (0.01, 0.05))\n",
    "    ]\n",
    "\n",
    "    #Aplicamos las modificaciones a muestras aleatorias \n",
    "    aug_layers = [keras_cv.layers.RandomApply(x, rate = 0.5) for x in aug_layers]\n",
    "\n",
    "    #Construimos la capa de modificaciones\n",
    "    augmenter = keras_cv.layers.Augmenter(aug_layers)\n",
    "\n",
    "    #Aplicamos las modificaciones\n",
    "    def augment(inp, label):\n",
    "        images = inp[\"images\"]\n",
    "        aug_data = {\"images\": images}\n",
    "        aug_data = augmenter(aug_data)\n",
    "        inp[\"images\"] = aug_data[\"images\"]\n",
    "        return inp, label\n",
    "    return augment\n",
    "\n",
    "    return augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50506ec8-e714-4fbc-b3dd-1905e77eb2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Augmenter' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/IPython/core/formatters.py:711\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    704\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    705\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    707\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    708\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    709\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    710\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 711\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/IPython/lib/pretty.py:411\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    410\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 411\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/IPython/lib/pretty.py:779\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(obj)\n\u001b[1;32m    780\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/Virtual_env_jup/jup_notebook/lib/python3.11/site-packages/keras/src/layers/layer.py:1274\u001b[0m, in \u001b[0;36mLayer.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, built=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1275\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Augmenter' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "x1 = build_augmenter()\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98858113-64d9-4be6-b20f-187525c36eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
